\documentclass{article}
\usepackage{amsfonts, amssymb, amsmath, amsthm, mathtools}
\usepackage{dsfont}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage[round]{natbib}
\geometry{verbose, margin=1in}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{endnotes}
\usepackage{bbm}
\usepackage{comment}
\usepackage{xtab}
\usepackage{booktabs}  
\usepackage{array}     
\usepackage[colorinlistoftodos]{todonotes}
\newcolumntype{C}{>{\centering\arraybackslash}p{1.8cm}}  

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\hypersetup{
	colorlinks=true,
	linktoc=all,
	linkcolor=blue,
	urlcolor=blue,
	citecolor=blue,
	filecolor=blue,
	linktocpage=true
}
\usepackage{cleveref}

\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}

\makeatother
\let\footnote=\endnote
%\setlength\parindent{0pt}

\newcommand{\mP}{\mathbb{P}}
\newcommand{\mE}{\mathbb{E}}
\newcommand{\mV}{\mathrm{Var}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\given}{\,|\,}
\newcommand{\sgiven}{\!\,|\,\!}
\newcommand{\I}{\mathbb{(I)}}
\newcommand{\II}{\mathbb{(II)}}
\newcommand{\III}{\mathbb{(III)}}
\newcommand{\IV}{\mathbb{(IV)}}
\newcommand{\independent}{\mbox{${}\perp\mkern-11mu\perp{}$}}
\newcommand{\nindep}{\mathrel{\not\!\perp\mkern-11mu\perp}}
\newcommand{\iid}{ \overset{\mathrm{i.i.d.}}{\sim}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\definecolor{DarkBlue}{rgb}{0,0,0.55}
\newcommand{\revised}[1]{\textcolor{DarkBlue}{#1}}

\renewcommand{\baselinestretch}{1.1}


\title{General Frameworks for Conditional Two-Sample Testing}

% \author{Yonsei University}
\author{
    Seongchan Lee\textsuperscript{1}\thanks{These authors contributed equally to this work.} \hspace{1.5cm} 
    Suman Cha\textsuperscript{1}\footnotemark[1] \hspace{1.5cm} 
    Ilmun Kim\textsuperscript{2}
}


\begin{document}

\maketitle
\footnotetext[1]{\textsuperscript{}Department of Statistics and Data Science, Yonsei University, Seoul, South Korea.}
\footnotetext[2]{\textsuperscript{}Department of Mathematical Sciences, KAIST, Daejeon, South Korea.}
\begin{abstract}
	We study the problem of conditional two-sample testing, which aims to determine whether two populations have the same distribution after accounting for confounding factors. This problem commonly arises in various applications, such as domain adaptation and algorithmic fairness, where comparing two groups is essential while controlling for confounding variables. We begin by establishing a hardness result for conditional two-sample testing, demonstrating that no valid test can have significant power against any single alternative without proper assumptions. We then introduce two general frameworks that implicitly or explicitly target specific classes of distributions for their validity and power. Our first framework allows us to convert any conditional independence test into a conditional two-sample test in a black-box manner, while preserving the asymptotic properties of the original conditional independence test. The second framework transforms the problem into comparing marginal distributions with estimated density ratios, which allows us to leverage existing methods for marginal two-sample testing. We demonstrate this idea in a concrete manner with classification and kernel-based methods. Finally, simulation studies are conducted to illustrate the proposed frameworks in finite-sample scenarios.
\end{abstract}


\tableofcontents
\section{Introduction}

This paper addresses the problem of testing for equivalence between two conditional distributions, namely conditional two-sample testing. Statistical methods for this problem have important applications across diverse fields such as domain adaptation and algorithmic fairness. In domain adaptation, for instance, this methodology can serve as a formal framework to validate the covariate shift assumption, where the conditional distribution of $Y$ given $X$ remains unchanged, while the marginal distributions of $X$ may differ. By confirming this assumption, practitioners can effectively re-weight the training data according to the marginal density ratio regarding $X$, which potentially leads to improved predictive performance and better adaptation to new domains~\citep{shimodaira2000improving,JMLR:v8:sugiyama07a,sugiyama2007direct}.
Moreover, in algorithmic fairness, conditional two-sample testing plays a role in detecting and mitigating biases. In particular, it helps identify whether a certain machine learning model unfairly favors or disadvantages specific groups based on demographic characteristics such as age, gender, or ethnicity~\citep{NIPS2016_9d268236,barocas2019fairness}. Conditional two-sample testing also finds applications beyond machine learning. In genomics, for example, scientists seek to identify differences in genetic distributions conditional on various factors such as disease status and environmental exposures~\citep{virolainen2022,wu2023}. This methodology aids scientists in understanding the genetic basis of diseases and in developing strategies for personalized medicine by providing a rigorous framework for comparing conditional distributions. 


\subsection{Problem Setup}
With the practical motivation in mind, we now formally set up the problem. Given $n_1,n_2 \in \mathbb{N}$, suppose we observe two mutually independent samples 
\begin{align*}
	\{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1} \iid P_{XY}^{(1)} \quad \text{and} \quad \{(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{n_2} \iid P_{XY}^{(2)}, 
\end{align*}
where $P_{XY}^{(1)}$ and $P_{XY}^{(2)}$ are joint distributions supported on some generic product space $\mathcal{X} \times \mathcal{Y}$. Let $P_{Y \sgiven X}^{(1)}$ and $P_{Y \sgiven X}^{(2)}$ denote the conditional distributions of $Y^{(1)} \given X^{(1)}$ and $Y^{(2)} \given X^{(2)}$, respectively. Similarly, let $P_{X}^{(1)}$ and $P_{X}^{(2)}$ denote the marginal distributions of $X^{(1)}$ and $X^{(2)}$, respectively. Given these two samples, our goal is to test the equality of two conditional distributions
\begin{align} \label{Eq: hypothesis}
	H_0: P_{X}^{(1)} \bigl\{ P_{Y \sgiven X}^{(1)}(\cdot \given X) = P_{Y \sgiven X}^{(2)}(\cdot \given X)   \big\} = 1 \quad \text{versus} \quad H_1: P_{X}^{(1)} \bigl\{ P_{Y \sgiven X}^{(1)}(\cdot \given X) \neq  P_{Y \sgiven X}^{(2)}(\cdot \given X) \big\} >0, 
\end{align}
where $P^{(j)}_X(\cdot\given x)$ denotes the conditional distribution of $Y^{(j)}$ given $X^{(j)}=x$ for $j \in \{1,2\}$. In other words, we are interested in determining whether two populations have the same distribution after controlling for potential confounding variables. Throughout this paper, we assume that $P_{X}^{(1)}$ and $P_{X}^{(2)}$ have the same support, satisfying $P_{X}^{(1)} \ll P_{X}^{(2)}$ and $P_{X}^{(2)} \ll P_{X}^{(1)}$ where the symbol $\ll$ denotes absolute continuity. Since $P_{X}^{(1)}$ and $P_{X}^{(2)}$ have the same support, the above hypotheses~\eqref{Eq: hypothesis} for conditional two-sample testing can be equivalently defined using $P_{X}^{(2)}$ instead of $P_{X}^{(1)}$. 

As pointed out by \citet{boeken2021bayesian} and \citet{yan2022nonparametric}, conditional two-sample testing is closely connected to conditional independence testing. To illustrate this connection, we introduce a binary variable $Z \in \{1,2\}$, and see that the conditional independence between $Y$ and $Z$ given $X$ is equivalently expressed as
\begin{align} \label{Eq: CIT = C2ST}
	Y \independent Z \given X ~ \Longleftrightarrow ~ (Y \given X, Z = 1) \overset{d}{=} (Y \given X, Z= 2), 
\end{align}
where the symbol $\overset{d}{=}$ denotes equality in distribution. This equivalence enables us to convert the problem of conditional two-sample testing to that of conditional independence testing based on the datasets $\{(Y_i,X_i) : Z_i = 1\}$ and $\{(Y_i,X_i) : Z_i = 2\}$. Consequently, we can leverage various existing methods for conditional independence testing to tackle conditional two-sample testing.  However, prior work has not rigorously explored this approach, and indeed \citet{yan2022nonparametric} claim that it is not a sensible approach as the variable $Z$ in the conditional two-sample problem is deterministic. Specifically, letting $n = n_1 + n_2$, $\sum_{i=1}^n \mathds{1}(Z_i = 1)$ and $\sum_{i=1}^n \mathds{1}(Z_i = 2)$ correspond to the sample sizes for two populations (i.e., $n_1$ and $n_2$), which are fixed in advance for the conditional two-sample problem. Therefore, a gap remains in rigorously connecting these seemingly similar, yet distinct, problems.


\subsection{An Overview of Our Results} 
In this work, we make several contributions to the field of conditional two-sample testing. First, we reaffirm that comparing conditional distributions is intrinsically more difficult than comparing marginal distributions. For marginal two-sample testing, one can design permutation tests that control the type I error, while being powerful against certain alternatives~\citep[e.g.,][]{kim2022minimax}. However, we show that this is not the case for conditional two-sample testing. Our result (\Cref{Theorem: negative result}) proves that any valid conditional two-sample test has power at most equal to its size against any single alternative if the type of a conditional random vector is continuous. This is reminiscent of the negative result for conditional independence testing proved in \citet{shah2020hardness}. It is worth highlighting, however, that their negative result does not directly imply our \Cref{Theorem: negative result}. The proof of \citet{shah2020hardness} relies on the assumption that the data $\{(X_i,Y_i,Z_i)\}_{i=1}^n$ are i.i.d., which does not hold in our setup as $\sum_{i=1}^n \mathds{1}(Z_i = 1)$ and $\sum_{i=1}^n \mathds{1}(Z_i = 2)$ are deterministic numbers. We handle this distinction through a concentration argument and show that conditional two-sample testing is as difficult as conditional independence testing. This negative result naturally motivates additional assumptions that make the problem feasible.

Our next contribution is to introduce two general frameworks for conditional two-sample testing. The first framework effectively addresses the issue pointed out by \citet{yan2022nonparametric}. In particular, we develop a generic method that converts any conditional independence test into a conditional two-sample test. This general method directly transfers the asymptotic properties of a conditional independence test computed using $\{(X_i,Y_i,Z_i)\}_{i=1}^n \iid P_{XYZ}$ to the setting of conditional two-sample testing (\Cref{Theorem: Converting C2ST into CIT}). At the heart of this approach is the concentration property of a Binomial random variable to its mean, which facilitates the effective construction of i.i.d.~samples drawn from $P_{XYZ}$ (see \Cref{Algorithm: Converting C2ST into CIT}). This development paves \ way to leverage any existing methods for conditional independence testing in the literature, thereby expanding the range of tools available to practitioners for conducting two-sample tests. 

The second framework that we introduce is based on density ratio estimation. To elaborate, let us assume that $P_{X}^{(1)}$ and $P_{X}^{(2)}$ have density functions $f_X^{(1)}$ and $f_{X}^{(2)}$ with respect to some base measure, and similarly $P_{Y \sgiven X}^{(1)}(\cdot \given x)$ and $P_{Y \sgiven X}^{(2)}(\cdot \given x)$ have density functions $f_{Y\sgiven X}^{(1)}(\cdot \given x)$ and $f_{Y\sgiven X}^{(2)}(\cdot \given x)$, respectively. Then for all $x,y \in \mathcal{X} \times \mathcal{Y}$, we have the identity:
\begin{align} \label{Eq: C2ST = Marginal 2ST}
	f_{Y \sgiven X}^{(1)}(y \given x) = f_{Y \sgiven X}^{(2)} (y \given x) ~\Longleftrightarrow~ f_{YX}^{(1)}(y,x) = \frac{f_{X}^{(1)}(x)}{f_{X}^{(2)}(x)} f_{YX}^{(2)}(y,x)\coloneqq f_{YX}(y,x),
\end{align} 
where $f_{YX}^{(1)}$ is the joint density function of $(Y^{(1)},X^{(1)})$ such that $f_{YX}^{(1)}(y,x) = f_{Y \sgiven X}^{(1)}(y \given x) f_{X}^{(1)}(x)$, and $f_{YX}^{(2)}$ is similarly defined for $(Y^{(2)},X^{(2)})$. The above equivalence~\eqref{Eq: C2ST = Marginal 2ST} allows us to transform the problem of testing for conditional distributions into the one that compares marginal distributions with densities $f_{YX}^{(1)}$ and $f_{YX}$. The latter problem has been extensively studied with various methods, ranging from classical approaches such as Hotelling's test to modern methods such as kernel maximum mean discrepancy~\citep{gretton2012kernel,liu2020learning,schrab2023mmd} and machine learning-based approaches~\citep[e.g.,][]{lopez2017revisiting,kim2019,kim2021classification,hediger2022use}. The issue, however, is that we do not observe samples from $f_{YX}$ but from $f_{YX}^{(2)}$. Therefore, the success of this framework relies on how accurate one can estimate the density ratio
\begin{align} \label{Eq: density ratio}
	r_X(x) \coloneqq  \frac{f_{X}^{(1)}(x)}{f_{X}^{(2)}(x)},
\end{align}
and incorporate it into the procedure to fill the gap between $f_{YX}$ and $f_{YX}^{(2)}$. We demonstrate this methodology focusing on a classification-based test in \Cref{Section: Classifier-based Approach} and a kernel-based test in \Cref{Section: Linear-Time MMD}. 



\subsection{Literature Review} \label{Section: Literature Review}
As mentioned earlier, conditional two-sample testing has a wide range of applications in various fields, including machine learning, genetics and economics, where it is important to compare two samples controlling for confounding variables. Despite its broad range of applications and significance, there has been limited research dedicated to tackling this fundamental problem. Similar problems, on the other hand, have been explored in the literature such as testing for the equality of conditional moments \citep{hall1990bootstrap, kulasekera1995comparison,kulasekera1997smoothing,fan1998test,neumeyer2003nonparametric, PardoFernndez2015} and goodness-of-fit testing for pre-specified conditional distributions \citep{Andrews1997, zheng2000consistent, fan2006nonparametric}. These methods aim to facilitate the comparison of specific aspects of a distribution such as the conditional mean or second moments, rather than the entire distribution. Our research, however, is centered on nonparametric comparisons of two conditional distributions. This approach is of great importance as it enables a more comprehensive comparison of distributions, capturing differences that may not be evident when only specific moments or pre-specified models are compared.

It is only in recent years that conditional two-sample testing has gained attention, with several novel methods being proposed. \citet{yan2022nonparametric}, for instance, proposed a method that extends unconditional energy distance to its conditional counterpart. They demonstrated that many key properties of the unconditional energy distance are retained in the conditional version. Moreover, they proposed a bootstrap procedure to calibrate their test statistic. To the best of our knowledge, however, the validity of their test remains unexplored, and the $O(n^4)$ time complexity of their algorithm poses a bottleneck to its practical application. 


As another example, \citet{hu2024two} built on the idea of conformal prediction and introduced a nonparametric conditional two-sample test using a weighted rank-sum statistic. This approach involves estimating both marginal and conditional density ratios, and the validity of their method depends on the quality of these ratio estimators. As explained in \Cref{Example: Rank sum statistic}, their test statistic can be viewed as an example of our general framework based on density ratio estimation. A more recent work by \citet{chen2024biased} extended the idea of \citet{hu2024two}, leveraging Neyman orthogonality to reduce the first-order bias for the asymptotic normality. As another closely related work, \citet{CHATTEJEE2024conditional} introduced a kernel-based conditional two-sample test using nearest neighbors. They considered the setting where a random sample $\{(X_i,Y_i^{(1)},Y_i^{(2)})\}_{i=1}^n$ is generated from a joint distribution, i.e., the response variables $Y^{(1)}$ and $Y^{(2)}$ are conditioned on the same set of covariates $X$. This setting is notably different from that considered in the prior work~\citep{yan2022nonparametric,hu2024two,chen2024biased} as well as in our study, which consider potentially different covariates. Hence, the methods proposed by \cite{CHATTEJEE2024conditional} are not directly comparable to ours. 

As explained before, the first framework that we propose can be constructed based on essentially any conditional independence tests from the literature. The problem of testing for conditional independence has been extensively studied, resulting in a variety of methods to handle different scenarios and challenges. \citet{shah2020hardness} proposed the Generalized Covariance Measure (GCM) whose validity depends on the performance of regression methods. Recent improvements to this method include the strategies such as weighting \citep{scheidegger2022weighted} and applying GCM to a projected random vector \citep{lundborg2022projected,chakraborty2024doubly}. Other notable methodologies for conditional independence testing include kernel-based tests~\citep{zhang2012kernel,doran2014permutation,strobl2019approximate,pogodin2024practical}, binning-based tests~\citep{neykov2021minimax,kim2022local,neykov2023nearly}, regression-based tests~\citep{dai2022significance,williamson2023general} and tests under the model-X framework~\citep{candes2018panning,berrett2020conditional,liu2022fast,tansey2022holdout}. Our method can leverage these developments to effectively solve the problem of conditional two-sample testing.

Our second framework can benefit from extensive research done on density ratio estimation in the literature. A straightforward way of estimating density ratio is to first estimate individual density functions, and take their ratio as an estimate. However, this method tends to become unstable, especially in high-dimensional settings. To overcome this issue, \citet{sugiyama2007direct} and \citet{tsuboi2009direct} developed methods that directly estimate density ratio without involving density estimation. \citet{kanamori2010theoretical} compared different methods of density ratio estimation, and discussed their theoretical properties. \citet{kanamori2009least} reformulated the problem as a least-squares problem to provide a closed-form solution, whereas \citet{liu2017trimmed} proposed trimmed density ratio estimation to improve stability and robustness by trimming extreme values. More recent advancements in density ratio estimation include \citet{choi2021featurized,rhodes2020telescoping,choi2022density}. As explained in \Cref{Section: Approach via Density Ratio Estimation}, our approach uses density ratio estimation to deal with discrepancies between $f_{YX}^{(1)}$ and $f_{YX}$, and transforms the problem of comparing conditional distributions into that of comparing marginal distributions.



\subsection{Organization} 
The rest of this paper is organized as follows. We begin with a hardness result for conditional two-sample testing in \Cref{Section: Impossibility Result of Conditional Two-Sample Testing}, which shows that no test can have power greater than its size against any alternative without additional assumptions. \Cref{Section: Approach via Conditional Independence Testing} presents our framework that converts tests for conditional independence into those for the equality of conditional distributions. \Cref{Section: Approach via Density Ratio Estimation} introduces another framework based on density ratio estimation. Numerical results illustrating the finite-sample performance of our methods are presented in \Cref{Section: Numerical Experiments}, followed by the conclusion in \Cref{Section: Conclusion}. The proofs of the results omitted in the main text can be found in the appendix.


\section{Hardness Result} \label{Section: Impossibility Result of Conditional Two-Sample Testing}
Before introducing our frameworks, we present a fundamental hardness result for conditional two-sample testing. Specifically, for a continuous random vector $X$, our result demonstrates that any valid conditional two-sample test has no power against any alternative. This finding parallels the negative result established by \citet{shah2020hardness} for conditional independence testing, and our proof builds crucially on their work.
Given the connection established in \eqref{Eq: CIT = C2ST}, one might argue that their negative result directly applies to the two-sample problem. However, additional effort is required to make this connection concrete since the sample sizes $n_1$ and $n_2$ are deterministic in our setting, which violates the i.i.d.~assumption required in \citet{shah2020hardness}.

% To state the result, let \revised{$\mathcal{P}_{0, M}$ be the class of pairs of distributions $(P_{XY}^{(1)}, P_{XY}^{(2)})$ that satisfy the null hypothesis $H_0$ in \eqref{Eq: hypothesis} and are supported on an $\ell_{\infty}$ ball of radius $M \in (0, \infty]$. Let $\mathcal{P}_{1, M}$ be the set of alternative pairs of distributions not in $\mathcal{P}_{0, M}$ but also supported on the same ball. The following theorem shows that no valid test has power greater than its size.}
\revised{To state the result, let $\mathcal{E}$ denote the set of all pairs of distributions $(P^{(1)}_{XY}, P^{(2)}_{XY})$ defined on a generic product space $\mathcal{X}\times\mathcal{Y}$. For each $j\in\{1,2\}$, assume that the marginals $P^{(j)}_X$ and $P^{(j)}_Y$ are absolutely continuous with respect to the Lebesgue measures on $\mathbb{R}^{d_X}$ and $\mathbb{R}^{d_Y}$, respectively. 
% Consider any $(P^{(1)}_{XY}, P^{(2)}_{XY}) \in \mathcal{E}$. % Let $\mathcal{P}_{0}\subset \mathcal{E}$ be the null distribution class consisting of pairs of distributions with identical conditional distributions, that is, $P_{Y|X}^{(1)}=P_{Y|X}^{(2)}$ (or \textit{satisfy the null hypothesis $H_0$ in \eqref{Eq: hypothesis}}). Define the alternative distribution class as $\mathcal{P}_{1}=\mathcal{E}\setminus \mathcal{P}_{0}$.
Let $\mathcal{P}_0 \subset \mathcal{E}$ denote the set of distribution pairs satisfying the null hypothesis $H_0$ in \eqref{Eq: hypothesis}, and let $\mathcal{P}_1 = \mathcal{E} \setminus \mathcal{P}_0$ denote the corresponding alternative class. For $M \in (0,\infty]$, let $\mathcal{E}_M \subseteq \mathcal{E}$ be the subset of pairs with support contained strictly with in an $\ell_{\infty}$ ball of radius $M$. We then define $\mathcal{P}_{0,M} = \mathcal{P}_0 \cap \mathcal{E}_M$ and $\mathcal{P}_{1,M} = \mathcal{P}_1 \cap \mathcal{E}_M.$ The following theorem establishes that no valid test $\phi$ for conditional two-sample testing can attain power exceeding its size.}
% \begin{theorem} \label{Theorem: negative result}
% 	Let $n_1,n_2 \in \mathbb{N}$ with $n=n_1+n_2$, $\alpha \in (0,1)$ and $M \in (0,\infty]$. For $\{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1} \iid P_{XY}^{(1)}$ and $\{(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{n_2} \iid P_{XY}^{(2)}$. Consider a test $\phi : \{(X_{_{i}}^{(1)},Y_{_{i}}^{(1)},1)\}_{i=1}^{n_{1}} \cup \{(X_{_{i}}^{(2)},Y_{_{i}}^{(2)},2)\}_{i=1}^{n_{2}} \mapsto \{0,1\}$. Suppose that $\phi$ controls the type I error at level $\alpha$
% 	\begin{align*}
%         \sup_{(P_{XY}^{(1)}, P_{XY}^{(2)}) \in \mathcal{P}_{0,M}} \mE[\phi(\{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1}, \{(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{n_2})] \leq \alpha.
%         \end{align*}
% 	Then for any $(P_{XY}^{(1)}, P_{XY}^{(2)}) \in \mathcal{P}_{1,M}$, the power of $\phi$ is at most $\alpha$ as 
% 	\begin{align*}
%         \mE\left[\phi(\{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1}, \{(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{n_2})\right] \leq \alpha.
%         \end{align*}
% \end{theorem}

% 분포가 pair단위로 joint를 정의. 

\begin{theorem} \label{Theorem: negative result}
\revised{Let $n_{1},n_{2}\in\mathbb{N}$, $\alpha\in(0,1)$ and $M\in(0,\infty]$. Suppose we observe two independent samples
\begin{align*}
	\{(X_{i}^{(1)},Y_{i}^{(1)})\}_{i=1}^{n_1}\iid P_{XY}^{(1)} \quad \text{and} \quad \{(X_{i}^{(2)},Y_{i}^{(2)})\}_{i=1}^{n_2}\iid P_{XY}^{(2)},
\end{align*}
where $(P_{XY}^{(1)}, P_{XY}^{(2)})=:P \in \mathcal{E}_M$. Consider a test $\phi: \{(X_{i}^{(1)},Y_{i}^{(1)})\}_{i=1}^{n_{1}}\cup\{(X_{i}^{(2)},Y_{i}^{(2)})\}_{i=1}^{n_{2}} \to \{0,1\}.$ Suppose $\phi$ controls the type I error at level $\alpha$, i.e., 
\begin{align*}
    \sup_{P \in \mathcal{P}_{0,M}}\mE_{P}[\phi]\leq \alpha.
\end{align*}
Then the power of $\phi$ is at most $\alpha$ for any $P \in \mathcal{P}_{1,M}$, that is,
\begin{align*}
    \sup_{P \in \mathcal{P}_{1,M}}\mE_{P}[\phi]\leq \alpha.
\end{align*}}

\end{theorem}

% \begin{theorem} \label{Theorem: negative result}
% Let $n_1, n_2 \in \mathbb{N}$ with $n = n_1 + n_2$, $\alpha \in (0,1)$ and $M \in (0,\infty]$. Define
% \begin{align*}
%     P_{XYZ} := \frac{n_1}{n}\,P_{XY}^{(1)} \otimes \delta_{\{Z=1\}}  
% + \frac{n_2}{n}\,P_{XY}^{(2)} \otimes \delta_{\{Z=2\}},
% \end{align*}
% where $\delta_{\{Z=z\}}$ denotes the Dirac measure at $Z=z$, i.e., the probability measure that assigns mass $1$ to the event $\{Z=z\}$. For brevity, we write $P := P_{XYZ}$. Let $\{({X}_i,{Y}_i,{Z}_i)\}_{i=1}^n \sim P$ with $\sum_{i=1}^n \mathds{1}({Z}_i=1) = n_1$ and $\sum_{i=1}^n \mathds{1}({Z}_i=2) = n_2$. Consider a test $\phi : \{(X_i,Y_i,Z_i)\}_{i=1}^n \mapsto \{0,1\}$. 
% Suppose that $\phi$ controls the type I error at level $\alpha$ as
% \begin{align*}
%     \sup_{P\in \mathcal{P}_{0,M}} \mathbb{E}_{P}[\phi] \leq \alpha.
% \end{align*}
% Then the power of $\phi$ is at most $\alpha$ for any $P\in\mathcal{P}_{1,M}$ as
% \begin{align*}
%     \sup_{P\in \mathcal{P}_{1,M}} \mathbb{E}_{P}[\phi] \leq \alpha.
% \end{align*}
% \end{theorem}

% \revised{Proving Theorem~\ref{Theorem: negative result} directly is challenging. Instead, we leverage the known hardness result for conditional independence testing established by \citet{shah2020hardness}. Specifically, we embed the conditional two-sample problem into the conditional independence framework by introducing a group indicator $Z \in \{1, 2\}$, as described in \Cref{Eq: CIT = C2ST}. The main challenge is that, in the conditional two-sample setting, the group sizes $n_1$ and $n_2$ are fixed, whereas the negative result of \citet{shah2020hardness} assumes i.i.d.~draws from a joint distribution $P_{XYZ}$. In our setting, however, the constraint
% \begin{align*}
%     \sum_{i=1}^n \mathds{1}\left(Z_i=1\right) = n_1, \;\sum_{i=1}^n \mathds{1}\left(Z_i=2\right) = n_2,
% \end{align*}
% implies that the $Z_i$’s are not sampled i.i.d., making a direct application of their result nontrivial.} 
% By introducing a binary variable $Z \in \{1, 2\}$, testing $P_{Y \given X}^{(1)} = P_{Y \given X}^{(2)}$ is equivalent to testing $Y \independent Z \given X$.} \revised{The key challenge, however, is that conditional two-sample problem operates on samples with fixed $n_1$ and $n_2$, whereas the result of \citet{shah2020hardness} applies to a setting with $n=n_1+n_2$ i.i.d. samples from $P_{XYZ}$, where the number of observations with $Z=1$ is a random variable.}
\noindent \textbf{Remark.} 
We note that \Cref{Theorem: negative result} only focuses on non-randomized tests for simplicity of presentation, but our proof also holds for randomized tests. The proof of \Cref{Theorem: negative result} is provided below.


% \revised{Proving Theorem~\ref{Theorem: negative result} directly is challenging. Instead, we leverage the known hardness result for conditional independence testing established by \citet{shah2020hardness}. To this end, we embed the conditional two-sample problem into the conditional independence framework by introducing a group indicator $Z \in \{1,2\}$, as described in \eqref{Eq: CIT = C2ST}. Suppose the two samples have deterministic sizes $n_1$ and $n_2$, and write $n := n_1+n_2$. Formally, we define the joint distribution of $(X,Y,Z)$ as
% \begin{align}\label{Joint pdf}
%     P_{XYZ} 
%     := \frac{n_1}{n}\, P_{XY}^{(1)} \otimes \delta_{\{Z=1\}}
%      + \frac{n_2}{n}\, P_{XY}^{(2)} \otimes \delta_{\{Z=2\}},
% \end{align}
% where $\delta_{\{Z=z\}}$ denotes the Dirac measure at $Z=z$. In other words, $(X,Y)$ follows $P_{XY}^{(1)}$ when $Z=1$ and $P_{XY}^{(2)}$ when $Z=2$. 
% Given a two-sample test 
% \begin{align*}
% \phi_{\text{two}} : \{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1} \cup \{(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{n_2} \to \{0,1\},    
% \end{align*}
% we then define the associated conditional independence test
% \begin{align*}
%     \phi_{\text{indep}}\big(\{(X_i,Y_i,Z_i)\}_{i=1}^n\big) 
%     := \phi_{\text{two}}\!\left(\{(X_i,Y_i): Z_i=1\} \cup \{(X_i,Y_i): Z_i=2\}\right).
% \end{align*}
% }
%---------------------------------------------------------------------------------
% ver.0 
%---------------------------------------------------------------------------------
% The main challenge is that, in the conditional two-sample setting, the group sizes $n_1$ and $n_2$ are fixed, whereas the hardness result of \citet{shah2020hardness} assumes that $\{(X_i, Y_i, Z_i)\}_{i=1}^{n}$ are i.i.d.~draws from $P_{XYZ}$. In the conditional two-sample problem, however, letting $N_1 = \sum_{i=1}^n \mathds{1}(Z_i=1)$ and $N_2 = \sum_{i=1}^n \mathds{1}(Z_i=2)$, the constraints $N_1 = n_1$ and $N_2 = n_2$ imply that the $Z_i$’s are not sampled i.i.d., which makes a direct application of their result nontrivial. Nevertheless, under the conditional independence framework with deterministic group sizes the type I error and power of $\phi_{\text{indep}}$ conditional on $N_1=n_1$ and $N_2=n_2$ coincide with those of $\phi_{\text{two}}$. Therefore, after some modification the negative result for conditional independence testing extends directly to Theorem~\ref{Theorem: negative result} for conditional two-sample testing. The proof of \Cref{Theorem: negative result} is provided below.

%---------------------------------------------------------------------------------
% ver.1
% \revised{The main challenge is that, in the conditional two-sample setting, the group sizes $N_1 = \sum_{i=1}^n \mathds{1}(Z_i=1)$ and $N_2 = \sum_{i=1}^n \mathds{1}(Z_i=2)$ are fixed, whereas the hardness result of \citet{shah2020hardness} assumes that $\{(X_i, Y_i, Z_i)\}_{i=1}^{n}$ are i.i.d. draws from $P_{XYZ}$. In our setting, the constraints $N_1 = n_1$ and $N_2 = n_2$ imply that the $Z_i$'s are not sampled i.i.d., making a direct application of their result nontrivial. To bridge this gap, we consider a larger i.i.d.~sample of size $N \geq 2n$ from $P_{XYZ}$ and show that $N_1' = \sum_{i=1}^N \mathds{1}(Z_i = 1)$ and $N_2' = \sum_{i=1}^N \mathds{1}(Z_i = 2)$ satisfy $N_j' \geq n_j$ with probability approaching 1 as $N \to \infty$. The key insight is conditioning on $\{N_1' \geq n_1, N_2' \geq n_2\}$ ensures we obtain the required samples sizes while maintaining the equivalence between conditional independence and conditional two-sample testing. Since the probability of the complementary event vanishes asymptotically, the hardness result transfers through this limiting procedure.}
%---------------------------------------------------------------------------------

% \noindent \textbf{Proof of \Cref{Theorem: negative result}.} 
% \revised{To establish the hardness result for conditional two-sample testing, we embed the problem into a conditional independence framework. This reformulation arises by introducing a group indicator $Z \in \{1,2\}$ to label the two samples, so that the null hypothesis of conditional two-sample testing ($P^{(1)}_{Y|X} = P^{(2)}_{Y|X}$) is equivalent to the conditional independence statement $Y \independent Z \mid X$. Formally, let $(X, Y, Z)$ be a random vector in $\mathbb{R}^{d_X+d_Y} \times \{1,2\}$, and let $\mathcal{E}^\prime$ denote the set of all distributions such that $X$ and $Y$ have marginal distributions absolutely continuous with respect to the Lebesgue measure. Assume that $Z$ is discrete, supported on $\{1,2\}$, with $\mathbb{P}(Z=1) = \lambda_n \in (0,1)$ where $\lambda_n := n_1/n$. Define $\mathcal{P}^\prime_0 \subset \mathcal{E}^\prime$ as the set of null distributions satisfying $Y \independent Z \mid X$, and let $\mathcal{P}^\prime_1 = \mathcal{E}^\prime \setminus \mathcal{P}^\prime_0$. For $M \in (0,\infty]$, let $\mathcal{E}^\prime_M \subseteq \mathcal{E}^\prime$ be the subset of distributions where $(X,Y)$ is supported in an $\ell_\infty$ ball of radius $M$, and define $\mathcal{P}^\prime_{0,M} = \mathcal{P}^\prime_0 \cap \mathcal{E}^\prime_M$ and $\mathcal{P}^\prime_{1,M} = \mathcal{P}^\prime_1 \cap \mathcal{E}^\prime_M$. Further, set $N_1 := \sum_{i=1}^n \mathds{1}(Z_i=1)$ and $N_2 := \sum_{i=1}^n \mathds{1}(Z_i=2)$. Note that the null distribution classes $\mathcal{P}_0$ (two-sample) and $\mathcal{P}^\prime_0$ (conditional independence) are equivalent: $P^{(1)}_{Y|X} = P^{(2)}_{Y|X}$ if and only if $Y \perp Z \mid X$. The same equivalence holds under the alternative: $P^{(1)}_{Y|X} \neq P^{(2)}_{Y|X}$ if and only if $Y \nindep Z \mid X$ in the joint representation with indicator $Z$. Our proof proceeds by embedding the given conditional two-sample test $\phi$ into this conditional independence framework. The hardness result of \citet{shah2020hardness} continues to hold under this restriction on the marginal distribution of $Z$. Moreover, following \citet{neykov2021minimax}, we show that the specific choice of the marginal distribution of $Z$ does not affect the hardness result, since the difficulty fundamentally arises from the continuous nature of the conditioning variable $X$. The detailed argument is provided in \Cref{Lemma: hardness marginal fixed}.}

% \revised{To bridge the two-sample and conditional independence formulations, we represent the joint distribution of $(X,Y,Z)$ as
% \begin{align*}
%     P_{XYZ} 
%     := \frac{n_1}{n}\,P_{XY}^{(1)} \otimes \delta_{\{Z=1\}}  
%     + \frac{n_2}{n}\,P_{XY}^{(2)} \otimes \delta_{\{Z=2\}},
% \end{align*}
% where $\delta_{\{Z=z\}}$ denotes the Dirac measure at $Z=z$, i.e., the probability measure that assigns unit mass to the event $\{Z=z\}$. For brevity, we write $P := P_{XYZ}$. Since \Cref{Theorem: negative result} is stated for a two-sample test $\phi_{\text{two}}$, we construct from it the associated conditional independence test $\phi_{\text{indep}} : \{(X_i,Y_i,Z_i)\}_{i=1}^n \iid P \to \{0,1\},$ which applies $\phi_{\text{two}}$ to the joint representation with group indicator $Z$.} 



\medskip 
\noindent \textbf{Proof of \Cref{Theorem: negative result}.} 
\revised{
Proving Theorem~\ref{Theorem: negative result} directly is challenging, so we instead rely on the known hardness result for conditional independence testing established by \citet{shah2020hardness}. To connect the two settings, we embed the conditional two-sample problem into the conditional independence framework by introducing a group indicator $Z \in \{1,2\}$, as in \eqref{Eq: CIT = C2ST}. Here, $Z$ simply indicates which sample an observation comes from, thereby allowing the two conditional distributions to be expressed within a single joint distribution over $(X,Y,Z)$. To make this precise, suppose the two samples have deterministic sizes $n_1$ and $n_2$, and write $n := n_1+n_2$. Formally, the joint distribution of $(X,Y,Z)$ is defined as
\begin{align}\label{Joint pdf}
P_{XYZ}
:= \frac{n_1}{n}\, P_{XY}^{(1)} \otimes \delta_{{Z=1}}
+ \frac{n_2}{n}\, P_{XY}^{(2)} \otimes \delta_{{Z=2}},
\end{align}
where $\delta_{{Z=z}}$ denotes the Dirac measure at $Z=z$, so that $(X,Y)$ follows $P_{XY}^{(1)}$ when $Z=1$ and $P_{XY}^{(2)}$ when $Z=2$.
Given a two-sample test
\begin{align*}
\phi_{\text{two}} : \{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1} \cup \{(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{n_2} \to \{0,1\},
\end{align*}
we define the associated conditional independence test by
\begin{align*}
\phi_{\text{indep}}\big(\{(X_i,Y_i,Z_i)\}_{i=1}^n\big)
:= \phi_{\text{two}}\big(\{(X_i,Y_i): Z_i=1\} \cup \{(X_i,Y_i): Z_i=2\}\big),
\end{align*}
where $\{(X_i,Y_i,Z_i)\}_{i=1}^n\iid P_{XYZ}.$ Under this representation, the two-sample null and alternative classes $\mathcal{P}_{0,M}$ and $\mathcal{P}_{1,M}$ coincide with the conditional independence classes defined by $Y \independent Z \mid X$ and $Y \nindep Z \mid X$, respectively. The main subtlety is that in the conditional two-sample setting the group sizes $N_1 = \sum_{i=1}^n \mathds{1}(Z_i=1)$ and $N_2 = \sum_{i=1}^n \mathds{1}(Z_i=2)$ are fixed, whereas the hardness result of \citet{shah2020hardness} assumes i.i.d.~draws from $P_{XYZ}$. Thus, the $Z_i$’s are not i.i.d., and a direct application of their result is not immediate.
Nevertheless, conditioning on $N_1=n_1$ and $N_2=n_2$, the type I error and power of $\phi_{\text{indep}}$ coincide exactly with those of $\phi_{\text{two}}$, so the reduction remains valid. Moreover, following \citet{neykov2021minimax}, the specific marginal distribution of $Z$ (e.g., $\mP(Z=1)=n_1/n$) does not affect the hardness argument that is the essential difficulty stems from the continuous nature of the conditioning variable $X$. A detailed justification of this claim is provided in \Cref{Lemma: hardness marginal fixed}.} For a constant $N \in \mathbb{N}$ greater than $2n$, we work with $N$ i.i.d.~random samples $\{(X_i,Y_i,Z_i)\}_{i=1}^N \iid P$ and define $N_1' = \sum_{i=1}^N \mathds{1}(Z_i = 1)$ and $N_2' = \sum_{i=1}^N \mathds{1}(Z_i = 2)$, which follow $N_1' \sim \mathrm{Binomial}(N,\lambda_n)$ and $N_2' \sim \mathrm{Binomial}(N,1-\lambda_n)$, respectively. For $n_1,n_2 \in \mathbb{N}$ given in the theorem statement, define a good event $\mathcal{A}\coloneqq \{N_1' \geq n_1, N_2' \geq n_2\}$, whose probability satisfies $\mP(\mathcal{A}) \geq 1 - \mP(N_1' < n_1) - \mP(N_2' < n_2)$ by the union bound.  Since $N \geq 2n$, we can ensure that $n_1 - N\lambda_n \leq -\frac{1}{2} N\lambda_n$ and thus
\begin{align*}
	\mP(N_1' < n_1) & =  \mP(N_1' - N\lambda_n < n_1 - N\lambda_n)  \leq \mP\Bigl( N_1' - N\lambda_n < - \frac{1}{2} N \lambda_n \Bigr) \\
	& \leq \mP\Bigl( \big|N_1' - N\lambda_n \big| >  \frac{1}{2} N \lambda_n \Bigr) \leq \frac{4(1 - \lambda_n)}{N\lambda_n},
\end{align*}
where the last inequality uses Chebyshev's inequality along with $N_1' \sim \mathrm{Binomial}(N,\lambda_n)$. We can similarly obtain that $\mP(N_2' < n_2) \leq \frac{4\lambda_n}{N(1-\lambda_n)}$. Therefore, the probability of the good event $\mathcal{A}$ is lower bounded as
\begin{align} \label{Eq: lower bound for the prob A}
	\mP(\mathcal{A}) \geq 1 - \frac{4(1-\lambda_n)^2 + 4\lambda_n^2}{N\lambda_n(1-\lambda_n)} \overset{\mathrm{set}}{=} 1 -  \varepsilon_N, 
\end{align}
where $\varepsilon_N \rightarrow 0$ as $N \rightarrow \infty$. 

We now consider a test \revised{$\phi_{\text{indep}}$} that only uses $n_1+n_2$ data points out of $N$ samples. Importantly, this sample consists of $n_1$ observations from $\left\{\left(Y_i, X_i\right): Z_i=1\right\}$ and $n_2$ observations from $\left\{\left(Y_i, X_i\right): Z_i=2\right\}$, whenever $\mathds{1}(\mathcal{A})=1$. If $\mathds{1}(\mathcal{A})=0$, this test simply returns 0 (i.e., accept $H_0$ ). Moreover, we assume that this test satisfies $\sup _{P \in \mathcal{P}_{0, M}} \mathbb{E}_P[\revised{\phi_{\text{indep}}} \mid \mathcal{A}] \leq \alpha$. In fact, since $\revised{\phi_{\text{indep}}}$ only uses $n_1$ observations with $Z=1$ and $n_2$ observations with $Z=2$, the previous inequality implies $\sup _{P \in \mathcal{P}_{0, M}} \mathbb{E}_P\left[\revised{\phi_{\text{indep}}} \mid N_1=n_1, N_2=n_2\right] = \sup _{P \in \mathcal{P}_{0, M}} \mathbb{E}_P\left[\revised{\phi_{\text{two}}} \right]\leq \alpha$, i.e., $\revised{\phi_{\text{two}}}$ is a valid level $\alpha$ test for conditional two-sample testing. Based on the previous results, the type I error of $\revised{\phi_{\text{indep}}}$ constructed based on $\left\{\left(X_i, Y_i, Z_i\right)\right\}_{i=1}^N \stackrel{\text { i.i.d. }}{\sim} P$ fulfills 
\begin{align*}
\sup_{P \in \mathcal{P}_{0,M}} \mathbb{E}_P[\revised{\phi_{\text{indep}}}] = \sup_{P \in \mathcal{P}_{0,M}} \mathbb{E}_P[\revised{\phi_{\text{indep}}} \mathds{1}(\mathcal{A})] \leq \sup_{P \in \mathcal{P}_{0,M}} \mathbb{E}_P[\revised{\phi_{\text{indep}}}| \mathcal{A}] \leq \alpha.
\end{align*}
This implies that $\phi$ is a valid test for conditional independence with size $\alpha$. Therefore, for any $P \in \mathcal{P}_{1, M}$,
\begin{align*}
& \mathbb{E}_P[\revised{\phi_{\text{indep}}} \mid \mathcal{A}]\left(1-\varepsilon_N\right) \stackrel{(\mathrm{i})}{\leq} \mathbb{E}_P[\revised{\phi_{\text{indep}}} \mid \mathcal{A}] \mathbb{E}_P[\mathds{1}(\mathcal{A})] \leq \mathbb{E}_P[\revised{\phi_{\text{indep}}}] \stackrel{(\mathrm{ii})}{\leq} \alpha \\
\Longleftrightarrow & \mathbb{E}_P[\revised{\phi_{\text{indep}}} \mid \mathcal{A}] \stackrel{(\mathrm{iii})}{=} \mathbb{E}_P\left[\revised{\phi_{\text{two}}}\right] \leq \frac{\alpha}{1-\varepsilon_N},
\end{align*}
where step~(i) uses the inequality in \ref{Eq: lower bound for the prob A}, 
step~(ii) holds by \citep[Theorem~2 and Remark~4]{shah2020hardness}, 
and step~(iii) uses the fact that $\revised{\phi_{\text{indep}}}$ only uses $n_1+n_2$ observations as described before \revised{and $(\phi_{\text{indep}} \mid N_{1}=n_1,\, N_{2}=n_2) = \phi_{\text{two}}$}. 
Since $N$ was an arbitrary number greater than or equal to $2n$ and $\varepsilon_N \rightarrow 0$ as $N \rightarrow \infty$, we can conclude that  $\mathbb{E}_P\left[\revised{\phi_{\text{two}}} \right] \leq \alpha$ for any $P \in \mathcal{P}_{1, M}$.\qed
% \revised{We now show that $\phi_{\text{indep}}$ is a valid conditional independence test with size $\alpha$. Under a null distribution $P \in \mathcal{P}_{0, M}$, the constructed subsample on which $\phi_{\text{indep}}$ operates has the same distribution as the on in the two-sample problem. Since $\phi_{\text{two}}$ is a valid level~$\alpha$ test for conditional two-sample problem, we have $\sup_{P \in \mathcal{P}_{0,M}}\mE_{P}[\phi \given \mathcal{A}] \leq \alpha$. The type I error of $\phi'$ is bounded by}
% \begin{align*}
%     \sup_{P \in \mathcal{P}_{0,M}}\mE[\phi' ] &= \sup_{P \in \mathcal{P}_{0,M}}\mE_P[\phi' \mathds{1}(\mathcal{A})] \\ 
%     &= \sup_{P \in \mathcal{P}_{0,M}}\mE_P[\phi \given \mathcal{A}]\mP(\mathcal{A})\leq \alpha.
% \end{align*}
% \revised{This confirms that $\phi'$ is a valid test for conditional independence. Therefore, for any $P \in \mathcal{P}_{1, M}$,} 
% \begin{align*}
%     &\mE_P[\phi \given \mathcal{A}] (1 - \varepsilon_N) \overset{\mathrm{(i)}}{\leq} \mE_P [\phi \given \mathcal{A}] \mE_P[\mathds{1}(\mathcal{A})] \leq \mE_P[\phi'] \overset{\mathrm{(ii)}}{\leq}  \alpha \\begin{align*}.5em] \Longleftrightarrow ~ &\mE_P[\phi \given \mathcal{A}] \overset{\mathrm{(iii)}}{=}\mE_P[\phi \given N_1=n_1, N_2=n_2] \leq \frac{\alpha}{1-\varepsilon_N},\end{align*}
% \revised{where step~$\mathrm{(i)}$ uses the inequality in \eqref{Eq: lower bound for the prob A}, step~$\mathrm{(ii)}$ holds by \citet{shah2020hardness} and step~$\mathrm{(iii)}$ uses the fact that $\phi$ only uses $n_1 +n_2$ observations as described before. Since $N$ was an arbitrary number greater than or equal to $2n$ and $\varepsilon_N \rightarrow 0$ as $N \rightarrow \infty$, we can conclude that $\mE_P[\phi \given N_1=n_1, N_2=n_2] \leq \alpha$ for any $P \in \mathcal{P}_{1, M}$. \qed}


% Now consider a test $\phi$ that only uses $n_1 + n_2$ data points out of $N$ samples. Importantly, this sample consists of $n_1$ observations from $\{(Y_i,X_i) : Z_i = 1\}$ and $n_2$ observations from $\{(Y_i,X_i) : Z_i = 2\}$, whenever $\mathds{1}(\mathcal{A}) = 1$. If $\mathds{1}(\mathcal{A}) = 0$, this test simply returns $0$ (i.e., accept $H_0$). Moreover, we assume that this test satisfies $\sup_{P \in \mathcal{P}_{0,M}} \mE_{P}[\phi \given \mathcal{A}] \leq \alpha$. In fact, since $\phi$ only uses $n_1$ observations with $Z=1$ and $n_2$ observations with $Z=2$, the previous inequality implies $\sup_{P \in \mathcal{P}_{0,M}} \mE_{P}[\phi \given N_1 = n_1, N_2 = n_2] \leq \alpha$, i.e., $\phi$ is a valid level $\alpha$ test for conditional two-sample testing. Based on the previous results, the type I error of $\phi$ constructed based on $\{(X_i,Y_i,Z_i)\}_{i=1}^N \iid P$ fulfills
% \begin{align*}
% 	\sup_{P \in \mathcal{P}_{0,M}} \mE_{P}[\phi] = \sup_{P \in \mathcal{P}_{0,M}} \mE_{P}[\phi\mathds{1}(\mathcal{A})]  \leq \sup_{P \in \mathcal{P}_{0,M}} \mE_{P}[\phi \given \mathcal{A}] \leq \alpha.
% \end{align*}
% This implies that $\phi$ is a valid test for conditional independence with size $\alpha$. Therefore, for any $P \in \mathcal{P}_{1,M}$,
% \begin{align*}
% 	& \mE_{P}[\phi \given \mathcal{A}] (1-\varepsilon_N) \overset{\mathrm{(i)}}{\leq} \mE_{P}[\phi \given \mathcal{A}] \mE_{P}[\mathds{1}(\mathcal{A})] \leq \mE_{P}[\phi] \overset{\mathrm{(ii)}}{\leq} \alpha  \\begin{align*}.5em]
% 	\Longleftrightarrow ~ & \mE_{P}[\phi \given \mathcal{A}] \overset{\mathrm{(iii)}}{=} \mE_{P}[\phi \given N_1 = n_1, N_2 = n_2] \leq \frac{\alpha}{1-\varepsilon_N},
% \end{align*}
% where step~(i) uses the inequality in \eqref{Eq: lower bound for the prob A}, step~(ii) holds by \citet[][Theorem 2 and Remark 4]{shah2020hardness} and step~(iii) uses the fact that $\phi$ only uses $n_1 + n_2$ observations as described before. Since $N$ was an arbitrary number greater than or equal to $2n$ and $\varepsilon_N \rightarrow 0$ as $N \rightarrow \infty$, we can conclude that $ \mE_{P}[\phi \given N_1 = n_1, N_2 = n_2] \leq \alpha$ for any $P \in \mathcal{P}_{1,M}$. \qed

\medskip 

\Cref{Theorem: negative result} clearly explains that it is necessary to impose additional assumptions (e.g., smoothness for distributions) in order to make the conditional two-sample problem feasible. In the next two sections, we explore two general frameworks, which implicitly or explicitly incorporate reasonable assumptions to address this problem. The first framework utilizes any conditional independence test and considers scenarios where this test performs well for verifying conditional independence. Conversely, the second framework assumes that the marginal density ratio $r_X$ is well-behaved and can be estimated with high accuracy.

\section{Approach via Conditional Independence Testing} \label{Section: Approach via Conditional Independence Testing}

In this section, we introduce our first framework that converts a conditional independence test to a conditional two-sample test, while maintaining the same asymptotic guarantees. The key idea is to construct a dataset $\mathcal{D}_{\tilde{n}}$ consisting of i.i.d.~random vectors $(Y,Z,X)$ of size $\tilde{n}$ based on the given two samples $\{(Y_i^{(1)}, X_i^{(1)})\}_{i=1}^{n_1}$ and $\{(Y_i^{(2)}, X_i^{(2)})\}_{i=1}^{n_2}$. To achieve this, letting $n = n_1 + n_2$, we first draw a random variable $\tilde{n}_1$ from Binomial($\tilde{n}$, $n_1/n$) where $\tilde{n}$ is set to be smaller than $n$ and $\tilde{n}/n \rightarrow 1$. Since a Binomial random variable is highly concentrated around its mean, we can guarantee that $\tilde{n}_1 \leq n_1$ and $\tilde{n}_2 \coloneqq  \tilde{n} - \tilde{n}_1 \leq n_2$ with high probability. If a bad event happens where either $\tilde{n}_1 > n_1$ or $\tilde{n}_2 > n_2$, making the construction of $\mathcal{D}_{\tilde{n}}$ infeasible, we simply accept the null hypothesis. This slightly inflates the type II error in finite-sample scenarios, but it is asymptotically negligible. A similar idea has been utilized in \citet{neykov2021minimax} in a different context to eliminate Poissonization for conditional independence testing.

Having constructed $\mathcal{D}_{\tilde{n}}$ consisting of i.i.d.~random samples drawn from the joint distribution of $(Y,Z,X)$, we can now implement a conditional independence test based on $\mathcal{D}_{\tilde{n}}$, while retaining the same theoretical guarantees for conditional two-sample testing. \Cref{Algorithm: Converting C2ST into CIT} summarizes this procedure, and the following theorem formally establishes its theoretical guarantees. 

% Denote -> denote
\begin{algorithm}[t!]
	\caption{Converting a Conditional Independence Test into a Conditional Two-Sample Test} \label{Algorithm: Converting C2ST into CIT}
	\begin{algorithmic}[1]
		\Require Data $\{(Y_i^{(1)}, X_i^{(1)})\}_{i=1}^{n_1}$ and $\{(Y_i^{(2)}, X_i^{(2)})\}_{i=1}^{n_2}$, a conditional independence test $\phi$ for $H_0: Y \independent Z \mid X$ of (asymptotic) size $\alpha \in (0,1)$, and an adjustment parameter $\varepsilon \in (0,1)$ \vskip .5em
		\State Let $n=n_1+n_2$. Draw $\tilde{n}_1 \sim \mathrm{Binomial}(\tilde{n}, n_1/n)$ where $\tilde{n} = k^\ast n$ and 
		$
			k^\ast = 1 - 3\log(\varepsilon)/(2n_1) - \sqrt{(1 - 3\log(\varepsilon)/(2n_1))^2 - 1}
		$.
		Set $\tilde{n}_2 = \tilde{n} - \tilde{n}_1$. \vskip .2em 
		\If {$\tilde{n}_1 > n_1$ or $\tilde{n}_2 > n_2$} Accept $H_0$.
		\Else
		\State Merge $\{(X_i^{(1)}, Y_i^{(1)}, Z_i=1)\}_{i=1}^{\tilde{n}_1}$ and $\{(X_i^{(2)}, Y_i^{(2)}, Z_i=2)\}_{i=1}^{\tilde{n}_2}$, yielding $\mathcal{D}_{\tilde{n}} \coloneqq  \{(X_i, Y_i, Z_i)\}_{i=1}^{\tilde{n}}$.
		\State Run a conditional independence test $\phi$ on $\mathcal{D}_{\tilde{n}}$ at level $\alpha$, and denote the resulting test as $\phi_{\tilde{n}}$.
		\If {$\phi_{\tilde{n}} = 1$} Reject $H_0$ \textbf{else} Accept $H_0$.
		\EndIf
		\EndIf
	\end{algorithmic}
\end{algorithm}

\begin{theorem} \label{Theorem: Converting C2ST into CIT}
\revised{Let $n_{1},n_{2}\in \mathbb{N}$ with $n = n_{1}+n_{2}$. Suppose we observe two independent samples 
\begin{align*}
	\{(X_{i}^{(1)},Y_{i}^{(1)})\}_{i=1}^{n_1}\iid P_{XY}^{(1)} \quad \text{and} \quad \{(X_{i}^{(2)},Y_{i}^{(2)})\}_{i=1}^{n_2}\iid P_{XY}^{(2)},
\end{align*}
where $(P_{XY}^{(1)},P_{XY}^{(2)}) \in \mathcal{P}$ for some class $\mathcal{P}$ of distribution pairs.
Let $\mathcal{P}_{0}\subset\mathcal{P}$ and $\mathcal{P}_{1}\subset\mathcal{P}$ denote disjoint subclasses corresponding to the null and alternative distribution classes, respectively. 
Construct the joint distribution $P_{XYZ}$ from $(P_{XY}^{(1)},P_{XY}^{(2)})$ as in \Cref{Joint pdf}. 
For $\{(X_i, Y_i, Z_i)\}_{i=1}^n\iid P_{XYZ}$, consider a conditional independence test $\phi : \{(X_i, Y_i, Z_i)\}_{i=1}^n\;\to\; \{0,1\}$ such that for any $\alpha \in (0,1)$,
\begin{align*}
    \lim_{n \to \infty} \sup_{P \in \mathcal{P}_0} \mE_P[\phi] \leq \alpha,
    \qquad
    \lim_{n \to \infty} \sup_{P \in \mathcal{P}_1} \mE_P[1-\phi] = 0.
\end{align*}
Let $\tilde{\phi}$ denote the test obtained by applying \Cref{Algorithm: Converting C2ST into CIT} 
with adjustment parameter $\varepsilon=o(1)$, where $\tilde{\phi}=1$ if and only if $H_{0}$ is rejected. 
Then $\tilde{\phi}$ preserves the same asymptotic guarantees:
\begin{align*}
    \lim_{n \to \infty} \sup_{P \in \mathcal{P}_0} \mE_P[\tilde{\phi}] \leq \alpha,
    \qquad
    \lim_{n \to \infty} \sup_{P \in \mathcal{P}_1} \mE_P[1-\tilde{\phi}] = 0.
\end{align*}}
\end{theorem}
\noindent \textbf{Remark.} 
\revised{The guarantees of \Cref{Theorem: Converting C2ST into CIT} should be interpreted as asymptotic. Our analysis relies on approximating the test statistic via \Cref{Algorithm: Converting C2ST into CIT}, which leaves an $o(1)$ remainder. In addition, the adjustment parameter $\varepsilon$ contributes explicit additive errors to the bounds for both type~I and type~II errors. Both effects vanish only in the limit as $n \to \infty$ and $\varepsilon = \varepsilon_n \to 0$, so exact finite-sample guarantees cannot be provided.}


% \begin{theorem} \label{Theorem: Converting C2ST into CIT}
% Let $n_1, n_2 \in \mathbb{N}$ with $n = n_1 + n_2$ and $\alpha \in (0,1)$. 
% Consider distributions of $(X,Y,Z)$ where $Z \in \{1,2\}$ takes values with probabilities $n_1/n$ and $n_2/n$, respectively. 
% For $\{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1} \iid P_{XY}^{(1)}$ and $\{(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{n_2} \iid P_{XY}^{(2)}$, define $P := \frac{n_1}{n}\,P_{XY}^{(1)} \otimes \delta_{\{Z=1\}} + \frac{n_2}{n}\,P_{XY}^{(2)} \otimes \delta_{\{Z=2\}},$ as in \Cref{Theorem: negative result}. Let $\mathcal{P}$ be the class of such distributions, and let $\mathcal{P}_0, \mathcal{P}_1 \subset \mathcal{P}$ be disjoint subclasses. 
% Suppose a test $\phi : \{(X_i,Y_i,Z_i)\}_{i=1}^n \mapsto \{0,1\}$ satisfies
% \begin{align*}
%     \lim_{n \to \infty} \sup_{P \in \mathcal{P}_0} \mathbb{E}_P[\phi] \leq \alpha,
%     \quad
%     \lim_{n \to \infty} \sup_{P \in \mathcal{P}_1} \mathbb{E}_P[1 - \phi] = 0.
% \end{align*}

% Let $\{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^n \sim P$ with $N_1 := \sum_{i=1}^n \mathds{1}(\tilde{Z}_i=1) = n_1$ and $N_2 := \sum_{i=1}^n \mathds{1}(\tilde{Z}_i=2) = n_2$. 
% Denote by $\tilde{\phi}$ the output of \Cref{Algorithm: Converting C2ST into CIT} with $\varepsilon = o(1)$, where $\tilde{\phi} = 1$ if and only if $H_0$ is rejected. 
% Then
% \begin{align*}
%     \lim_{n \to \infty} \sup_{P \in \mathcal{P}_0} \mathbb{E}_P[\tilde{\phi}] \leq \alpha,
%     \quad
%     \lim_{n \to \infty} \sup_{P \in \mathcal{P}_1} \mathbb{E}_P[1 - \tilde{\phi}] = 0.
% \end{align*}
% \end{theorem}


% \begin{theorem} \label{Theorem: Converting C2ST into CIT}
% 	\sloppy Consider a class of distributions $\mathcal{P}$ of $(X,Y,Z)$ where $Z$ takes a value among $\{1,2\}$ with probability $n_1/n$ and $n_2/n$, respectively, and let $\{(X_i,Y_i,Z_i)\}_{i=1}^n \iid P \in \mathcal{P}$. For disjoint subclasses $\mathcal{P}_0 \subset \mathcal{P}$ and $\mathcal{P}_1 \subset \mathcal{P}$ and $\alpha \in (0,1)$, assume that a test $\phi : \{(X_i,Y_i,Z_i)\}_{i=1}^n \mapsto \{0,1\}$ satisfies  
% 	\begin{align*}
% 		\lim_{n \rightarrow \infty} \sup_{P \in \mathcal{P}_0}\mE_P[\phi] \leq \alpha \quad \text{and} \quad \lim_{n \rightarrow \infty} \sup_{P \in \mathcal{P}_1}\mE_P[1 - \phi]  = 0.
% 	\end{align*}
% 	Denote the output of \Cref{Algorithm: Converting C2ST into CIT} with $\varepsilon = o(1)$ as $\tilde{\phi} \in \{0,1\}$ where $\tilde{\phi} = 1$ if and only if $H_0$ is rejected. Then it holds that 
% 	\begin{align*}
% 		\lim_{n \rightarrow \infty} \sup_{P \in \mathcal{P}_0}\mE_P[\tilde{\phi} \given N_1 = n_1, N_2 = n_2] \leq \alpha \quad \text{and} \quad 	\lim_{n \rightarrow \infty} \sup_{P \in \mathcal{P}_1}\mE_P[1 - \tilde{\phi} \given N_1 = n_1, N_2 = n_2]  = 0,
% 	\end{align*}
% 	where we recall $N_1 = \sum_{i=1}^n \mathds{1}(Z_i = 1)$ and $N_2 = \sum_{i=1}^n \mathds{1}(Z_i = 2)$.
% \end{theorem}

% \begin{theorem} \label{Theorem: Converting C2ST into CIT}
% \sloppy Consider a class of pairs of distributions $\mathcal{P}$ on $\mathcal{X} \times \mathcal{Y}$, and let $\{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1} \sim P^{(1)} \in \mathcal{P}$ and $\{(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{n_2} \sim P^{(2)} \in \mathcal{P}$ be independent samples, with $n = n_1 + n_2$. For disjoint subclasses $\mathcal{P}_0 \subset \mathcal{P}$,$\mathcal{P}_1 \subset \mathcal{P}$ and $\alpha \in (0,1)$, assume that a test $\phi : \{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^{\tilde{n}} \mapsto \{0,1\}$ satisfies 
% \begin{align*}
% \lim_{n \rightarrow \infty} \sup_{{(P^{(1)},P^{(2)}) \in \mathcal{P}_0}}\mE[\phi] \leq \alpha \quad \text{and} \quad \lim_{n \rightarrow \infty} \sup_{(P^{(1)},P^{(2)}) \in \mathcal{P}1}\mE[1 - \phi] = 0.
% \end{align*}
% Denote the output of Algorithm 1 with $\varepsilon = o(1)$ as $\tilde{\phi} \in \{0,1\}$ where $\tilde{\phi} = 1$ if and only if $H_0$ is rejected. Then it holds that
% \begin{align*}
% \lim_{n \rightarrow \infty} \sup_{(P^{(1)},P^{(2)}) \in \mathcal{P}_0}\mE[\tilde{\phi}] \leq \alpha \quad \text{and} \quad \lim_{n \rightarrow \infty} \sup_{(P^{(1)},P^{(2)}) \in \mathcal{P}_1}\mE[1 - \tilde{\phi}] = 0.
% \end{align*}
% \end{theorem}

\noindent \textbf{Proof of \Cref{Theorem: Converting C2ST into CIT}.} 
\revised{We may write $\tilde{\phi} = \mathds{1}(\tilde{n}_1 \leq n_1)\mathds{1}(\tilde{n}_2 \leq n_2)\phi_{\tilde{n}},$ where $\phi_{\tilde{n}}$ is defined as $\phi$ based on $\mathcal{D}_{\tilde{n}} = \{(X_i,Y_i,Z_i)\}_{i=1}^{\tilde{n}}$ in \Cref{Algorithm: Converting C2ST into CIT}. 
For convenience, we denote $P := P_{XYZ}$.} Now generate new i.i.d.~samples $\tilde{\mathcal{D}}_{\tilde{n}} = \{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^{\tilde{n}} \iid P$ independent of $\mathcal{D}_{\tilde{n}}$ and define as
\begin{align*}
	\tilde{\phi}^\dagger = \mathds{1}\biggl\{\sum_{i=1}^{\tilde{n}} \mathds{1}(\tilde{Z}_i=1) \leq n_1 \biggr\} \mathds{1}\biggl\{\sum_{i=1}^{\tilde{n}} \mathds{1}(\tilde{Z}_i=2) \leq n_2 \biggr\} \phi_{\tilde{n}}^\dagger,
\end{align*}
where $\phi_{\tilde{n}}^\dagger$ is defined as $\phi$ but based on $\tilde{\mathcal{D}}_{\tilde{n}}$.
\revised{Although the distributions of $\tilde{\phi}$ (from subsampling the original dataset) 
and $\tilde{\phi}^\dagger$ (from new i.i.d.~samples) are not identical, the difference vanishes as $\tilde{n}/n \to 1$. Hence, for every $P$,
\begin{align*}
    \mE_P[\tilde{\phi}] = \mE_P[\tilde{\phi}^\dagger] + o(1).
\end{align*}
Moreover, by construction $\mE_P[\tilde{\phi}^\dagger] \leq \mE_P[\phi_{\tilde{n}}^\dagger]$ holds. Therefore
\begin{align*}
    \mE_P[\tilde{\phi}] \leq \mE_P[\phi_{\tilde{n}}^\dagger] + o(1), 
\end{align*}
for all $P$. In other words, we can analyze $\tilde{\phi}$ by working with $\tilde{\phi}^\dagger$ up to 
an asymptotically negligible $o(1)$ term, which suffices for type~I error control.
}
% We may write $\tilde{\phi} = \mathds{1}(\tilde{n}_1 \leq n_1) \mathds{1}(\tilde{n}_2 \leq n_2) \phi_{\tilde{n}}$ where $\phi_{\tilde{n}}$ is defined as $\phi$ based on $\mathcal{D}_{\tilde{n}} = \{(X_i,Y_i,Z_i)\}_{i=1}^{\tilde{n}}$ in \Cref{Algorithm: Converting C2ST into CIT}. \revised{For convenience, we denote $P := P_{XYZ}$}. Now generate new i.i.d.~samples $\tilde{\mathcal{D}}_{\tilde{n}} = \{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^{\tilde{n}} \iid P$ independent of $\mathcal{D}_{\tilde{n}}$ and define as
% \begin{align*}
% 	\tilde{\phi}^\dagger = \mathds{1}\biggl\{\sum_{i=1}^{\tilde{n}} \mathds{1}(\tilde{Z}_i=1) \leq n_1 \biggr\} \mathds{1}\biggl\{\sum_{i=1}^{\tilde{n}} \mathds{1}(\tilde{Z}_i=2) \leq n_2 \biggr\} \phi_{\tilde{n}}^\dagger,
% \end{align*}
% where $\phi_{\tilde{n}}^\dagger$ is defined as $\phi$ but based on $\tilde{\mathcal{D}}_{\tilde{n}}$. Observe that the conditional distribution of $\tilde{\phi}$ given $N_1 = n_1, N_2 = n_2$ is the identical to the marginal distribution of $\tilde{\phi}^\dagger$. Thus it can be seen that 
% \begin{align*}
% 	\mE_P\bigl[ \tilde{\phi}  \given N_1 = n_1, N_2 = n_2 \bigr] =  \mE_P[\tilde{\phi}^\dagger] \leq \mE_P[ \phi_{\tilde{n}}^\dagger],
% \end{align*}
% for all $P$. In other words, we can effectively remove conditioning on $N_1 = n_1, N_2 = n_2$ by working with $\tilde{\phi}^\dagger$. Therefore, the first claim on type I error control follows. 
Moving to the type II error, observe that 
\begin{align*}
	\mathds{1}(\tilde{n}_1 \leq n_1) \mathds{1}(\tilde{n}_2 \leq n_2)\phi_{\tilde{n}}  & = \phi_{\tilde{n}}  -  \mathds{1}(\tilde{n}_1 > n_1 ~ \text{or} ~ \tilde{n}_2 > n_2)\phi_{\tilde{n}}  \\
	& \geq \phi_{\tilde{n}}  -  \mathds{1}(\tilde{n}_1 > n_1) - \mathds{1}(\tilde{n}_2 > n_2),
\end{align*}
by the union bound, which yields 
\revised{
\begin{align} \label{Eq: type II error bound}
	\mE_P[1 - \tilde{\phi} ] \leq \mE_P[1 - \phi_{\tilde{n}}^\dagger]  + \mE_P[\mathds{1}(\tilde{n}_1 > n_1)] + \mE_P[\mathds{1}(\tilde{n}_2 > n_2)] + o(1).
\end{align}}
% \begin{align} \label{Eq: type II error bound}
% 	\mE_P[1 - \tilde{\phi}  \given N_1 = n_1, N_2 = n_2  ] = \mE_P[1 - \tilde{\phi}^\dagger ] \leq \mE_P[1 - \phi_{\tilde{n}}^\dagger]  + \mE_P[\mathds{1}(\tilde{n}_1 > n_1)] + \mE_P[\mathds{1}(\tilde{n}_2 > n_2)].
% \end{align}
In addition, letting $p = n_1/n$ in \Cref{Lemma: Concentration inequality} of \Cref{Section: Supporting Lemmas}, take $(1+\delta)\tilde{n}n_1/n = k(1+\delta)n_1 = n_1$, which gives $1 + \delta = k^{-1} \Longleftrightarrow \delta = k^{-1} - 1$. Thus, by \Cref{Lemma: Concentration inequality}, we have 
\begin{align*}
	\mE_P[\mathds{1}(\tilde{n}_1 > n_1)] \leq \exp\biggl( - \frac{n_1k(k^{-1}-1)^2}{3} \biggr).
\end{align*}
Letting the right-hand side equal $\varepsilon$ and solving for $k \in (0,1)$, we derive the form of $k^{\ast}$ as presented in \Cref{Algorithm: Converting C2ST into CIT}, which shows that $\mE_P[\mathds{1}(\tilde{n}_1 > n_1)] \leq \varepsilon$. By symmetry, the same analysis holds for the inequality $\mE_P[\mathds{1}(\tilde{n}_2 > n_2)] \leq \varepsilon$. As a result, continuing from the inequality~\eqref{Eq: type II error bound}, we can upper bound the type II error of 
\revised{$\tilde{\phi}$ (the level-$\alpha$ test obtained from Algorithm~\ref{Algorithm: Converting C2ST into CIT})} as
\revised{
\begin{align*}
	\mE_P[ 1 - \tilde{\phi}] \leq   \mE_P[1 - \phi_{\tilde{n}}^\dagger]  + 2\varepsilon + o(1). 
\end{align*}	
}
% \begin{align*}
% 	\mE_P[ 1 - \tilde{\phi} \given N_1 =  n_1, N_2 = n_2] \leq   \mE_P[1 - \phi_{\tilde{n}}^\dagger]  + 2\varepsilon. 
% \end{align*}	
Since $\varepsilon = o(1)$, the above display proves the second claim on type II error control. This completes the proof of \Cref{Theorem: Converting C2ST into CIT}.  \qed 


\medskip


Our analysis in \Cref{Theorem: Converting C2ST into CIT} is not limited to conditional two-sample testing, and it can be applied to marginal two-sample testing as well. Indeed, the problem of conditional two-sample testing becomes equivalent to the unconditional counterpart when $X$ is degenerate (e.g., $X =0$ with probability one). Thus, our algorithm serves as a generic method to convert unconditional independence tests to unconditional two-sample tests as well. We also mention that a specific form of $k^\ast$ in \Cref{Algorithm: Converting C2ST into CIT} is derived from the multiplicative Chernoff bound for a Binomial random variable (\Cref{Lemma: Concentration inequality}), which can be refined by numerically computing the tail probability of a Binomial random variable.



Despite its generality, one obvious drawback of \Cref{Algorithm: Converting C2ST into CIT} is that it does not take the datasets $\{(Y_i^{(1)}, X_i^{(1)})\}_{i= \tilde{n}_1+1}^{n_1}$ and $\{(Y_i^{(2)}, X_i^{(2)})\}_{i= \tilde{n}_2+1}^{n_2}$ into account in the procedure when $\tilde{n}_1 < n_1$ and $\tilde{n}_2 < n_2$. It can be seen that the expected number of discarded samples, i.e., $\mE[n - \tilde{n}_1 -\tilde{n}_2]$, is $O\bigl(\sqrt{n\log(1/\varepsilon)}\bigr)$. This loss might degrade the performance in small-sample size regimes, but it can be negligible when $n$ is sufficiently large and $\varepsilon$ decreases slowly (see \Cref{Appendix: With_Without_Algorithm_1,Appendix: Sensitivity Analysis by epsilon} for empirical support). Nevertheless, when a test statistic is sufficiently stable \revised{(formally defined in \Cref{def:stability})} the conclusion of \Cref{Theorem: Converting C2ST into CIT} may hold without further modification of $\phi$, meaning the conditional testing errors of $\phi$ are asymptotically equivalent to its marginal errors. 

To illustrate this, we build upon the coupling argument presented by \citet{chung2013}. First, draw $\bar{n}_1 \sim \mathrm{Binomial}(n,n_1/n)$ and set $\bar{n}_2 = n - \bar{n}_1$. If $\bar{n}_1 > n_1$, draw $\bar{n}_1 - n_1$ additional samples $\{(Y_i^{(1)}, X_i^{(1)})\}_{i=n_1+1}^{\bar{n}_1}$ from $P_{XY}^{(1)}$. Otherwise, draw $\bar{n}_2 - n_2$ additional samples $\{(Y_i^{(2)}, X_i^{(2)})\}_{i=n_2+1}^{\bar{n}_2}$ from $P_{XY}^{(2)}$. In either case, set $\{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^n  = \{X_i^{(1)},Y_i^{(1)}, 1\}_{i=1}^{\bar{n}_1} \cup \{X_i^{(2)},Y_i^{(2)}, 2\}_{i=1}^{\bar{n}_2}$, which can be viewed as i.i.d.~draws from a joint distribution of $(X,Y,Z)$, after randomly permuting indices. When this newly constructed dataset is compared with the original dataset $\{(X_i,Y_i,Z_i)\}_{i=1}^n = \{X_i^{(1)},Y_i^{(1)}, 1\}_{i=1}^{n_1} \cup \{X_i^{(2)},Y_i^{(2)}, 2\}_{i=1}^{n_2}$, there are $|\bar{n}_1 - n_1|$ distinct data points with the expectation $\mE[|\bar{n}_1 - n_1|] \leq \sqrt{n/4}$. 

\begin{definition}[Stability of a test statistic] \label{def:stability}
\revised{Let $T_n$ be a test statistic computed on the original dataset 
$\{(X_i,Y_i,Z_i)\}_{i=1}^n$, 
and let $\tilde{T}_n$ denote its analogue computed on the perturbed dataset 
$\{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^n$ 
constructed by the above coupling scheme. 
We say that $T_n$ is \emph{(asymptotically) stable} if 
\begin{align*}
\lim_{n \to \infty} \mP\bigl(|T_n - \tilde{T}_n| > \epsilon \bigr) = 0
\quad \text{for every } \epsilon > 0.    
\end{align*}
In this case, $T_n$ and $\tilde{T}_n$ are said to be \emph{asymptotically equivalent}.}
\end{definition}
\revised{This suggests that if a test statistic $T_n$ is asymptotically invariant to $\sqrt{n}$-data perturbations, its asymptotic behavior remains consistent across both the original and the newly constructed datasets. Formally, let $\tilde{T_n}$ denote the analogous test statistic computed on the perturbed dataset $\{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^n$. 
However, whether a given $T_n$ is stable must be assessed on a case-by-case basis. 
Below, we provide examples illustrating both stable and unstable cases.}

% This suggests that if a test statistic $T$ is asymptotically invariant to $\sqrt{n}$-data perturbations, the asymptotic behavior of $T$ remains consistent across both the original and the newly constructed datasets. However, the stability of $T$ needs to be evaluated on a case-by-case basis. Below, we provide examples illustrating both stable and unstable cases.


\begin{example}[Stable case] \label{Example : Stable case}\normalfont
	To simplify our presentation, consider a univariate case of $Y \in \mathbb{R}$, and assume $f(x) \coloneqq  \mE[Y \given X = x]$ and $g(x) \coloneqq  \mE[Z \given X =x]$ are known. Letting $R_i \coloneqq  \{Y_i - f(X_i)\} \{Z_i - g(X_i)\}$, the generalized covariance measure introduced by \cite{shah2020hardness} is 
	\begin{align*}
		T_n = \frac{\frac{1}{\sqrt{n}}\sum_{i=1}^n R_i}{\big\{ \frac{1}{n} \sum_{i=1}^n R_i^2 - \bigl(\frac{1}{n} \sum_{r=1}^n R_r\bigr)^2  \big\}^{1/2}},
	\end{align*}
	and let $\tilde{T}_n$ be similarly defined as $T_n$ based on $\{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^n$. Focusing on the numerators of $T_n$ and $\tilde{T}_n$, it can be seen that their difference is 
	\begin{align*}
		\frac{1}{\sqrt{n}} \sum_{i=1}^n R_i - \frac{1}{\sqrt{n}} \sum_{i=1}^n \tilde{R}_i = \frac{1}{\sqrt{n}} \sum_{i= n_1 + 1}^{\bar{n}_1} (R_i - \tilde{R}_i) \cdot \mathds{1}(\bar{n}_1 > n_1) + \frac{1}{\sqrt{n}} \sum_{i= \bar{n}_1 + 1}^{n_1} (R_i - \tilde{R}_i) \cdot \mathds{1}(\bar{n}_1 \leq n_1).
	\end{align*}
	 Under the null hypothesis, the expectation of the difference is zero and the variance is bounded above by $1/\sqrt{n}$ up to a constant, provided that each $Y_i$ has a finite second moment. Therefore, the difference of the numerators is asymptotically negligible. We can show similarly that the difference of the denominators is also asymptotically negligible as detailed in \Cref{Appendix : Proof of Example : Stable case}. \revised{Putting thing together concludes that $T_{n}$ is stable in the sense of \Cref{def:stability}.}
\end{example}
In the above example, we assumed that the conditional expectations $f$ and $g$ are known. In practice, $f$ and $g$ are estimated from the data, and the generalized covariance measure can become highly unstable when the estimators of $f$ and $g$ are themselves unstable.
\begin{example}[Unstable case] \normalfont
	Consider an extreme case where the estimators $\hat{f}$ and $\hat{g}$ are defined as follows: $\hat{f}(X_i) \coloneqq  Y_i \mathds{1}(\sum_{i=1}^n Z_i = n_1) + f(X_i) \mathds{1}(\sum_{i=1}^n Z_i \neq n_1)$ and $\hat{g}(X_i) \coloneqq  Z_i \mathds{1}(\sum_{i=1}^n Z_i = n_1) + g(X_i) \mathds{1}(\sum_{i=1}^n Z_i \neq n_1)$. In this case, $T_n$ is not well-defined as it takes the form $0/0$ deterministically. On the other hand, when $n_1$ and $n_2$ are well-balanced (e.g., $n_1/n=1/2$), the probability of the event $\sum_{i=1}^n \tilde{Z}_i = n_1$ converges to zero as $n$ increases. Under such condition and assuming suitable moment conditions, the test statistic $\tilde{T}_n$ based on $\{(\tilde{X}_i, \tilde{Y}_i, \tilde{Z}_i)\}_{i=1}^n$ can still converge to a Gaussian limit. This example illustrates that the limiting behavior of $T$ and $\tilde{T}_n$ can differ significantly, which is attributed to the instability of the estimators $\hat{f}$ and $\hat{g}$. \revised{In fact, $T_n$ is \emph{not stable} in the sense of \Cref{def:stability}.} 
\end{example}
The previous examples highlight the need for caution when converting a conditional independence test to a conditional two-sample test, and also justify our generic approach to converting a conditional independence test to a conditional two-sample test in \Cref{Algorithm: Converting C2ST into CIT}. The next section introduces another general framework for conditional two-sample testing based on density ratio estimation.



\section{Approach via Density Ratio Estimation} \label{Section: Approach via Density Ratio Estimation}
In this section, we present our second framework, which transforms the problem of conditional two-sample testing into one that involves comparing marginal distributions via density ratio estimation. Concretely, we recall from \eqref{Eq: C2ST = Marginal 2ST} that the null hypothesis of equality of two conditional distributions holds if and only if $f_{YX}^{(1)} = f_{YX}$ where $f_{YX} = r_X \cdot f_{YX}^{(2)}$ and $r_X$ is the density ratio defined in \eqref{Eq: density ratio}. A challenge when applying this approach is that we only have samples from $f_{YX}^{(2)}$, and not from $f_{YX}$, which makes it impossible to directly compare samples from $f_{YX}^{(1)}$ with those from $f_{YX}$. However, once the density ratio is known or accurately estimated, we can effectively correct the bias arising from the difference between $f_{YX}^{(2)}$ and $f_{YX}$ in various test statistics, frequently used for marginal two-sample testing. To facilitate our discussion, we first assume that the density ratio $r_X$ is known and provide a detailed analysis on how to deal with the unknown case by focusing on a few cases.

At the core of our idea is importance weighting~\citep[][for a survey]{kimura2024short}, a technique that assigns different levels of importance to data points to correct biases and prioritize relevant data. For instance, suppose we would like to estimate the expectation of $X$ under the distribution $P$ with density $p$, while we only observe data $X_1,\ldots,X_n$ from another distribution $Q$ with density $q$. Then by re-weighting data points using the density ratio $p/q$, we can obtain an unbiased estimator of the expectation under $P$ as 
\begin{align*}
	\frac{1}{n} \sum_{i=1}^n \frac{p(X_i)}{q(X_i)} X_i \quad \text{such that} \quad \mE_Q \biggl[\frac{1}{n} \sum_{i=1}^n \frac{p(X_i)}{q(X_i)} X_i\biggr] = \mE_P[X].
\end{align*}
This idea can be applied to a range of marginal two-sample test statistics as we demonstrate below. Throughout this section, we use the shorthand $V^{(1)} \coloneqq  (X^{(1)},Y^{(1)})$ and $V^{(2)} \coloneqq  (X^{(2)},Y^{(2)})$ to simplify the notation. 
 
\begin{example} \normalfont 
	\textbf{(Mean comparison)} We start with a simple case of comparing the mean of transformed samples. Given a feature map $\psi : \mathcal{X} \times \mathcal{Y} \mapsto \mathbb{R}$, one can consider 
	\begin{align*}
		\frac{1}{n_1} \sum_{i=1}^{n_1} \psi\bigl(V_i^{(1)}\bigr) - \frac{1}{n_2} \sum_{i=1}^{n_2} r_X\bigl(X_i^{(2)}\bigr)\psi\bigl(V_i^{(2)}\bigr)
	\end{align*}
    as a test statistic for the hypotheses in \eqref{Eq: hypothesis}. The expectation of this statistic is equal to zero under the null hypothesis. Moreover, since the test statistic is simply a linear combination of independent random variables, it can be calibrated using the Gaussian approximation.
\end{example}

\begin{example} \normalfont \label{Example: Rank sum statistic}
	\textbf{(Rank sum statistic)} Instead of comparing the mean, one can compare the stochastic order of two distributions using ranks. Specifically, given a feature map $\psi : \mathcal{X} \times \mathcal{Y} \mapsto \mathbb{R}$, a rank sum statistic based on the transformed samples can be computed as 
	\begin{align*}
		\frac{1}{n_1n_2} \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} r_X\bigl(X_j^{(2)}\bigr)\mathds{1}\bigl\{ \psi\bigl(V_{j}^{(2)}\bigr) < \psi\bigl(V_{i}^{(1)}\bigr)\bigr\}.
	\end{align*}
	Under the null hypothesis, and assuming no ties among transformed samples, it can be seen that the expectation is equal to \revised{1/2}. As in \cite{hu2024two}, the test statistic can be shown to be asymptotically Gaussian using the asymptotic theory of U-statistics under conditions. Therefore, the critical value can be determined based on this Gaussian approximation. The power, however, changes depending on $\psi$. \citet{hu2024two} takes $\psi$ as an estimate of $f_{Y\sgiven X}^{(1)}(\cdot \given \cdot) /  f_{Y\sgiven X}^{(2)}(\cdot \given \cdot)$.
\end{example}

\begin{example} \normalfont 
	\textbf{(Classifier-based approach)} Let $\mathcal{H}$ be a class of classifiers. Given a binary classifier $h: \mathcal{X} \times \mathcal{Y} \mapsto \{1,2\}$ where $h \in \mathcal{H}$,  $\ell: \mathbb{R} \times \{1,2\} \mapsto \mathbb{R}$ is a loss function that measures the difference between the predicted value and the true output. The core idea behind classifier-based two-sample tests~\citep{lopez2017revisiting,kim2021classification,hediger2022use} is that when the null hypothesis of equality of distributions is true, any classifier will return a random guess. On the other hand, when two distributions are significantly different, the accuracy of a reasonable classifier would be greater than chance level. Therefore, empirical classification accuracy can serve as an effective test statistic. However, since we do not observe a sample from $f_{XY}$ but a sample from $f_{YX}^{(2)}$, we need to take this into consideration when we train a classifier. Specifically, we compute a classifier
	\begin{align} \label{Eq: classifier}
		\widehat{h} = \argmin_{h \in \mathcal{H}}\biggl\{ \frac{1}{n_1} \sum_{i=1}^{n_1} \ell\bigl(h\bigl(V_i^{(1)}\bigr), \revised{1}\bigr) + \frac{1}{n_2} \sum_{i=1}^{n_2} r_X\bigl(X_i^{(2)}\bigr) \ell\bigl(h\bigl(V_i^{(2)}\bigr), \revised{2}\bigr) \biggr\},
    \end{align}
	and use the empirical  classification accuracy of $\widehat{h}$, again corrected by the density ratio, as our test statistic. When training and testing are performed on independent datasets, the asymptotic null distribution of the classification accuracy is approximately Gaussian~\citep{kim2021classification,hediger2022use}; thereby the critical value can be determined based on this Gaussian approximation. We provide a detailed analysis of this approach in \Cref{Section: Classifier-based Approach}, and present numerical results in \Cref{Section: Numerical Experiments}.
\end{example}

\begin{example} \normalfont \label{Example : Quadratic time MMD}
	\textbf{(Kernel MMD)} The last example is a kernel MMD statistic~\citep{gretton2012kernel}. Given a kernel $k$, the population MMD compares the kernel mean embeddings of two distributions with density functions $f_{YX}^{(1)}$ and $f_{YX}$, respectively. In a kernel form, the squared MMD can be written as 
	\begin{align*}
		\mathrm{MMD}^2 = \mE[k(V_1^{(1)}, V_2^{(1)})] + \mE[r_X\bigl(X_1^{(2)}\bigr)r_X\bigl(X_2^{(2)}\bigr)k(V_1^{(2)}, V_2^{(2)})] - 2 \mE[r_X\bigl(X_2^{(2)}\bigr)k(V_1^{(1)}, V_2^{(2)})],
	\end{align*}
	where the bias is corrected via importance weighting. The squared MMD can be estimated as 
	\begin{align*}
		\frac{1}{n_1(n_1-1)}\sum_{1 \leq i \neq j \leq n_1} & k\bigl(V_i^{(1)},V_j^{(1)}\bigr) + \frac{1}{n_2(n_2-1)}\sum_{1 \leq i \neq j \leq n_2} r_X\bigl(X_i^{(2)}\bigr)r_X\bigl(X_j^{(2)}\bigr)k\bigl(V_i^{(2)},V_j^{(2)}\bigr) \\
		& - \frac{2}{n_1n_2}\sum_{i=1}^{n_1}\sum_{j=1}^{n_2} r_X\bigl(X_j^{(2)}\bigr) k\bigl(V_i^{(1)},V_j^{(2)}\bigr), 
	\end{align*}
	which is an unbiased estimator of the population MMD. Unlike the other three test statistics mentioned earlier, this estimator converges to an infinite sum of weighted chi-squared distributions, whose weights are unknown in practice. This was not a major issue for marginal two-sample testing, where permutation tests could calibrate any test statistic in a nonparametric way. However, the standard permutation approach is no longer valid for conditional two-sample testing, which presents a challenge. Therefore, we focus on another estimator, a linear-time MMD statistic, in \Cref{Section: Linear-Time MMD}, which offers advantages in both the tractability of the asymptotic distribution and computational efficiency.
\end{example}

The preceding discussion assumes that the marginal density ratio $r_X$ is known. As mentioned before, the success of this approach hinges on accurately estimating the density ratio $r_X$. To this end, one can draw upon a wide range of existing techniques in the literature for density ratio estimation~\citep[e.g.,][]{sugiyama2010density, sugiyama2012density} to obtain a reliable testing result. Using the same dataset for both density ratio estimation and other parts of a statistic often results in plug-in bias. Hence, we recommend using auxiliary dataset obtained through, e.g., sample splitting, to estimate the density ratio. We concretely illustrate this approach in \Cref{Section: Classifier-based Approach} and \Cref{Section: Linear-Time MMD}, using a classifier-based test statistic and a linear-time MMD statistic.  

While various existing tools for density ratio estimation offer flexibility, an inherent drawback of this approach is that the behavior of the test statistic could be erratic when the density ratio is irregular and potentially unbounded. This issue can be mitigated by clipping the density ratio estimate at a certain value or shrinking it to zero~\citep[e.g.,][]{shimodaira2000improving}. Nevertheless, when prior knowledge indicates that the density ratio behaves poorly or is difficult to estimate, the testing performance may degrade (see \Cref{Section: Numerical Experiments} for numerical results) and thus this approach should be used with caution. 





\subsection{Classifier-based Approach} \label{Section: Classifier-based Approach}
This subsection illustrates a classifier-based test for conditional two-sample testing. To simplify the presentation, we assume that $n_1 = n_2 = 2n$ and split the dataset into two: $D_{a} \coloneqq  \{V_i^{(1)}\}_{i=1}^{n} \cup \{V_i^{(2)}\}_{i=1}^{n}$ and $D_{b} \coloneqq  \{V_i^{(1)}\}_{i=n+1}^{2n} \cup \{V_i^{(2)}\}_{i=n+1}^{2n}$. For some positive integer $m < n$, we further divide $D_a$ as $D_a^{\ast} \coloneqq  \{V_i^{(1)}\}_{i=1}^{m} \cup \{V_i^{(2)}\}_{i=1}^{m}$ and \revised{$D_a^{\ast\ast} \coloneqq  D_a  \setminus D_a^{\ast}$}, and let $\widehat{r}_X$ denote an estimator of $r_X$ formed on $D_a^{\ast\ast}$. Additionally, let $\widehat{h}$ be a classifier trained as in \eqref{Eq: classifier} based on $D_b$. Let us write $\widehat{A}_{1,i} \coloneqq   \mathds{1}\{\widehat{h}(V_i^{(1)})=1\}$ and $\widehat{A}_{2,i} \coloneqq  \widehat{r}_X(X_i^{(2)}) \mathds{1}\{\widehat{h}(V_i^{(2)})=2\}$ for $i \in \{1,\ldots,m\}$, and define $\overline{A}_1 \coloneqq  m^{-1} \sum_{i=1}^m \widehat{A}_{1,i}$ and $\overline{A}_2 \coloneqq  m^{-1} \sum_{i=1}^m \widehat{A}_{2,i}$. The population-level classification accuracy of $\widehat{h}$ is $\mP\{\widehat{h}(V^{(1)})=1\}/2 + \mE[r_X(X^{(2)}) \mathds{1}\{\widehat{h}(V^{(2)})=2\}]/2$, which is $1/2$ for any classifier $\widehat{h}$ under the null hypothesis. This observation leads to a classifier-based test statistic for conditional two-sample testing given as
\begin{align} \label{Eq: Acc statistic}
	\widehat{\mathrm{Acc}} \coloneqq  \frac{\sqrt{m}(\overline{A}_1 + \overline{A}_2 - 1)}{\sqrt{\widehat{\sigma}_1^2 + \widehat{\sigma}_2^2}},
\end{align}
where $\widehat{\sigma}_1^2 \coloneqq  (m-1)^{-1} \sum_{i=1}^m (\widehat{A}_{1,i} - \overline{A}_1)^2$ and $\widehat{\sigma}_2^2 \coloneqq  (m-1)^{-1} \sum_{i=1}^m (\widehat{A}_{2,i} - \overline{A}_2)^2$. To formally establish the limiting distribution of $\widehat{\mathrm{Acc}}$, we consider the following assumptions. 
\begin{assumption} \label{Assumption: classifier} 
	Let $m_n\coloneqq m$ be an increasing sequence of positive integers with $\lim_{n \rightarrow \infty} m_n = \infty$. Consider a class of null distributions $\mathcal{P}_0$ such that
	\begin{enumerate}
		\item[(a)] There are constants $c_1,c_2 \in (0,1)$ such that $c_1 \leq \inf_{P \in \mathcal{P}_0}\mP_P\{\widehat{h}(V^{(1)})=1 \given \widehat{h} \} \leq \sup_{P \in \mathcal{P}_0}\mP_P\{\widehat{h}(V^{(1)})=1 \given \widehat{h} \} \leq c_2$ for all sufficiently large $n$. Moreover, assume that there exist constants $C,\delta >0$ such that $\sup_{P \in \mathcal{P}_0} \mE_P[\{\widehat{r}_X(X^{(2)})\}^{2+\delta}] \leq C$ for all sufficiently large $n$.
		\item[(b)] For any $\epsilon > 0$, the density ratio estimator satisfies
		\begin{align*}
			\lim_{n \to \infty} \sup_{P \in \mathcal{P}_0} \mP_P \bigl( m \mE_P[\{\widehat{r}_X(X^{(2)}) - r_X(X^{(2)})\}^2 \given \widehat{r}_X] \geq \epsilon \bigr) = 0.
		\end{align*}
	\end{enumerate}
\end{assumption} 
\Cref{Assumption: classifier}(a) is imposed to establish the (conditional) central limit theorem for the test statistic with the true density ratio, and it excludes a deterministic classifier, which would return the same prediction value regardless of inputs, under the null hypothesis. \Cref{Assumption: classifier}(b) ensures that the approximation error from $\widehat{r}_X$ is asymptotically negligible, which is similarly assumed in \cite{hu2024two}. In order to theoretically justify this condition, one needs to take $m$ much smaller than the sample size used for training $\widehat{r}_X$. However, our empirical results in \Cref{Section: Numerical Experiments} illustrate that $\widehat{\mathrm{Acc}}$ approximates $N(0,1)$ closely even under balanced splitting. Therefore, echoing \cite{hu2024two}, we suggest taking $m = \lfloor{n/2\rfloor}$ in practice. 

Under \Cref{Assumption: classifier}, the classifier-based test statistic in \eqref{Eq: Acc statistic} converges to $N(0,1)$ uniformly over $\mathcal{P}_0$. 
\begin{theorem} \label{Theorem: Asymptotic Normality of Classification Accuracy}
		For the class of null distributions $\mathcal{P}_0$ satisfying \Cref{Assumption: classifier}, $\widehat{\mathrm{Acc}}$ in \eqref{Eq: Acc statistic} converges to $N(0,1)$:
	\begin{align*}
		\lim_{n \rightarrow \infty}\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} \big| \mP_P(\widehat{\mathrm{Acc}} \leq t) - \Phi(t) \big| = 0.
	\end{align*}
\end{theorem}
The proof of \Cref{Theorem: Asymptotic Normality of Classification Accuracy} can be found in \Cref{Section: Proof: Asymptotic Normality of Classification Accuracy}. According to \Cref{Theorem: Asymptotic Normality of Classification Accuracy}, the classifier-based test rejects the null hypothesis when $\widehat{\mathrm{Acc}} > \Phi^{-1}(1-\alpha)$, which has asymptotic validity over $\mathcal{P}_0$ satisfying \Cref{Assumption: classifier}. We next improve the efficiency of this procedure via $K$-fold cross-validation. To describe the procedure, we begin by considering $K$ disjoint subsets of $D_a$, denoted as $D_{a,1},D_{a,2},\ldots,D_{a,K}$, of equal size $m \coloneqq \lfloor n/K \rfloor$ for simplicity. For $j \in \{1,\ldots,K\}$, let $\overline{A}_{1,j} + \overline{A}_{2,j} - 1$ and $\widehat{\sigma}_{1,j}^{2} + \widehat{\sigma}_{2,j}^{2}$ denote the quantities analogous to $\overline{A}_{1} + \overline{A}_{2} - 1$ and $\widehat{\sigma}_{1}^{2} + \widehat{\sigma}_{2}^{2}$, respectively, by letting $D_a^\ast = D_{a,j}$ and \revised{$D_a^{\ast\ast} = D_a \! \setminus \! D_{a}^{\ast}$}. We then define the cross-validated classification accuracy statistic as
\begin{align} \label{Eq: cv_Acc statistic}
	\widehat{\mathrm{Acc}}_{\mathrm{cv}} \coloneqq  \frac{1}{\sqrt{K}}\sum_{j = 1}^K \frac{\sqrt{m}(\overline{A}_{1,j} + \overline{A}_{2,j} - 1)}{\sqrt{\widehat{\sigma}_{1,j}^2 + \widehat{\sigma}_{2,j}^2}}.
\end{align}
The next corollary proves that the cross-validated accuracy statistic is asymptotically normally distributed under the null hypothesis. 
\begin{corollary} \label{Corollary: Asymptotic Normality of CV Accuracy}
	Consider the same setting as in \Cref{Theorem: Asymptotic Normality of Classification Accuracy}. For any fixed $K \geq 2$, it holds that 
	\begin{align*}
		\lim_{n \rightarrow \infty}\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} \big| \mP_P(\widehat{\mathrm{Acc}}_{\mathrm{cv}} \leq t) - \Phi(t) \big| = 0.
	\end{align*}
\end{corollary}
According to \Cref{Corollary: Asymptotic Normality of CV Accuracy}, the test that rejects the null when $\widehat{\mathrm{Acc}}_{\mathrm{cv}} > \Phi^{-1}(1-\alpha)$ maintains the asymptotic type I error under control. In terms of power, the cross-validated version is generally more powerful than the accuracy test without cross-validation as it uses the sample more efficiently. We numerically demonstrate this point in \Cref{Section: Numerical Experiments}.

Many practical classifiers attempt to mimic the Bayes optimal classifier. For the balanced-sample setting, the Bayes optimal classifier is given as $h^\star(x,y) \coloneqq  \mathds{1}\bigl(f_{YX}^{(1)}(y,x) / \{f_{YX}^{(1)}(y,x) + f_{YX}(y,x)\} > 1/2\bigr)$ whose classification accuracy can be explicitly computed in terms of the total variation (TV) distance. Specifically, twice the classification accuracy can be computed as 
\begin{align*}
	\mP\{h^\star(V^{(1)})=1\} + \mE[r_X(X^{(2)}) \mathds{1}\{h^\star(V^{(2)})=2\}] = 1 + \mathrm{TV}(f_{YX}^{(1)}, f_{YX}),
\end{align*}
where $\mathrm{TV}(f_{YX}^{(1)}, f_{YX})$ denotes the TV distance between two distributions with densities $f_{YX}^{(1)}$ and $f_{YX}$, respectively. Since the TV distance becomes zero if and only if two distributions are identical, our classifier-based test can be powerful against general alternatives when the classifier in use approximates the Bayes classifier. 

The next subsection develops parallel results using a linear-time MMD statistic. \\


\subsection{Linear-time MMD} \label{Section: Linear-Time MMD}
In this subsection, we provide a detailed treatment of our second framework by focusing on a linear-time MMD statistic~\citep[][Lemma 14]{gretton2012kernel} with a kernel $k$. As in \Cref{Section: Classifier-based Approach}, we assume that $n_1 = n_2 = 2n$ and split the dataset into two \revised{subsets}: $D_{a} \coloneqq  \{V_i^{(1)}\}_{i=1}^{n} \cup \{V_i^{(2)}\}_{i=1}^{n}$ and $D_{b} \coloneqq  \{V_i^{(1)}\}_{i=n+1}^{2n} \cup \{V_i^{(2)}\}_{i=n+1}^{2n}$. Letting $\widehat{r}_X$ be an estimator of $r_X$ formed on $D_{b}$ and $m = \lfloor n/2 \rfloor$, define
\begin{align*}
	\widehat{S}_i & \coloneqq  k(V_{i}^{(1)},V_{i+m}^{(1)}) + \widehat{r}_X(X_{i}^{(2)})\widehat{r}_X(X_{i+m}^{(2)})k(V_{i}^{(2)},V_{i+m}^{(2)}) \\
	& ~ - \widehat{r}_X(X_{i}^{(2)}) k(V_{i}^{(2)},V_{i+m}^{(1)}) - \widehat{r}_X(X_{i+m}^{(2)}) k(V_{i}^{(1)},V_{i+m}^{(2)}).
\end{align*}
The test statistic that we analyze is a $t$-statistic based on $\widehat{S}_1,\ldots,\widehat{S}_m$. Specifically, letting $\overline{S} \coloneqq  m^{-1} \sum_{i=1}^m \widehat{S}_i$ and $\widehat{\sigma}^2 \coloneqq  (m-1)^{-1} \sum_{i=1}^m (\widehat{S}_i - \overline{S})^2 $, the (studentized) linear-time MMD statistic is given as
\begin{align} \label{Eq: MMD statistic}
	\widehat{\mathrm{MMD}}^2_{\ell} \coloneqq   \frac{\sqrt{m}\overline{S}}{\widehat{\sigma}}.
\end{align}
In order to establish the asymptotic normality of $\widehat{\mathrm{MMD}}^2_{\ell}$, we make the following assumptions. Below, let $S_i$ denote the quantity defined similarly as $\widehat{S}_i$ by replacing $\widehat{r}_X$ with the population counterpart $r_X$. 
\begin{assumption} \label{Assumption: linear-time MMD} Consider a class of null distributions $\mathcal{P}_0$ and assume that 
	\begin{enumerate}
		\item[(a)] There exist constants $c, C>0$ such that $\inf_{P \in \mathcal{P}_0}\mE_P[S_1^2] \geq c$ and $\sup_{P \in \mathcal{P}_0}\mE_P[S_1^{2+\delta}] \leq C$ for some $\delta>0$.
		\item[(b)] $\sup_{P \in \mathcal{P}_0} \mE_P[ \{r_X(X^{(2)})\}^2] < \infty$ and $\sup_{P \in \mathcal{P}_0}\mE_P\bigl[ \big\{\widehat{r}_X\bigl( X^{(2)} \bigr)-r_X\big(X^{(2)}\big)\big\}^2 \bigr] = o(m^{-1/2})$.
		\item[(c)] The kernel is uniformly bounded as $\|k\|_\infty \leq K$.
	\end{enumerate}
\end{assumption} 
\Cref{Assumption: linear-time MMD}(a) is about the moment condition for the population counterpart of $\widehat{S}_i$. This assumption is required to apply the uniform central limit theorem.  \Cref{Assumption: linear-time MMD}(b) is, on the other hand, required to prove that the difference between $\widehat{\mathrm{MMD}}^2_{\ell}$ using $\{\widehat{S}_i\}_{i=1}^m$ and that using $\{S_i\}_{i=1}^m$ are asymptotically negligible. This is similar to \Cref{Assumption: classifier}(b) and the assumption made in \citet{hu2024two}, but this condition is considerably weaker in terms of convergence rate. \Cref{Assumption: linear-time MMD}(c) assumes that the kernel $k$ is uniformly bounded. While this assumption is met for many practical kernels (e.g., Gaussian kernel), it can be relaxed by adopting more complex moment or convergence assumptions.

Having stated the assumptions, we now present the asymptotic normality of $\widehat{\mathrm{MMD}}^2_{\ell}$ under the null hypothesis in \eqref{Eq: hypothesis}.

\begin{theorem} \label{Theorem: Asymptotic Normality of Linear Time MMD}
	For the class of null distributions $\mathcal{P}_0$ satisfying \Cref{Assumption: linear-time MMD}, $\widehat{\mathrm{MMD}}^2_{\ell}$ converges to $N(0,1)$ as 
	\begin{align*}
		\lim_{n \rightarrow \infty}\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} \big| \mP_P(\widehat{\mathrm{MMD}}^2_{\ell} \leq t) - \Phi(t) \big| = 0.
	\end{align*}
\end{theorem}
We defer the proof of \Cref{Theorem: Asymptotic Normality of Linear Time MMD} to \Cref{Section: Proof: Asymptotic Normality of linear time MMD}. Based on \Cref{Theorem: Asymptotic Normality of Linear Time MMD}, the test that rejects the null when $\widehat{\mathrm{MMD}}^2_{\ell} > \Phi^{-1}(1-\alpha)$ controls the size uniformly over the class of distributions that satisfy \Cref{Assumption: linear-time MMD}. 

Similarly to the classification-based test, we can improve the efficiency of $\widehat{\mathrm{MMD}}^2_{\ell}$ via $K$-fold cross-validation. To describe this process, we begin by partitioning the dataset of size $2n$ into $K$-folds, denoted as $D_1,D_2,\ldots,D_K$, of equal size for simplicity. For $j \in \{1,\ldots,K\}$, let $\overline{S}_{j}$ and $\widehat{\sigma}_{j}^{2}$ denote the quantities similarly defined as $\overline{S}$ and $\widehat{\sigma}^2$, respectively, by letting $D_a = D_j$ and $D_b = \cup_{i=1}^{K} D_i \! \setminus \! D_j$. We then define the cross-validated MMD statistic as
\begin{align}  \label{Eq: cv_MMD statistic}
	\widehat{\mathrm{MMD}}^2_{\mathrm{cv}} \coloneqq  \frac{1}{K}\sum_{j = 1}^K \frac{\sqrt{n}\overline{S}_j}{\widehat{\sigma}_j}.
\end{align}
The next corollary shows that $\widehat{\mathrm{MMD}}^2_{\mathrm{cv}}$ converges to $N(0,1)$ under the same conditions for \Cref{Theorem: Asymptotic Normality of Linear Time MMD}.
\begin{corollary} \label{Corollary:  Asymptotic Normality of CV MMD}
	Consider the same setting as in \Cref{Theorem: Asymptotic Normality of Linear Time MMD}. Then for a fixed $K \geq 2$, it holds that
	\begin{align*}
		\lim_{n \rightarrow \infty}\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} \big| \mP_P(\widehat{\mathrm{MMD}}^2_{\mathrm{cv}} \leq t) - \Phi(t) \big| = 0.
	\end{align*}
\end{corollary}
Again, the test that rejects the null when $\widehat{\mathrm{MMD}}^2_{\mathrm{cv}} > \Phi^{-1}(1-\alpha)$ is asymptotically level $\alpha$ based on \Cref{Corollary:  Asymptotic Normality of CV MMD}. When the kernel $k$ is a characteristic kernel~\citep[e.g.,][]{fukumizu2007kernel}, the population MMD becomes equal to zero if and only if two distributions coincide. Thus as for the classifier-based tests in \Cref{Section: Classifier-based Approach}, the MMD-based tests can be powerful against general alternatives, provided that the density ratio $r_X$ can be accurately estimated. 

\subsection{\textcolor{DarkBlue}{Block-wise MMD}}\label{Section: Block-wise MMD}
In this subsection, we develop our second framework, building on the block-wise MMD statistic proposed by \cite{zaremba2013b} with kernel $k$. This statistic admits asymptotic normality and, depending on the choice of block size $B$, it encompasses the linear-time MMD statistic~\citep[Lemma 14]{gretton2012kernel} as a special case while achieving greater statistical power when $B$ is chosen appropriately. Unlike the classifier-based approach described in \Cref{Section: Classifier-based Approach}, we partition the data into two subsets: $D_{a} := \{V_i^{(1)}\}_{i=1}^{\,n-N} \cup \{V_i^{(2)}\}_{i=1}^{\,n-N}$ and $D_{b} := \{V_i^{(1)}\}_{i=n-N+1}^{n} \cup \{V_i^{(2)}\}_{i=n-N+1}^{n}$. Let $\widehat{r}_X$ be an estimator of $r_X$ constructed on $D_b$, and denote $M=n-N$. Letting $S := \lfloor M/B \rfloor$ as the number of blocks, we define for each $b=1,\dots,S$ the index set
\begin{align*}
I_b := \bigl\{ (i,j)\in\mathbb N^{2} : (b-1)B < i < j \le bB \bigr\}. 
\end{align*}
For each $i$, define $W_i \coloneqq (V_i^{(1)}, V_i^{(2)})$.  
Then, for each pair $(i,j) \in I_b$, we define the pairwise kernel contribution as
\begin{equation}\label{equation : kernel}
\begin{aligned}
  \widehat H(W_{i},W_{j}) 
  \;=\;&\, k(V_i^{(1)},V_j^{(1)})
        + \widehat r_X(X_i^{(2)})\,\widehat r_X(X_j^{(2)})k(V_i^{(2)},V_j^{(2)}) \\
       &- \widehat r_X(X_i^{(2)})k(V_i^{(2)},V_j^{(1)})
        - \widehat r_X(X_j^{(2)})k(V_i^{(1)},V_j^{(2)}).
\end{aligned}
\end{equation}
For notational simplicity, we write $\widehat{H}_{ij} := \widehat H(W_i,W_j)$. For each block $b$, define the block statistic and its normalized aggregate as follows:  
\begin{align*}
  \widehat\eta_b
  := B\cdot \binom{B}{2}^{-1}\!
      \sum_{(i,j)\in I_b}\widehat H_{ij}, 
      \quad \bar\eta := \frac1S\sum_{b=1}^{S}\widehat\eta_b,
      \quad \widehat\sigma_B^{2}:=\frac{1}{S-1}\sum_{b=1}^{S}\bigl(\widehat\eta_b-\bar\eta\bigr)^{2}.
\end{align*}
The studentized block-wise MMD statistic is then 
\begin{align*}  \widehat{\mathrm{MMD}}_{B}^{2}:=\frac{\sqrt{S}\,\bar\eta}{\widehat\sigma_B}.
\end{align*}

To establish the asymptotic normality of $\widehat{\mathrm{MMD}}_{B}^{2}$, we impose the following assumptions. Define $H_{ij}$ as the version of $\widehat{H}_{ij}$ obtained by replacing $\widehat{r}_{X}$ with the true density ratio $r_X$, and let $\eta_b$ be the corresponding counterpart of $\hat{\eta}_b$ based on $H_{ij}$.
\begin{assumption}\label{Assumption : Block wise MMD} Consider a class of null distributions $\mathcal{P}_0$ and assume that 
\begin{itemize}
    \item[(a)] There exist constants $c_1,c_2>0$ such that $\inf _{P \in \mathcal{P}_0} \mathbb{E}_P\big[{H^{2}_{12}}\big] \geq c_1$ and $\sup _{P \in \mathcal{P}_0} \mathbb{E}_P\big[|H_{12}|^{2+\delta}\big] \leq c_2$ for some $\delta>0$.
    \item[(b)] {Suppose that
    $\sup _{P \in \mathcal{P}_0} \mathbb{E}_P\big[r_X(X^{(2)})^2\big]<\infty$ and $\sup _{P \in \mathcal{P}_0} \mathbb{E}_P\big[\{\widehat{r}_X(X^{(2)})-r_X(X^{(2)})\}^2\big]=o\big(N^{-1/2}\big)$.} 
    \item[(c)] The kernel is uniformly bounded as $\|k\|_{\infty} \leq K$.
    \item[(d)] The block size $B$ satisfies $c_3 M^{\gamma} \leq B \leq c_4 M^{\gamma}$ for some constants $c_3, c_4 > 0$ and $0 \leq \gamma < 1$, 
    for all sufficiently large $M$. 
    \item[(e)] Suppose the subsample size satisfies $M\to \infty$, with growth rates such that $M^{1+\gamma}/N \to c_5 \in [0,\infty)$ for some $0 \leq \gamma < 1$ and constant $c_5$.
\end{itemize}
\end{assumption}
\Cref{Assumption : Block wise MMD}(a) imposes a moment condition on the population counterpart of $\widehat{H}_{ij}$ to ensure that the block-wise averages $\eta_b$ have uniformly bounded moments, a key requirement for applying the uniform central limit theorem. \Cref{Assumption : Block wise MMD}(b) guarantees that replacing the population kernel terms $H_{ij}$ with their estimates $\widehat{H}_{ij}$ has only a negligible effect on $\widehat{\mathrm{MMD}}_{B}^{2}$. A related condition in \cite{hu2024two} is strictly stronger. Under equal sample splitting, they require $\sqrt{n_{11}}\|\widehat{G}_{11}-G_{11}\|_{2,*}=o_P(1)$ 
(with $\|\cdot\|_{2,*}$ denoting the conditional $\ell_2$ norm, 
$\widehat{G}_{11}$ corresponding to the estimator based on the density ratio $\widehat{r}_X$, $G_{11}$ to its population counterpart $r_X$, and $o_P(1)$ indicating convergence in probability). This condition forces the estimation error to decay at the $n_{11}^{-1/2}$ rate. In contrast, our assumption directly bounds the squared error with sample size $N$, 
which corresponds to only an $N^{-1/4}$ rate in root form, and is therefore weaker.
 \Cref{Assumption : Block wise MMD}(c) assumes that the kernel $k$ is uniformly bounded. While this assumption is satisfied by many practical kernels (e.g., the Gaussian kernel), it can be relaxed by adopting more complex moment or convergence assumptions. \Cref{Assumption : Block wise MMD}(d) requires the block size $B$ to grow in a balanced manner: if blocks are too small ($\gamma=0$), the statistic reduces to the linear-time MMD, coinciding exactly with it when $B=2$, whereas if they are too large ($\gamma=1$), then $B$ is of order $M$, so the number of blocks $S$ does not increase, preventing the CLT from holding. To ensure $S \geq 2$, we restrict to $0 \leq \gamma < 1$. \Cref{Assumption : Block wise MMD}(e) requires $M$ and $N$ to grow at compatible rates so that density ratio error is negligible and the block-wise statistic admits a Gaussian limit. As noted by \citet[Example~1]{bordino2025density}, density ratio estimation and statistic computation may use different effective sample sizes, motivating our choice to adopt distinct splitting ratios, though the exact rates differ from theirs.



\begin{theorem}\label{theorem: Asymptotic Normality of Block MMD} For the class of null distributions $\mathcal{P}_0$ satisfying \Cref{Assumption : Block wise MMD}, $\widehat{\operatorname{MMD}}_{B}^2$ converges to $N(0,1)$ as 
\begin{align*}
    \lim _{M \rightarrow \infty} \sup _{P \in \mathcal{P}_0} \sup _{t \in \mathbb{R}}\left|\mathbb{P}_P\left(\widehat{\operatorname{MMD}}_{B}^2 \leq t\right)-\Phi(t)\right|=0.
\end{align*}
\end{theorem}
To further enhance sample efficiency while preserving the favorable asymptotic properties of the block-wise MMD statistic, we adopt a $K$-fold cross-validation strategy, similar in spirit to the approach used in linear-time MMD tests.

Let the full dataset of size $n$ be evenly partitioned into $K$ disjoint folds, denoted by $D_1, \ldots, D_K$, each containing $n/K$ observations. For each $j \in \{1, \ldots, K\}$, define $D_a := D_j$ as the held-out block used to compute the test statistic, and let $D_b := \cup_{i=1}^K D_i \setminus D_j$ be the union of the remaining $K-1$ folds used to estimate $r_X$. This strategy is theoretically justified because it allocates a larger portion of the data, specifically a $(1 - 1/K)$ fraction, to density-ratio estimation, which can improve both accuracy and stability.

We compute $\bar{\eta}_j$ and $\widehat{\sigma}_j$ based on $D_a$ and $D_b$, following the block-wise MMD construction. The cross-fitted statistic is then defined as
\begin{align*}
  ^{\dagger}\widehat{\mathrm{MMD}}_{\mathrm{B}}^2 := \frac{1}{K} \sum_{j=1}^K \frac{\sqrt{S} \bar{\eta}_j}{\widehat{\sigma}_j}.
\end{align*}

The following corollary extends the central limit theorem result to the cross-fitted statistic:

\begin{corollary}\label{cor:cv_block_MMD}
Under the conditions stated in Theorem~\ref{theorem: Asymptotic Normality of Block MMD}, and for a fixed $K \ge 2$, it holds that
\begin{align*}
  \lim_{M\to \infty} \sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} \left| \mathbb{P}_P\left(^{\dagger}\widehat{\mathrm{MMD}}_{\mathrm{B}}^2 \leq t\right) - \Phi(t) \right| = 0.
\end{align*}
\end{corollary}

This result justifies the use of a test that rejects $H_0$ whenever $^{\dagger}\widehat{\mathrm{MMD}}_{\mathrm{B}}^2 > \Phi^{-1}(1-\alpha)$ as an asymptotically level-$\alpha$ procedure. Moreover, since each fold contributes to both estimation and evaluation in a balanced manner, this cross-fitting strategy yields a more efficient use of the data, reducing variance while maintaining theoretical guarantees.

% \subsection{\textcolor{red}{Quadratic-time MMD}}\label{Section: Quadratic-Time MMD}
% In this subsection, we analyze our second framework, which is based on a quadratic-time MMD statistic with kernel $k$, introduced in \Cref{Example : Quadratic time MMD} and formalized in \citet[Theorem 12]{gretton2012kernel}. As in \Cref{Section: Block-wise MMD}, we split the sample of size $n$ into two disjoint parts: $D_a$, containing the first $M=n-N$ observations used to compute the test statistic, and $D_b$, containing the remaining $N$ observations used to estimate the density ratio. Using $D_b$, we then construct the estimator $\widehat{r}_X$ of $r_X$. 

% \noindent Based on the kernel $\widehat H$ introduced in \eqref{equation : kernel}, the quadratic-time MMD estimator takes the U-statistic form
% \begin{align*}
% \widehat{\mathrm{MMD}}_u^2 
% = \frac{1}{M(M-1)} \sum_{1\leq i \neq j\leq M} \widehat{H}(W_i, W_j).
% \end{align*}
% In order to study the asymptotic behavior of $\widehat{\mathrm{MMD}}_u^2$, 
% we introduce several notational conventions. First, let $H(W_i,W_j)$ denote the population version of the kernel, 
% obtained by replacing $\widehat{r}_X$ with $r_X$. 
% Define $\varphi(W_i) := \psi(V_i^{(1)}) - r_X(X_i^{(2)})\,\psi(V_i^{(2)})$, 
% so that $H(W_i,W_j) = \langle \varphi(W_i), \varphi(W_j)\rangle_{\mathcal{H}_k}$.
% \medskip

% Next, for each sample size $n$, let 
% $P_{n} := P_{XY,n}^{(1)} \otimes P_{XY,n}^{(2)}$ 
% be the joint distribution of $W=(V^{(1)},V^{(2)})$, 
% with mean embedding $\mu_{P_{n}} := \mathbb{E}_{P_{n}}[\varphi(W)]$. 
% This triangular-array formulation allows us to capture settings 
% where the underlying distribution may vary with $n$.  Using this embedding, we define the centered kernel
% \begin{equation}\label{equation : centered kernel}
% \begin{aligned}
% \widetilde{H}_n(W_i,W_j) 
% &:= \langle \varphi(W_i)-\mu_{P_{n}},\, \varphi(W_j)-\mu_{P_{n}}\rangle_{\mathcal{H}_k} \\
% &= H(W_i,W_j) - \mathbb{E}_{W \sim P_{n}}[H(W_i,W)] 
%                  - \mathbb{E}_{W \sim P_{n}}[H(W,W_j)] 
%                  + \mathbb{E}_{W,W' \sim P_{n}}[H(W,W')].
% \end{aligned}
% \end{equation}
% Finally, for any fixed $n$, let 
% $\{(\lambda_{\ell,n}^{(P_{n})},\Psi_{\ell,n}^{(P_{n})}): \ell \geq 1\}$ 
% denote the eigenvalue--eigenfunction pairs of the operator
% \begin{align*}
% g \mapsto \int \widetilde{H}_n(\cdot, w)\, g(w)\, dP_{n}(w).
% \end{align*}
% When $\widetilde{H}_n$ is square-integrable, 
% it admits the spectral expansion
% \begin{align*}
% \widetilde{H}_{n}(w,w') 
% = \sum_{\ell=1}^\infty \lambda_{\ell,n}^{(P_{n})}\,\Psi_{\ell,n}^{(P_{n})}(w)\Psi_{\ell,n}^{(P_{n})}(w').
% \end{align*}
% Let $\{\mathcal{E}_n\}_{n \ge 1}$ denote a sequence of distribution classes, 
% where each $\mathcal{E}_n$ consists of pairs of distributions for sample size $n$. 
% Define $\mathcal{P}_{n}^{(0)} \subset \mathcal{E}_n$ as the set of distribution pairs 
% satisfying the null hypothesis $H_0$ in \eqref{Eq: hypothesis}. For notational simplicity, we omit the dependence on $P_n$ and simply write $\lambda_{\ell,n}$ and $\Psi_{\ell,n}$ in place of 
% $\lambda_{\ell,n}^{(P_n)}$ and $\Psi_{\ell,n}^{(P_n)}$.

% \begin{assumption}\label{Assumption : quadratic time MMD} Consider a sequence of null distribution classes $\{\mathcal{P}_{n}^{(0)}\}_{n \ge 1}$.  
% For each $n$, let $(\lambda_{\ell,n})_{\ell \ge 1}$ denote the eigenvalues of the Hilbert--Schmidt operator associated with $\widetilde{H}_n$, ordered such that 
% $\lambda_{1,n} \ge \lambda_{2,n} \ge \cdots$, and satisfying $\lambda_{1,n} \ge \lambda_{2,n} > 0$ for all $n$.  
% The following conditions are assumed to hold:
% \begin{enumerate}
%     \item[(a)] Suppose that
%   \begin{align*}
%     \lim_{M\rightarrow \infty}\sup_{P_{n}\in\mathcal{P}_{n}^{(0)}}\frac{\mE_{P_n}[|\widetilde{H}_{n}(W_{1},W_{2})|^3]}{M^{1/2}\mE_{P_n}[\widetilde{H}_{n}(W_{1},W_{2})^2]^{3/2}}=0.
%   \end{align*}
%     \item[(b)] Suppose that $\sup_{n\in\mathbb{N}}\sup _{P_{n} \in \mathcal{P}_{n}^{(0)}} \mathbb{E}_{P_n}\big[r_X(X^{(2)})^2\big]< \infty$ and $\sup _{P_n \in \mathcal{P}_{n}^{(0)}} \mathbb{E}_{P_n}\big[\{\widehat{r}_X(X^{(2)})-r_X(X^{(2)})\}^2\big]=o(N^{-1/2})$.
%     \item[(c)] The kernel is uniformly bounded as $\|k\|_{\infty} \leq K$.
%     \item[(d)] Suppose the subsample size satisfies $M \to \infty$, with growth rates such that $M^{2}/N \to c \in [0,\infty)$ for some constant $c$.
% \end{enumerate}
% \end{assumption}

% \Cref{Assumption : quadratic time MMD}(a) is essential for applying the Berry--Esseen bound in \Cref{lemma:degenerate_ustat_berryessen}, as it ensures that the third moment of the centered kernel is negligible relative to its variance, allowing the quadratic-time $U$-statistic to be well approximated by its limiting quadratic form. The remaining conditions, \Cref{Assumption : quadratic time MMD}(b)–(d), play the same technical roles as those introduced in \Cref{Assumption : Block wise MMD}: \Cref{Assumption : quadratic time MMD}(b) ensures that the difference between the U-statistics based on $\widehat{H}$ and $H$ is asymptotically negligible, \Cref{Assumption : quadratic time MMD}(c) requires the kernel to be uniformly bounded, and \Cref{Assumption : quadratic time MMD}(d) specifies compatible growth rates of the two sample sizes so that density ratio error is negligible and the quadratic-time statistic is well defined. In fact, \Cref{Assumption : quadratic time MMD}(d) coincides exactly with the case discussed in \Cref{Assumption : Block wise MMD}(e) when the parameter $\gamma$ is equal to one.



% \begin{theorem}\label{Theorem : Asymptotic distribution of quadratic time MMD}
% For the class of null distributions $\mathcal{P}_{n}^{(0)}$ satisfying 
% \Cref{Assumption : quadratic time MMD}, 
% the quadratic-time MMD U-statistic satisfies the uniform distributional convergence
% \begin{align*}
% \lim_{M \to \infty} \;
% \sup_{P_{n} \in \mathcal{P}_{n}^{(0)}}
% \sup_{t \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_{n}}
% \big(M \cdot \widehat{\mathrm{MMD}}_{u}^2 \leq t\big)
% - 
% \mathbb{P}_{P_{n}}(G_n \leq t)
% \Big|=0,
% \end{align*}
% where 
% \begin{align*}
% G_n = \sum_{\ell=1}^{\infty} \lambda_{\ell,n}(a_\ell^2-1),
% \qquad \{a_\ell\}_{\ell \geq 1} \stackrel{\text{i.i.d.}}{\sim} N(0,1),    
% \end{align*}
% and $\{\lambda_{\ell,n}^{(P_n)}\}_{\ell \geq 1}$ are the eigenvalues in the spectral expansion of $\widetilde{H}_n$.
% \end{theorem}

% Since the limiting distribution is given by an infinite weighted sum of centered chi-squared components, which is analytically intractable, we adopt a multiplier bootstrap approach for hypothesis testing.  
% Let $\xi_1, \ldots, \xi_M \stackrel{\text{i.i.d.}}{\sim} N(0,1)$ be standard Gaussian multipliers, independent of the sample $\mathbb{W}_M = (W_1, \ldots, W_M)$.  
% We then define the multiplier bootstrap statistic as
% \begin{align*}
% \widehat{\mathrm{MMD}}^2_{\mathrm{Boot}}
% := \frac{1}{M(M-1)}\sum_{i\neq j}\xi_{i}\xi_{j}\,\widehat{H}(W_{i},W_{j}),
% \end{align*}
% where $\widehat H$ denotes the kernel function defined in \eqref{equation : kernel}.

% \begin{proposition}\label{thm:wild-bootstrap-consistency}
% Let the sequence of null distribution classes 
% $\{\mathcal{P}_{n}^{(0)}\}_{n \ge 1}$ satisfy
% \Cref{Assumption : quadratic time MMD}~(b)–(d).
% Suppose further that the kernel $\widetilde{H}_n$
% additionally satisfies
% \begin{align*}
% \lim_{M \to \infty}
% \sup_{P_n \in \mathcal{P}_{n}^{(0)}}
% \frac{
% \mathbb{E}_{P_n}\!\big[\widetilde{H}_n(W_1, W_1)^2\big]
% }{
% M \, \mathbb{E}_{P_n}\!\big[\widetilde{H}_n(W_1, W_2)^2\big]
% }
% = 0.
% \end{align*}
% Define
% \begin{align*}
% \widetilde{B}_n 
% := 
% \sup_{x \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_n}\!\big(M\,\widehat{\mathrm{MMD}}^2_{\mathrm{Boot}} \leq x \mid \mathbb{W}_M\big)
% -
% \mathbb{P}_{P_n}(G_n \le x)
% \Big|,
% \end{align*}
% where $G_n$ is as defined in
% \Cref{Theorem : Asymptotic distribution of quadratic time MMD}.
% Then the multiplier bootstrap procedure is \emph{uniformly consistent}, in the sense that for every fixed $\varepsilon>0$,
% \begin{align*}
% \lim_{M \to \infty}
% \sup_{P_n \in \mathcal{P}_{n}^{(0)}}
% \mathbb{P}_{P_n}\!\big(\widetilde{B}_n > \varepsilon\big)
% = 0.
% \end{align*}
% \end{proposition}


% Let $\{\widehat{T}^{(b)} : b=1,\ldots,B\}$ denote the multiplier bootstrap replicates 
% of the quadratic-time MMD statistic, generated using independent Gaussian weights 
% $\xi^{(b)} := (\xi_1^{(b)},\ldots,\xi_M^{(b)})$ as in 
% Proposition~\ref{thm:wild-bootstrap-consistency}.  
% Define the augmented set 
% \begin{align*}
% \big\{\widehat{T}^{(1)},\ldots,\widehat{T}^{(B)},\widehat{T}\big\},    
% \end{align*}
% where $\widehat{T}$ denotes the original statistic 
% $M \cdot \widehat{\mathrm{MMD}}^2_{u}$.  
% Let 
% \begin{align*}
% \widehat{T}_{(1)} \le \widehat{T}_{(2)} \le \cdots \le \widehat{T}_{(B+1)}    
% \end{align*}
% denote its order statistics.  
% For $\alpha \in (0,1)$, the empirical $(1-\alpha)$ wild bootstrap quantile is defined as
% \begin{align*}
% \widehat{q}_{1-\alpha}^{B}\big(\{\widehat{T}^{(b)}\}_{1 \leq b \le B} \mid \mathbb{W}_M\big)
% &:= \inf \Big\{ u \in \mathbb{R} : \frac{1}{B+1}\sum_{b=1}^{B+1} 
% \mathds{1}\big(\widehat{T}^{(b)} \le u\big) \geq 1-\alpha \Big\} \\
% &= \widehat{T}_{(\lceil (B+1)(1-\alpha) \rceil)}.
% \end{align*}
% For notational simplicity, and whenever no ambiguity arises, we write the threshold as $\widehat{q}_{1-\alpha}^{\,B}$.  
% By combining \Cref{Theorem : Asymptotic distribution of quadratic time MMD} and \Cref{thm:wild-bootstrap-consistency}, 
% the test that rejects the null whenever 
% $M \cdot \widehat{\mathrm{MMD}}^2_{u} > \widehat{q}_{1-\alpha}^{\,B}$ 
% achieves asymptotically valid level~$\alpha$ control 
% uniformly over the class of null distributions satisfying 
% \Cref{Assumption : quadratic time MMD}(b)–(d), 
% together with the moment conditions 
% required by \Cref{Theorem : Asymptotic distribution of quadratic time MMD} and \Cref{thm:wild-bootstrap-consistency}.


\subsection{\textcolor{DarkBlue}{Quadratic-time MMD}}\label{Section: Quadratic-Time MMD}

In this subsection, we analyze our second framework, which builds on a quadratic-time MMD statistic with kernel $k$, introduced in \Cref{Example : Quadratic time MMD} and formalized in \citet[Theorem~12]{gretton2012kernel}.  
As in \Cref{Section: Block-wise MMD}, we divide the sample of size $n$ into two disjoint subsets: $D_a$, containing the first $M = n - N$ observations used to compute the test statistic, and $D_b$, containing the remaining $N$ observations used to estimate the density ratio. Based on $D_b$, we construct the estimator $\widehat{r}_X$ of $r_X$.  

For each sample size $n \ge 1$, let $\{\mathcal{E}_n\}_{n \ge 1}$ denote a sequence of distribution classes,  
where each $\mathcal{E}_n$ consists of pairs of distributions corresponding to the sample size $n$.  
Define $\mathcal{P}_n^{(0)} \subseteq \mathcal{E}_n$ as the subset of distribution pairs satisfying the null hypothesis $H_0$ in \eqref{Eq: hypothesis}.  
All probabilistic statements in what follows are made uniformly over $P \in \mathcal{P}_n^{(0)}$.  

Using the kernel $\widehat{H}$ defined in \eqref{equation : kernel},  
the quadratic-time MMD statistic is given by  
\begin{align*}
\widehat{\mathrm{MMD}}_u^2 
= \frac{1}{M(M-1)} 
  \sum_{1 \le i \neq j \le M} 
  \widehat{H}(W_i, W_j),
\end{align*}
where $W_i = (V_i^{(1)}, V_i^{(2)})$.  

To study the asymptotic behavior of $\widehat{\mathrm{MMD}}_u^2$,  
let $H(W_i, W_j)$ denote the population version of the kernel obtained by replacing $\widehat{r}_X$ with the true ratio $r_X$.  
Define the feature map  
\begin{align*}
\varphi(W_i) := \psi(V_i^{(1)}) - r_X(X_i^{(2)}) \, \psi(V_i^{(2)}),
\end{align*}
so that the population kernel can be written as  
\begin{align*}
H(W_i, W_j) = \langle \varphi(W_i), \varphi(W_j) \rangle_{\mathcal{H}_k}.
\end{align*}

For each $P \in \mathcal{P}_n^{(0)}$, define the mean feature representation  
\begin{align*}
\mu_P := \mathbb{E}_P[\varphi(W)],
\end{align*}
and construct the centered kernel  
\begin{align}\label{equation : centered kernel}
\begin{aligned}
\widetilde{H}_n(W_i, W_j) 
&:= \langle \varphi(W_i) - \mu_P,\ \varphi(W_j) - \mu_P \rangle_{\mathcal{H}_k} \\
&= H(W_i, W_j)
   - \mathbb{E}_{W \sim P}[H(W_i, W)]
   - \mathbb{E}_{W \sim P}[H(W, W_j)]
   + \mathbb{E}_{W, W' \sim P}[H(W, W')].
\end{aligned}
\end{align}

For each fixed $n$, let $\{(\lambda_{\ell,n}, \Psi_{\ell,n}) : \ell \ge 1\}$  
denote the eigenvalue–eigenfunction pairs of the Hilbert–Schmidt operator
\begin{align*}
g \mapsto \int \widetilde{H}_n(\cdot, w)\, g(w)\, dP_n(w).
\end{align*}
When $\widetilde{H}_n$ is square-integrable, it admits the spectral expansion
\begin{align*}
\widetilde{H}_n(w, w') 
= \sum_{\ell=1}^\infty 
  \lambda_{\ell,n}\,
  \Psi_{\ell,n}(w)\Psi_{\ell,n}(w').
\end{align*}

\begin{assumption}\label{Assumption : quadratic time MMD}
Consider a sequence of null distribution classes $\{\mathcal{P}_{n}^{(0)}\}_{n \ge 1}$.  
For each $n$, let $(\lambda_{\ell,n})_{\ell \ge 1}$ denote the eigenvalues of the Hilbert--Schmidt operator associated with $\widetilde{H}_n$, ordered such that 
$\lambda_{1,n} \ge \lambda_{2,n} \ge \cdots$ for all $n$.  
Assume the following conditions hold:
\begin{enumerate}

    \item[(a)] Suppose that
  \begin{align*}
    \lim_{M\rightarrow \infty}\sup_{P\in\mathcal{P}_{n}^{(0)}}\frac{\mE_{P}[\widetilde{H}_{n}(W_{1},W_{2})^4]}{M\,\mE_{P}[\widetilde{H}_{n}(W_{1},W_{2})^2]^{2}}=0.
  \end{align*}
  \item[(b)] There exist constant $c_1>0$ such that $\sup_{P\in\mathcal{P}_{n}^{(0)}}\mE_{P}\big[|\widetilde{H}_n(W_{1},W_{1})|\big]\leq c_{1}$.
    \item[(c)] Suppose that $\sup_{n\in\mathbb{N}}\sup _{P \in \mathcal{P}_{n}^{(0)}} \mathbb{E}_{P}\big[r_X(X^{(2)})^2\big]< \infty$ and $\sup _{P \in \mathcal{P}_{n}^{(0)}} \mathbb{E}_{P}\big[\{\widehat{r}_X(X^{(2)})-r_X(X^{(2)})\}^2\big]=o(N^{-1/2})$.
    \item[(d)] The kernel is uniformly bounded as $\|k\|_{\infty} \leq K$.
    \item[(e)] Suppose the subsample size satisfies $M \to \infty$, with growth rates such that $M^{2}/N \to c_2 \in [0,\infty)$ for some constant $c_2$.
\end{enumerate}
\end{assumption}

\Cref{Assumption : quadratic time MMD}(a) provides the moment control required for applying the Berry--Esseen bound in \Cref{lemma:degenerate_ustat_berryessen},  
ensuring that the fourth moment of the centered kernel remains sufficiently small relative to its squared variance.  
This condition allows the quadratic-time $U$-statistic to be accurately approximated by its limiting quadratic form.  
Building on this, \Cref{Assumption : quadratic time MMD}(b) imposes a uniform boundedness condition on the diagonal term of the centered kernel,  
which stabilizes the statistic by keeping the diagonal component stochastically bounded under the null.  
Together with the subsequent conditions, this boundedness contributes to rendering the remainder terms asymptotically negligible,  
thereby ensuring that the asymptotic behavior of the statistic is dominated by its off-diagonal component.  
The remaining assumptions, \Cref{Assumption : quadratic time MMD}(c)–(e), serve complementary technical roles analogous to those in \Cref{Assumption : Block wise MMD}: 
\Cref{Assumption : quadratic time MMD}(c) guarantees that the difference between the $U$-statistics based on $\widehat{H}$ and $H$ is asymptotically negligible;  
\Cref{Assumption : quadratic time MMD}(d) enforces uniform boundedness of the kernel function;  
and \Cref{Assumption : quadratic time MMD}(e) specifies compatible growth rates of $M$ and $N$  
to keep the density ratio estimation error negligible and ensure the well-posedness of the quadratic-time statistic.  
In particular, \Cref{Assumption : quadratic time MMD}(e) coincides with \Cref{Assumption : Block wise MMD}(e) when the parameter $\gamma = 1$.


\begin{theorem}\label{Theorem : Asymptotic distribution of quadratic time MMD}
For the class of null distributions $\mathcal{P}_{n}^{(0)}$ satisfying 
\Cref{Assumption : quadratic time MMD}, 
the quadratic-time MMD $U$-statistic satisfies the uniform distributional convergence
\begin{align*}
\lim_{M \to \infty} \;
\sup_{P \in \mathcal{P}_{n}^{(0)}}
\sup_{t \in \mathbb{R}}
\Big|
\mathbb{P}_{P}
\big(M \cdot \widehat{\mathrm{MMD}}_{u}^2 \leq t\big)
- 
\mathbb{P}_{P}(G_n \leq t)
\Big| = 0,
\end{align*}
where 
\begin{align*}
G_n = \sum_{\ell=1}^{\infty} \lambda_{\ell,n}(a_\ell^2-1),
\qquad 
\{a_\ell\}_{\ell \ge 1} \stackrel{\text{i.i.d.}}{\sim} N(0,1),
\end{align*}
and $\{\lambda_{\ell,n}\}_{\ell \ge 1}$ are the eigenvalues from the spectral expansion of $\widetilde{H}_n$.
\end{theorem}

Since the limiting distribution is expressed as an infinite weighted sum of centered chi-squared components, which is analytically intractable, we adopt a multiplier bootstrap approach for hypothesis testing.  
Let $\xi_1, \ldots, \xi_M \stackrel{\text{i.i.d.}}{\sim} N(0,1)$ be standard Gaussian multipliers, independent of the sample $\mathbb{W}_M = (W_1, \ldots, W_M)$.  
We then define the multiplier bootstrap statistic as
\begin{align*}
\widehat{\mathrm{MMD}}^2_{\mathrm{Boot}}
:= \frac{1}{M(M-1)}\sum_{i \neq j}\xi_{i}\xi_{j}\,\widehat{H}(W_{i},W_{j}),
\end{align*}
where $\widehat{H}$ is the kernel function defined in \eqref{equation : kernel}.

\begin{theorem}\label{thm:wild-bootstrap-consistency}
Let the sequence of null distribution classes 
$\{\mathcal{P}_{n}^{(0)}\}_{n \ge 1}$ satisfy
\Cref{Assumption : quadratic time MMD}.  
Suppose further that $\widetilde{H}_n$ satisfies, for some universal constant $C > 0$,
\begin{align}\label{eq:moment condition}
\sup_{P \in \mathcal{P}_{n}^{(0)}}
\frac{
\mathbb{E}[\widetilde{H}_n(W_{1},W_{1})^2]
}{
\mathbb{E}[\widetilde{H}_n(W_{1},W_{2})^4]^{1/2}
} \le C.
\end{align}
Define
\begin{align*}
\widetilde{B}_n 
:= 
\sup_{x \in \mathbb{R}}
\Big|
\mathbb{P}_{P}\big(M\,\widehat{\mathrm{MMD}}^2_{\mathrm{Boot}} \leq x \mid \mathbb{W}_M\big)
-
\mathbb{P}_{P}(G_n \le x)
\Big|,
\end{align*}
where $G_n$ is as defined in
\Cref{Theorem : Asymptotic distribution of quadratic time MMD}.
Then the multiplier bootstrap procedure is \emph{uniformly consistent}, in the sense that for every fixed $\varepsilon > 0$,
\begin{align*}
\lim_{M \to \infty}
\sup_{P \in \mathcal{P}_{n}^{(0)}}
\mathbb{P}_{P}\big(\widetilde{B}_n > \varepsilon\big)
= 0.
\end{align*}
\end{theorem}

Let $\{\widehat{T}^{(b)} : b=1,\ldots,B\}$ denote the multiplier bootstrap replicates 
of the quadratic-time MMD statistic, generated using independent Gaussian weights 
$\xi^{(b)} := (\xi_1^{(b)},\ldots,\xi_M^{(b)})$ as in 
\Cref{thm:wild-bootstrap-consistency}.  
Define the augmented set 
\begin{align*}
\big\{\widehat{T}^{(1)},\ldots,\widehat{T}^{(B)},\widehat{T}\big\},    
\end{align*}
where $\widehat{T}$ denotes the original statistic 
$M \cdot \widehat{\mathrm{MMD}}^2_{u}$.  
Let 
\begin{align*}
\widehat{T}_{(1)} \le \widehat{T}_{(2)} \le \cdots \le \widehat{T}_{(B+1)}    
\end{align*}
denote its order statistics.  
For $\alpha \in (0,1)$, the empirical $(1-\alpha)$ wild bootstrap quantile is defined as
\begin{align*}
\widehat{q}_{1-\alpha}^{\,B}\big(\{\widehat{T}^{(b)}\}_{1 \leq b \le B} \mid \mathbb{W}_M\big)
&:= \inf \Big\{ u \in \mathbb{R} : \frac{1}{B+1}\sum_{b=1}^{B+1} 
\mathds{1}\big(\widehat{T}^{(b)} \le u\big) \geq 1-\alpha \Big\} \\
&= \widehat{T}_{(\lceil (B+1)(1-\alpha) \rceil)}.
\end{align*}
For notational simplicity, and whenever no ambiguity arises, we write the threshold as $\widehat{q}^{\,B}_{1-\alpha}$. 

\begin{proposition}[Consistency of the bootstrap quantile]\label{prop:bootstrap-quantile-consistency}
Under the setup of \Cref{thm:wild-bootstrap-consistency},  
suppose that the limiting distribution function 
$F_0(t) = \mathbb{P}(G_n \le t)$ is continuous at its $(1-\alpha)$ quantile 
$q_{1-\alpha} := \inf \{ t \in \mathbb{R} : F_0(t) \ge 1 - \alpha \}$.  
Then, under the moment condition~\eqref{eq:moment condition} and  
\Cref{Assumption : quadratic time MMD}, for any $\varepsilon > 0$,
\begin{align*}
\lim_{M \to \infty} 
\sup_{P \in \mathcal{P}_{n}^{(0)}}
\mathbb{P}\big( |\widehat{q}_{1-\alpha}^{\,B} - q_{1-\alpha}| \ge \varepsilon \big)
= 0,
\end{align*}
as $M, B \to \infty$.
\end{proposition}
\Cref{prop:bootstrap-quantile-consistency} establishes that the empirical bootstrap quantile $\widehat{q}_{1-\alpha}^{\,B}$ 
converges uniformly in probability to the population quantile $q_{1-\alpha}$ of the limiting distribution $F_0$.  
Intuitively, as both the sample size $M$ and the number of bootstrap replicates $B$ increase,  
the bootstrap approximation uniformly reproduces the $(1-\alpha)$ critical value of the true limiting law across all distributions $P \in \mathcal{P}_{n}^{(0)}$. 
This uniform convergence directly implies that the bootstrap quantile 
$\widehat{q}_{1-\alpha}^{\,B}$ consistently reproduces the theoretical cutoff $q_{1-\alpha}$ 
defining the rejection region.  
Hence, the bootstrap-based test inherits the same asymptotic rejection probability 
as the ideal test based on the limiting distribution $F_0$.

By \Cref{prop:bootstrap-quantile-consistency},  
the test that rejects the null hypothesis whenever  
\begin{align*}
M \cdot \widehat{\mathrm{MMD}}_{u}^2 > \widehat{q}_{1-\alpha}^{\,B}
\end{align*}
achieves asymptotically valid level~$\alpha$ control.  
This result holds uniformly over all distributions $P \in \mathcal{P}_{n}^{(0)}$  
satisfying \Cref{Assumption : quadratic time MMD} and the moment condition in \eqref{eq:moment condition}.









% % uniform convergence를 보여야 함. 

% \section{\textcolor{red}{(Residual kernel) Quadratic-time MMD}}
% We now present our second framework, which is based on a quadratic-time MMD statistic.  
% As in Section~\Cref{Section: Linear-Time MMD} and \Cref{Section: Block-wise MMD}, 
% we split the sample into $D_a$ and $D_b$ to compute the test statistic and 
% to estimate the density ratio, respectively.

% For each $n\geq 2$, let $Z=(V^{(1)},V^{(2)})$ with joint distribution $P_{Z_n}$ on $\cZ=\cV\times\cV$, 
% where $P_{Z_n}=P_{XY,n}^{(1)}\otimes P_{XY,n}^{(2)}$.  
% Let $(\cH,\langle\cdot,\cdot\rangle_\cH)$ be a Hilbert space in which all kernels are embedded.  
% For each $n$, let $k_n:\cV\times\cV\to\cH$ be a reproducing kernel with feature map $g_n(v)=k_n(\cdot,v)\in\cH$.  
% Let $S_n\subset\cH$ be a subspace spanned by a sieve $m_n:\cV\to\cH$.  
% The residual feature map is defined as
% \begin{align*}
% R_n(v) := (I-\Pi_{S_n})g_n(v) = g_n(v)-m_n(v),
% \end{align*}
% where $\Pi_{S_n}$ denotes the orthogonal projection onto $S_n$, 
% and we assume $\mE_{P_{Z_n}}[\|R_n(V)\|_{\cH}^2]<\infty$. For a density ratio $\widehat r_X$, define the residual feature representation
% \begin{align*}
% \varphi_{n}^{(\widehat r_X)}(Z) 
% := R_n(V^{(1)}) - \widehat r_X(X^{(2)})\,R_n(V^{(2)}).
% \end{align*}
% The associated residual kernel is
% \begin{align}\label{equation : estimated quadratic MMD}
% \widetilde H_{n}^{(\widehat r_X)}(Z,Z')
% := \langle \varphi_{n}^{(\widehat r_X)}(Z),\,\varphi_{n}^{(\widehat r_X)}(Z')\rangle_\cH.
% \end{align}
% The corresponding quadratic-time statistic is
% \begin{align*}
% U_{n}^{\mathrm{res}}(\widehat{r}_X) 
% := \frac{1}{n(n-1)} \sum_{i\neq j} \widetilde H_{n}^{(\widehat r_X)}(W_i,W_j).
% \end{align*}
% Replacing the estimator $\widehat r_X$ with the true density ratio $r_X$ 
% yields the oracle feature representation $\varphi_n^{(r_X)}$, 
% kernel $\widetilde H_n^{(r_X)}$, 
% and corresponding statistic $U_n^{\mathrm{res}}(r_X)$ defined analogously.
% \subsection{Limiting distribution under the null hypothesis}
% To proceed, it is useful to introduce the centered oracle residual kernel. Define the mean embedding $\mu_{P_{Z_n}}^{(r_X)} := \mE_{Z \sim P_{Z_n}} \big[\varphi_{n}^{(r_X)}(Z)\big],$
% and the centered residual kernel
% \begin{align*}
% \bar H_n^{(r_X)}(Z,Z')
% := \big\langle \varphi_{n}^{(r_X)}(Z) - \mu_{P_{Z_n}}^{(r_X)},\,
%            \varphi_{n}^{(r_X)}(Z') - \mu_{P_{Z_n}}^{(r_X)} \big\rangle_\cH.
% \end{align*}
% For any fixed $n$, let $T_n:L^2(P_{Z_n}) \to L^2(P_{Z_n})$ be the integral operator 
% with kernel $\bar H_n^{(r_X)}$, defined by
% \begin{align*}
% (T_n g)(\cdot) := \int \bar H_n^{(r_X)}(\cdot, Z)\, g(Z)\, dP_{Z_n}(Z),
% \qquad g \in L^2(P_{Z_n}).    
% \end{align*}
% Let $\{(\lambda_{l,n}, \Psi_{l,n}): l \geq 1\}$ denote the eigenvalue–eigenfunction 
% sequence of $T_n$. If $\bar H_n^{(r_X)}$ is symmetric and square-integrable, 
% then it admits the spectral representation
% \begin{align*}
%  \bar H_n^{(r_X)}(Z,Z') 
% = \sum_{l=1}^\infty \lambda_{l,n}\, \Psi_{l,n}(Z)\, \Psi_{l,n}(Z').    
% \end{align*}
% When the kernel $\bar H^{(r_X)}$ and the distribution $P_Z$ do not depend on $n$, 
% the operator $T_n$ reduces to a fixed Hilbert--Schmidt operator $T$ on $L^2(P_Z)$. 
% In this case, we omit the subscript $n$ and simply write 
% $\bar H^{(r_X)}$, $\mu_{P_Z}^{(r_X)}$, and $\{(\lambda_l, \Psi_l)\}_{l \geq 1}$ 
% for the kernel, mean embedding, and eigenvalue–eigenfunction sequence 
% associated with $T$, respectively.
% \begin{theorem}\label{thm:residual-mmd-fixed k P null}
% Suppose that $\varphi^{(r_X)}$ and $P_{Z}$ do not depend on $n$. If
% \begin{enumerate}
%     \item[1.] $\mE[r(X^{(2)})^2 \,\|R(V^{(2)})\|_\cH^2] < \infty$.
%     \item[2.] The density ratio estimator $\widehat r_X$ has negligible error relative to the residual features : 
%     \begin{align*}
%     \mE\!\left[(\widehat r_X(X^{(2)})-r_X(X^{(2)}))^2 \,\|R(V^{(2)})\|_\cH^2\right] 
%     = o\!\left(\frac{1}{n}\right).
%     \end{align*}
% \end{enumerate}
% Then
% \begin{align*}
%     n\cdot U_{n}^{\mathrm{res}}(\widehat{r}_X) 
%     \;\;\stackrel{D}{\longrightarrow}\;\;
%     \sum_{\ell \geq 1} \lambda_\ell \,(Z_\ell^2-1),
%     \qquad Z_\ell \iid N(0,1),
% \end{align*}
% where $\stackrel{D}{\longrightarrow}$ denotes convergence in distribution, 
% and $\{\lambda_\ell\}_{\ell \geq 1}$ are the eigenvalues of the integral operator 
% associated with the kernel $\bar H^{(r_X)}$.
% \end{theorem}
% We next present a more general result that implies \Cref{thm:residual-mmd-fixed k P null}
% \begin{theorem}\label{thm:residual-mmd-varying-kernel}
% Suppose that $P_Z$ is fixed, but the residual feature map $\varphi_{n}^{(r_X)}$ 
% and hence the associated kernel $\bar H_{n}^{(r_X)}$ may vary with $n$. 
% Let $T_n:L^2(P_Z)\to L^2(P_Z)$ be the Hilbert--Schmidt operator induced by 
% $\bar H_{n}^{(r_X)}$, with eigenvalues $\{\lambda_{\ell,n}\}_{\ell \geq 1}$. 
% Assume that
% \begin{enumerate}
%     \item[1.] For all $n \geq 2$, 
%     \begin{align*}
%     \mE\!\left[r(X^{(2)})^2 \,\|R_{n}(V^{(2)})\|_\cH^2\right] < \infty.
%     \end{align*}
%     \item[2.] For each $\ell \geq 1$, the eigenvalues converge as 
%     \begin{align*}
%        \lambda_{\ell,n} \;\to\; \lambda^{\star}_\ell \qquad (n \to \infty),
%     \end{align*}
%     and the limiting sequence satisfies
%     \begin{align*}
%        \sum_{\ell=1}^\infty (\lambda^{\star}_\ell)^2 < \infty.
%     \end{align*}
%     Moreover, the sequence $\{\lambda_{\ell,n}\}$ is uniformly square-summable:
%     \begin{align*}
%        \sup_{n \geq 1} \sum_{\ell=1}^\infty \lambda_{\ell,n}^2 < \infty.
%     \end{align*}
%     \item[3.] The density ratio estimator $\widehat r_X$ satisfies the weighted error condition
%     \begin{align*}
%     \mE\big[(\widehat r_X(X^{(2)})-r_X(X^{(2)}))^2 
%         \,\|R_n(V^{(2)})\|_\cH^2\big] 
%     = o\Big(\frac{1}{n}\Big).
%     \end{align*}
% \end{enumerate}
% Then
% \begin{align*}
% n\cdot U_n^{\mathrm{res}}(\widehat r_X) 
% \;\;\xrightarrow{D}\;\; 
% \sum_{\ell=1}^\infty \lambda^{\star}_\ell \,(Z_\ell^2-1),
% \qquad Z_\ell \iid N(0,1),
% \end{align*}
% where $\{\lambda^{\star}_\ell\}_{\ell \geq 1}$ is the limiting eigenvalue sequence of $\{T_n\}$.
% \end{theorem}

% \revised{To do
% \begin{enumerate}
%     \item[1.] (Uniform convergence) Changing $P_{Z_{n}}, \varphi^{(r_{X})}_{n}$ with $n$.
%     \item[2.] (Uniform convergence) Wild bootstrap
% \end{enumerate}}
% Since the limiting distribution is given by an infinite sum of shifted chi-squared variables, which is intractable in practice, we adopt a multiplier bootstrap approach for hypothesis testing.
% Let $W := (W_{1}, \ldots, W_{n})$ be $n$ i.i.d.~Rademacher random variables taking values in ${-1, +1}^{n}$, independent of the sample $\mathbb{Z}_{n} = (Z_{1}, \ldots, Z_{n})$.
% The wild bootstrap statistic is then defined as
% \begin{align}\label{equation : wild bootstrap MMD}
% \widehat{\mathrm{MMD}}^2_{\mathrm{wild}}
% := \frac{1}{n(n-1)} \sum_{i \neq j} W_{i} W_{j},\widetilde{H}^{(\widehat r_X)}(W_i, W_j),
% \end{align}
% where $\widetilde{H}^{(\widehat r_X)}$ denotes the kernel defined in \eqref{equation : estimated quadratic MMD}, which does not depend on $n$.
% \begin{proposition}
% \label{thm:wild-bootstrap-consistency}
% \revised{Find the condition}
% \begin{align*}
%     \lim_{n\to \infty}\sup_{P\in\mathcal{P}_{0}}\sup_{x \in \mathbb{R}}
%     \Big|
%     \mP_P\big( n \widehat{\mathrm{MMD}}^2_{\mathrm{wild}} \leq x \given \mathbb{Z}_n \big)
%     -
%     \mP_P\big( U_n^{\mathrm{res}}(r_X) \leq x \big)
%     \Big| =0.
% \end{align*}
% \end{proposition}


% Our main methodological contribution is the test based on the statistic 
% $n \cdot U_{n}^{\mathrm{res}}(\widehat{r}_{X})$, denoted by $\Psi$. 
% The test rejects the null hypothesis whenever 
% $n \cdot U_{n}^{\mathrm{res}}(\widehat{r}_{X})$ exceeds the critical value 
% $t_{1-\alpha}$, defined as the $(1-\alpha)$ quantile obtained via the Wild bootstrap. 
% Formally, \revised{Define what is $\mV^{(1)}, \mV^{(2)}$.}
% \begin{align*}
% \Psi(\mV^{(1)}, \mV^{(2)})
% = \mathds{1}\big\{n \cdot U_{n}^{\mathrm{res}}(\widehat{r}_{X}) \geq t_{1-\alpha}\big\}.
% \end{align*}
% From the results established earlier, $\Psi$ controls the type-I error at level $\alpha$, i.e.,
% \begin{align*}
%     \sup_{P \in \mathcal{P}_{0}} 
%     \mE_{P}\big[\Psi(\mV^{(1)}, \mV^{(2)})\big] \leq \alpha.
% \end{align*}

% \subsection{Consistency result}
% Define 
% \begin{align*}
%     \gamma_{\mathrm{res}}^2 
%     := \mathrm{MMD}_{\mathrm{res}}^2(P_{XY}^{(1)},P_{XY}) 
%     &= \mathbb{E}\big[k^{\mathrm{res}}(V_1^{(1)}, V_2^{(1)})\big] 
%     + \mathbb{E}\big[r_X(X_1^{(2)}) r_X(X_2^{(2)}) k^{\mathrm{res}}(V_1^{(2)}, V_2^{(2)})\big] \\
%     &\quad - 2\,\mathbb{E}\big[r_X(X_2^{(2)}) k^{\mathrm{res}}(V_1^{(1)}, V_2^{(2)})\big],
% \end{align*}
% where $k^{\mathrm{res}}(v,v') := \langle R(v), R(v')\rangle_\cH$. By construction, $\gamma_{\mathrm{res}}^2$ is strictly positive under the alternative. We next show that the test $\Psi$ is consistent against any fixed alternative. In particular, mirroring the argument of \Cref{thm:residual-mmd-fixed k P null}, the following theorem establishes consistency under $H_{1}$ in \eqref{Eq: hypothesis}.


% \begin{theorem}\label{thm:fixed k p consistent}
% Suppose $P_{XY}^{(1)}$, $P_{XY}^{(2)}$, and the feature map $\varphi^{(r_X)}$ 
% do not depend on $n$, and consider the alternative $H_{1}$ in 
% \Cref{Eq: hypothesis}. If $\varphi^{(r_X)}$ is the feature map of a characteristic kernel and the following conditions hold:
% \begin{enumerate}
%     \item $\mE\!\left[r_X(X^{(2)})^2 \|R(V^{(2)})\|_\cH^2\right] < \infty.$
%     \item $\mE\!\left[r_X(X^{(2)})^4 \|R(V^{(2)})\|_\cH^4\right] < \infty.$
%     \item $\mE\!\left[\big(\widehat r_X(X^{(2)}) - r_X(X^{(2)})\big)^2 
%     \|R(V^{(2)})\|_{\cH}^2\right] = o(1/n).$
% \end{enumerate}
% Then the test $\Psi$ is consistent, meaning it has asymptotic power 1.
% \end{theorem}
% \revised{Explain what each condition implies.}

\newpage
The next section illustrates the numerical performance of the proposed tests in comparison to existing methods. 


\section{Numerical Experiments} \label{Section: Numerical Experiments}
In this section, we evaluate the empirical performance of the proposed tests, alongside existing methods from the literature,  across various scenarios. Within each scenario, we compare the conditional independence testing (CIT) approach described in \Cref{Section: Approach via Conditional Independence Testing} with the density ratio-based testing (DRT) approach described in \Cref{Section: Approach via Density Ratio Estimation}. A brief overview of these methods is provided in \Cref{Section: Overview of Testing Methods}, with additional implementation details available in \Cref{section: Additional Experiments}. 

We empirically evaluate the type I error and power of these methods using both synthetic datasets in \Cref{Section: Synthetic Data Examples} and two real-world datasets in \Cref{Section: Real Data Analysis}. Our simulation studies with synthetic datasets cover three distinct scenarios, each featuring both bounded and unbounded marginal density ratios. This setup allows us to explore a range of situations from relatively simple cases (with bounded density ratios) to more complex and challenging ones (with unbounded density ratios). 

In all simulation studies, the dimension of the covariates $X$ is fixed at $p = 10$. The simulation results are averaged over 500 repetitions with a significance level of $\alpha = 0.05$. For DRT methods, we employ a probabilistic classification approach using linear logistic regression for density ratio estimation \citep[Section 3]{sugiyama2010density}. For CIT methods relying on regression estimation, such as the Generalized Covariance Measure \citep{shah2020hardness} and the Projected Covariance Measure \citep{lundborg2022projected}, we use a Random Forest model as the underlying regression method. On the other hand, for the WGSC \citep{williamson2023general}, we utilize XGBoost as it appears to perform best in our simulation scenarios. 

It is crucial to note that the efficacy and validity of these methods depend on specific assumptions, which vary across different approaches. For example, DRT methods rely heavily on accurate density ratio estimation, while CIT methods depend on reliable estimation of conditional operators, such as regression functions. We aim to emphasize this distinction by providing a comprehensive evaluation of these approaches for conditional two-sample testing. This analysis sheds light on how each method performs under different conditions, guiding practitioners in choosing the most suitable approach for their specific applications. 


\subsection{Overview of Testing Methods} \label{Section: Overview of Testing Methods}
This subsection outlines the testing methods employed in our experiments. Further details on the implementation of these methods can be found in \Cref{section: Additional Experiments}. We denote the single-split classifier-based test in \Cref{Section: Classifier-based Approach} as CLF and its cross-fit version as $^{\dagger}$CLF. Both classifier-based test statistics are built on a specific form of classifiers detailed in \Cref{Section: Experimental Details}. Moreover, we denote the linear-time MMD in \Cref{Section: Linear-Time MMD} as MMD-$\ell$ and its cross-fit version as $^{\dagger}$MMD-$\ell$. \revised{The block-wise MMD test introduced in \Cref{Section: Block-wise MMD} is denoted as MMDb, and its cross-fit version as $^\dagger$MMDb.} Additional conditional two-sample testing methods included in our experiments are as follows: 

\vspace{0.5em}

\noindent\textbf{CP}: 
The conformal prediction (CP) test utilizes a conformity score to produce a weighted rank sum test statistic. This statistic is constructed by estimating both marginal and conditional density ratios, which can be approached using various density ratio estimation methods. For further details, please refer to \cite{hu2024two}.

\vspace{0.5em}

\noindent\textbf{DCP}: The debiased conformal prediction (DCP) test refines the CP test by reducing bias through the use of Neyman orthogonality and using cross-fitting to improve efficiency. This enhancement guarantees asymptotic normality under certain conditions. Further technical details and theoretical guarantees are described in \cite{chen2024biased}.

\vspace{0.5em}

For CIT methods, we employ one kernel-based and three regression-based testing approaches. All of these CIT methods are implemented via \Cref{Algorithm: Converting C2ST into CIT}. We empirically observe in \Cref{Appendix: With_Without_Algorithm_1} that the performance of the CIT methods remains largely consistent regardless of whether \Cref{Algorithm: Converting C2ST into CIT} is applied or not, especially when the sample size is large. The following methods are included in our experiments: 

\vspace{0.5em}

\noindent\textbf{RCIT}: The randomized conditional independence test (RCIT) approximates the kernel conditional independence test by leveraging random Fourier features, allowing it to scale linearly with sample size. We use the default options in its implementation, and further details are provided in \cite{strobl2019approximate}.

\vspace{0.5em}

\noindent\textbf{GCM}: The generalized covariance measure (GCM) by \cite{shah2020hardness} utilizes the normalized covariance between residuals from regression models as a test statistic. This approach provides a flexible framework that can be adapted to various settings by selecting appropriate regression techniques. 

\vspace{0.5em}

\noindent\textbf{PCM}: The projected covariance measure (PCM) is a variation of the GCM applied to a transformed version of $X$. For our simulations, we follow Algorithm 1 from \cite{lundborg2022projected}. This method retains the general structure of the GCM while introducing a projection step, which enhances power, particularly when the conditional covariance is zero or near zero. 

\vspace{0.5em}

\noindent\textbf{WGSC}: This testing procedure proposes a general framework for nonparametric inference on interpretable, algorithm-agnostic variable importance. In our simulation, we follow the approach outlined in \citet[Algorithm 3]{williamson2023general}, which utilizes sample splitting and cross-fitting.

\vspace{0.5em}

\noindent The code that reproduces all simulation results is available at: \url{https://github.com/suman-cha/Cond2ST}.


\subsection{Synthetic Data Examples} \label{Section: Synthetic Data Examples}
We design three synthetic data scenarios to evaluate the performance of conditional two-sample testing methods under different conditions. Each scenario is implemented with both unbounded (U) and bounded (B) density ratios to assess how the difficulty of density ratio estimation affects the performance of each method. The marginal distributions of $X$ remain consistent across scenarios with unbounded density ratios, and  similarly, across scenarios with bounded density ratios. 


For the unbounded case (U), we employ Gaussian distributions for marginal distributions of $X$. Specifically, for $j=1$, samples are drawn from a standard Gaussian distribution, i.e., $x^{(1)} \sim N(0, I_p)$, where $I_p$ is the identity matrix of dimension $p$. For $j=2$, we introduce a covariate shift by sampling from a Gaussian distribution with mean vector $\mu = (1, 1, -1, -1, 0, \ldots, 0)^{\top}$ and the same covariance structure, i.e., $x^{(2)} \sim N(\mu, I_p)$. In the bounded case (B), we truncate the support of both distributions to $[-0.5, 0.5]$ in each dimension, resulting in truncated Gaussian distributions, $x^{(1)} \sim TN(0, I_p)$ and $x^{(2)} \sim TN(\mu, I_p)$, where $\mu$ is the same as in the unbounded case. 

\vspace{0.5em}

\noindent \textbf{Scenario 1: Linear Model with Mean Shift.}
Inspired by the work of \citet{hu2024two}, this scenario investigates the efficacy of testing methods in detecting the mean difference between two linear models. For each $j \in \{1, 2\}$, we set $y^{(j)} \given x^{(j)} = \delta^{(j)} + x^{(j) \top}\beta + \epsilon^{(j)}$, where $\epsilon^{(j)}$ follows a $t$-distribution with 2 degrees of freedom. The regression coefficient $\beta$ is set to  $(1, -1, -1, 1, 0, \ldots, 0)^{\top}$. Under the null hypothesis, we set $\delta^{(1)} = \delta^{(2)} = 0$, while for the alternative hypothesis, we introduce a mean shift by setting $\delta^{(1)} = 0$ and $\delta^{(2)} = 0.5$, thereby creating a difference in the two conditional distributions. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{Figures/Scenario1_ver1.pdf}
    \captionsetup{skip=0pt}  
    \caption{Rejection rates for Scenario 1 under null and alternative hypotheses, shown for both unbounded (U) and bounded (B) settings. Results are averaged over 500 repetitions with significance level $\alpha = 0.05$.}
    \label{fig:Scenario 1}
\end{figure}


\medskip

\noindent \textbf{Scenario 2: High Variability in Conditional Distribution.}
We also investigate the effect of high variability in the conditional distribution, slightly modifying the example outlined in \citet[][Section 6.2]{CHATTEJEE2024conditional}. Under the null hypothesis, we model the conditional distributions as 
$y^{(j)} \given x^{(j)} \sim N\left(x^{(j) \top}\beta^{(j)}, (\sigma^{(j)})^2 \right)$,
where $\beta^{(j)} = \mathbf{1}_{p}$ defined as a $p$-dimensional vector of ones and $(\sigma^{(j)})^2 = 10^2$ for both $j \in \{1, 2\}$. This implies that $\beta^{(1)}$ and $\beta^{(2)}$ are identical under the null hypothesis.
For the alternative hypothesis, we modify $\beta^{(2)}$ to $(1, \ldots, 1, 0)^\top$ and introduce heteroscedasticity by varying the variance for $j = 2$ as $(\sigma^{(2)})^2 = 10\left(1 + \exp\left(-\lVert x^{(2)} - 0.5 \mathbf{1}_{p} \rVert^2_{2}/64 \right)\right)$. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{Figures/Scenario2_ver1.pdf}
    \captionsetup{skip=0pt}  
    \caption{Rejection rates for Scenario 2 under null and alternative hypotheses, shown for both unbounded (U) and bounded (B) settings. Results are averaged over 500 repetitions with significance level $\alpha = 0.05$.}
    \label{fig:Scenario 2}
\end{figure}

\medskip

\noindent \textbf{Scenario 3: Post-Nonlinear Model.}
Our final scenario considers a post-nonlinear (PNL) model, which is widely used in causal predictive inference~\citep{zhang2017causal,Li2023knn}. It tests the capability of the methods to detect differences in non-linear relationships between variables. We model the conditional distributions as $y^{(j)} \given x^{(j)} = f^{(j)}(x^{(j)\top}\mathbf{1}_{p} + 2\epsilon)$, where $\epsilon \sim N(0,1)$ and $j \in \{1, 2\}$. Under the null hypothesis, we set $f^{(j)}(x) = \cos(x)$ for both $j \in \{1, 2\}$, while for the alternative hypothesis, $f^{(2)}(x)$ is randomly sampled from the set $\{x, x^2, x^3, \sin(x), \tanh(x)\}$.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{Figures/Scenario3_ver1.pdf}
    \captionsetup{skip=0pt}  
    \caption{Rejection rates for Scenario 3 under null and alternative hypotheses, shown for both unbounded (U) and bounded (B) settings. Results are averaged over 500 repetitions with significance level $\alpha = 0.05$.}
    \label{fig:Scenario 3}
\end{figure}

Our experimental results provide several key insights into the performance of conditional two-sample testing methods across diverse scenarios. A consistent pattern observed throughout all scenarios is the superior performance of DRT methods in bounded settings compared to unbounded settings. This improvement can be attributed to the relative ease of density ratio estimation when the density ratio is bounded, leading to more stable results. In contrast, CIT methods exhibit relatively consistent performance regardless of whether the density ratios are bounded or unbounded. 

In Scenario 1, the classifier-based test shows the most significant improvement in performance when transitioning from unbounded to bounded cases. However, MMD-$\ell$ shows lower sensitivity in detecting mean shifts compared to other DRT methods. Among CIT methods, RCIT and GCM exhibit the best performance in this scenario. Scenario 2 highlights the strengths of MMD-$\ell$, which only considers the marginal density ratio of $X$, in comparison to other DRT methods that account for conditional density ratios. MMD-$\ell$ shows a distinct advantage in this scenario. CIT methods also generally perform well under these conditions, showing their robustness to complex distributional changes. In Scenario 3, which tests the ability of methods to detect non-linear relationships, all DRT methods improve performance in the bounded case. Among CIT methods, there is no significant difference in performance, except for GCM and WGSC. GCM shows improved performance in the bounded case, while WGSC shows degraded performance.

These results underscore the critical role of accurate density ratio estimation in determining the performance of DRT methods. While CIT methods demonstrate consistent performance across both bounded and unbounded cases, suggesting their utility in a wide range of practical scenarios, they also have limitations. CIT methods, particularly regression-based approaches like GCM, PCM, and WGSC, can be sensitive to the choice of regression model, as we demonstrate in \Cref{section: Additional Experiments}. Notably, RCIT exhibits high type I error rates in all scenarios when sample sizes are relatively small, suggesting that caution is needed when applying RCIT to limited datasets. On the other hand, some methods show overly conservative behavior in certain scenarios. The cross-validated versions of DRT methods ($^{\dagger}$MMD-$\ell$ and $^\dagger$CLF) consistently show power gains compared to their non-cross-validated counterparts as discussed in \Cref{Section: Classifier-based Approach} and \Cref{Section: Linear-Time MMD}. Overall, our findings offer important insights into the strengths and limitations of different conditional two-sample testing methods.



\subsection{Real Data Analysis} \label{Section: Real Data Analysis}
We further evaluate the performance of our proposed approaches on two real-world datasets: the diamonds dataset and the superconductivity dataset. 
Following \citet{kim2023conditional}, we treat each dataset as a population from which we draw samples, allowing for controlled experiments with known ground truth. Prior to analysis, we apply standard scaling to both $X$ and $Y$ variables. To introduce covariate shift, we implement biased sampling procedures. Specifically, we sample $X^{(1)}$ uniformly from the original feature space, while $X^{(2)}$ is sampled with probability proportional to $\exp(-x_1^2)$, where $x_1$ denotes the first feature of $X$. For the response variable $Y$, under the null hypothesis, we employ uniform sampling for both $Y^{(1)}$ and $Y^{(2)}$, \revised{breaking the original $X$-$Y$ dependence to ensure identical conditional distributions}. Under the alternative hypothesis, $Y^{(1)}$ is sampled uniformly, while $Y^{(2)}$ is sampled with probability proportional to $\exp(-y)$, where $y$ represents the values of $Y$ in the dataset. Figure \ref{fig:real DRT} illustrates the performance of the DRT methods on both datasets, using linear logistic (\revised{LLR}) and kernel logistic regression (KLR) for density ratio estimation. 



\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{Figures/Real_DRT.pdf}
	\caption{Performance comparison of DRT methods on diamonds and superconductivity datasets using \revised{LLR} and KLR for density ratio estimation. Rejection rates are averaged over 500 repetitions with $\alpha = 0.05$, under null (\emph{top}) and alternative (\emph{bottom}) hypotheses.}
	\label{fig:real DRT}
\end{figure}


\vspace{0.5em}
\noindent\textbf{Diamonds dataset.} The diamonds dataset, available in the R package \texttt{ggplot2}, consists of 53,490 observations and 10 features, including price, carat, clarity and color. In our analysis, we set the price variable as $Y$, and use the 6 numerical variables \texttt{(carat, depth, table, x, y, z)} as $X$. As illustrated in Figure \ref{fig:real DRT}, most DRT methods exhibit good type I error control under both \revised{LLR} and KLR, with rejection rates generally close to the significance level $\alpha$. Under the alternative hypothesis, we observe a clear trend of increasing power with sample size for all methods. Particularly, the cross-validated versions ($^{\dagger}$MMD-$\ell$ and $^\dagger$CLF) exhibit improved power, consistent with our observations in the synthetic data examples.


\vspace{0.5em}
\noindent\textbf{Superconductivity dataset. }
The superconductivity dataset, obtained from the UCI Machine Learning Repository and compiled by \citet{Hamidieh2018}, presents a more complex and high-dimensional challenge compared to the diamonds dataset. It comprises 81 features extracted from 21,263 superconductors, with the critical temperature at which the material transitions to a superconducting state serving as the response variable $Y$. The results reveal a significant contrast between density ratio estimation methods based on \revised{LLR} and KLR. Under \revised{LLR}, several DRT methods, especially the classifier-based tests, struggle to control the type I error, with rejection rates far exceeding the significance level. Conversely, when using KLR for density ratio estimation, DRT methods show improved type I error control. 

\vspace{0.5em}

These empirical findings emphasize the importance of carefully considering the nature of the data and the choice of density ratio estimation techniques when applying DRT methods for conditional two-sample testing. The performance of different methods can vary significantly, indicating the need for careful method selection and, potentially, more advanced approaches when handling complex and high-dimensional data. While we focus on DRT methods in this section, experimental results for CIT methods are presented in \Cref{Appendix: CIT Real Data Results} for completeness. 



\section{Conclusion} \label{Section: Conclusion}
In this paper, we shed new light on the relatively underexplored problem of conditional two-sample testing. We begin by characterizing the fundamental difficulty of the problem and highlighting the importance of assumptions to make it feasible. We then introduce two general frameworks: (1) converting conditional independence tests into conditional two-sample tests and (2) transforming the problem of comparing conditional distributions into marginal distributions based on density ratio estimation. Both approaches offer significant flexibility, allowing one to leverage well-developed tools to effectively tackle the problem. 


Our work opens up several interesting directions for future work. One promising avenue is to extend our framework to conditional $K$-sample testing with a general $K \geq 2$. Such an extension would expand the applicability of our framework beyond the comparison of just two groups. This setting is related to conditional independence testing where $Z$ is a categorical random variable taking values in $\{1,2,\ldots,K\}$. We expect our results established in \Cref{Section: Approach via Conditional Independence Testing} to serve as a cornerstone for this extension. Another direction worth exploring is establishing a framework for conditional two-sample testing based on resampling methods. One promising approach is the Sampling Importance Resampling (SIR) algorithm \citep[][Chapter 6.3]{GiveHoet12}, which allows us to obtain an approximate sample from the distribution with density $f_{YX}$. Future work can focus on methods that compare the sample from $P_{XY}^{(1)}$ with the approximate sample obtained from the SIR algorithm. Finally, one can explore other two-sample test statistics beyond those listed in \Cref{Section: Approach via Density Ratio Estimation}. Of particular interest is the block-wise MMD statistic~\citep{zaremba2013b}. This statistic has a tractable limiting distribution, while achieving lower variance with a slight increase in computational cost compared to the linear-time MMD statistic. We leave all these interesting topics for future work.

\paragraph{Acknowledgments} We acknowledge support from the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2022R1A4A1033384),
and the Korea government (MSIT) RS-2023-00211073. We are grateful to Gyumin Lee for the careful proofreading.

\textcolor{red}{(update the reference)}
\bibliographystyle{apalike}
\bibliography{reference}
 

\appendix
\paragraph{Overview of Appendices.}
In \Cref{section: Proofs}, we present the proofs omitted in the main paper. \Cref{Section: Supporting Lemmas} gathers several lemmas that support these proofs. Finally, \Cref{section: Additional Experiments} provides implementation details of numerical experiments and additional simulation results.

\section{Proofs}\label{section: Proofs}
\paragraph{Notation.} For real sequences $(a_n)$ and $(b_n)$, we say that $a_n \lesssim b_n$ if there exists a constant $C>0$ such that $a_n \leq C b_n$ for all $n$. Let $(X_{P,n})_{n \in \mathbb{N}, P\in \mathcal{P}}$ be a family of sequences of random variables determined by $P \in \mathcal{P}$. We say that $X_{P,n} = X_n = o_{\mathcal{P}}(n^{-a})$ \revised{and $X_n = O_{\mathcal{P}}(n^{-a})$ to mean respectively for all $\epsilon > 0$,}
\begin{gather*}
    \sup_{P \in \mathcal{P}} \,\mP_P\!\left(n^{a}|X_{P,n}| > \epsilon \right) \;\to\; 0, \quad\text{and} \\
    \revised{\text{there exists } M > 0 \text{ such that } 
      \sup_{n \in \mathbb{N}} \sup_{P \in \mathcal{P}} 
      \mP_P\!\left(n^{a}|X_{P,n}| > M \right) < \epsilon.}
\end{gather*}
For a positive integer $n$, we use the shorthand $[n]$ to denote the set $\{1,\ldots,n\}$.

\subsection{\texorpdfstring{Proof of \Cref{Theorem: Asymptotic Normality of Classification Accuracy}}{Proof of Theorem: Asymptotic Normality of Classification Accuracy}} 
\label{Section: Proof: Asymptotic Normality of Classification Accuracy}

We analyze the numerator and the denominator of $\widehat{\mathrm{Acc}}$, separately. In particular, we first show that the numerator converges to a Gaussian distribution and the denominator is ratio-consistent to the population-level standard deviation under \Cref{Assumption: classifier}.  
\paragraph{Analysis of the numerator.} 
Starting with the numerator, let us rewrite 
\begin{align*}
	\overline{A}_1 + \overline{A}_2 - 1  & = \frac{1}{m} \sum_{i=1}^m \bigl[ \mathds{1}\{\widehat{h}(V_i^{(1)})=1\} + r_X(X_i^{(2)})\mathds{1}\{\widehat{h}(V_i^{(2)})=2\} - 1\bigr] \\
	& \hskip 5em + \frac{1}{m} \sum_{i=1}^m  \bigl\{ \widehat{r}_X(X_i^{(2)}) - r_X(X_i^{(2)}) \bigr\} \mathds{1}\{\widehat{h}(V_i^{(2)})=2\} \\
	& = \frac{1}{m} \sum_{i=1}^m \underbrace{\bigl[ \mathds{1}\{\widehat{h}(V_i^{(1)})=1\} + r_X(X_i^{(2)})\mathds{1}\{\widehat{h}(V_i^{(2)})=2\} - 1\bigr]}_{\coloneqq L_i(\widehat{h})} + o_{\mathcal{P}_0}(m^{-1/2}),
\end{align*}
where the last approximation holds since 
\begin{align*}
	\bigg|\frac{1}{m} \sum_{i=1}^m  \bigl\{ \widehat{r}_X(X_i^{(2)}) - r_X(X_i^{(2)}) \bigr\} \mathds{1}\{\widehat{h}(V_i^{(2)})=2\} \bigg| \leq \sqrt{\frac{1}{m} \sum_{i=1}^m  \bigl\{ \widehat{r}_X(X_i^{(2)}) - r_X(X_i^{(2)}) \bigr\}^2},
\end{align*}
and the upper bound is $o_{\mathcal{P}_0}(m^{-1/2})$ due to \Cref{Assumption: classifier}(b). Thus $\overline{A}_1 + \overline{A}_2 - 1$ is dominated by the average of $L_i(\widehat{h})$ values. Given this and Slutsky's theorem, it suffices to study the limiting distribution of the sample average of $L_i(\widehat{h})$. Indeed, under \Cref{Assumption: classifier}(a), the conditional central limit theorem (\Cref{Lemma: conditional clt}) yields that 
\begin{align*}
	\lim_{n \rightarrow \infty}\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} \Bigg| \mP_P\Biggl( \frac{\frac{1}{\sqrt{m}}\sum_{i=1}^m L_i(\widehat{h})}{\{\mV[L(\widehat{h}) \given \widehat{h}]\}^{1/2}} \leq t \Biggr) - \Phi(t) \Bigg| = 0.
\end{align*}


\paragraph{Consistency of the variance estimate.} We next show the ratio-consistency of the variance estimator. Observe that
\begin{align*}
	\mV[L(\widehat{h}) \given \widehat{h}] = \underbrace{\mV[ \mathds{1}\{\widehat{h}(V_i^{(1)})=1\} \given \widehat{h}]}_{\coloneqq  \sigma_1^2} + \underbrace{\mV\bigl[ r_X(X_i^{(2)})\mathds{1}\{\widehat{h}(V_i^{(2)})=2\} \bigr]}_{\coloneqq  \sigma_2^2},
\end{align*}
and 
\begin{align*}
	\bigg|\frac{\widehat{\sigma}_1^2+ \widehat{\sigma}_2^2}{\sigma_1^2 + \sigma_2^2} - 1 \bigg| \leq \bigg|\frac{\widehat{\sigma}_1^2 - \sigma_1^2}{\sigma_1^2} \bigg| + \bigg|\frac{\widehat{\sigma}_2^2 - \sigma_2^2}{\sigma_2^2} \bigg|. 
\end{align*}
Therefore, in order to show the ratio consistency of $\widehat{\sigma}_1^2+ \widehat{\sigma}_2^2$, it suffices to show the ratio consistency of $\widehat{\sigma}_1^2$ and $\widehat{\sigma}_2^2$, individually. To this end, we use conditional Chebyshev's inequality and show
\begin{align*}
	\sup_{P \in \mathcal{P}_0}\mP_P\bigl( |\widehat{\sigma}_1^2/\sigma_1^2 - 1| \geq t \given \widehat{h} \bigr) \leq \frac{1}{t^2} \sup_{P \in \mathcal{P}_0}\mV_P(\widehat{\sigma}_1^2/\sigma_1^2 \given \widehat{h}) \leq \frac{1}{t^2c_1(1-c_2)m},
\end{align*}
for sufficiently large $n$ and for all $t > 0$, under \Cref{Assumption: classifier}(a). Hence $\widehat{\sigma}_1^2/\sigma_1^2$ converges to one in probability uniformly over $\mathcal{P}_0$. On the other hand, letting $A_{2,i} \coloneqq  \widehat{r}_X(X_i^{(2)}) \mathds{1} \{\widehat{h}(V_i^{(2)}) = 2\}$, we have
\begin{align*}
	\widehat{\sigma}_2^2 & = \frac{1}{m-1}\sum_{i=1}^m \biggl[ A_{2,i} - \frac{1}{m} \sum_{j=1}^m A_{2,j} \biggr]^2 \\
	& + \frac{1}{m-1}\sum_{i=1}^m \biggl[ \bigl(\widehat{A}_{2,i} - A_{2,i}\bigr) - \frac{1}{m} \sum_{j=1}^m \bigl( \widehat{A}_{2,j} -  A_{2,j}\big) \biggr]^2\\
	& + \frac{2}{m-1}\sum_{i=1}^m\biggl[ A_{2,i} - \frac{1}{m} \sum_{j=1}^m A_{2,j} \biggr] \cdot \biggl[ \bigl(\widehat{A}_{2,i} - A_{2,i}\bigr) - \frac{1}{m} \sum_{j=1}^m \bigl( \widehat{A}_{2,j} -  A_{2,j}\big) \biggr] \\
	& \coloneqq  (\mathbb{I}) + (\mathbb{II}) + (\mathbb{III}). 
\end{align*}
Similarly as before, the term $(\mathbb{I})/\sigma_2^2$ converges to one in probability uniformly over $\mathcal{P}_0$ under \Cref{Assumption: classifier}(a). It can be further shown that the term $(\mathbb{II})/\sigma_2^2$ is $o_{\mathcal{P}_0}(1)$ by Markov's inequality combined with \Cref{Assumption: classifier}(b). Lastly, the term ($\mathbb{III}$) satisfies $(\mathbb{III}) \leq \sqrt{(\mathbb{I}) \times (\mathbb{II})}$, which is again $o_{\mathcal{P}_0}(1)$. Therefore, $\widehat{\sigma}_2^2/\sigma_2^2$ converges to one in probability uniformly over $\mathcal{P}_0$. This further proves that $\sqrt{(\sigma_1^2 + \sigma_2^2)/(\widehat{\sigma}_1^2 + \widehat{\sigma}_2^2)} = 1 + o_{\mathcal{P}_0}(1)$ by \citet[][Lemma S7]{lundborg2022projected}.

Putting all pieces together with \Cref{Lemma: uniform Slutsky}(b) proves the claim. 


\subsection{\texorpdfstring{Proof of \Cref{Corollary: Asymptotic Normality of CV Accuracy}}{Proof of Corollary: Asymptotic Normality of CV Accuracy}}

For each $j \in \{1,\ldots,K\}$, the proof of \Cref{Theorem: Asymptotic Normality of Classification Accuracy} shows that $\sqrt{\sigma_{1}^2 + \sigma_2^2}/\sqrt{\widehat{\sigma}_{1,j}^2 + \widehat{\sigma}_{2,j}^2} = 1 + o_{\mathcal{P}_0}(1)$. Thus, by \Cref{Lemma: uniform Slutsky}(b), it is enough to show the asymptotic normality of  
\begin{align*}
	\frac{1}{\sqrt{K}}\sum_{j = 1}^K \frac{\sqrt{m}(\overline{A}_{1,j} + \overline{A}_{2,j} - 1)}{\sqrt{\sigma_{1}^2 + \sigma_2^2}}.
\end{align*}
Without loss of generality, denote the sample indices of $D_1,D_2,\ldots,D_K$ as
\begin{align*}
	I_1 = \bigl\{1,\ldots,m\bigr\}, \ I_2 = \bigl\{m+1,\ldots,\ 2m \bigr\},\ldots, I_K =\bigl\{ m(K-1) + 1,\ldots, mK\bigr\}.
\end{align*}
Then the proof of  \Cref{Theorem: Asymptotic Normality of Classification Accuracy} establishes that 
\begin{align*}
	\frac{1}{\sqrt{K}}\sum_{j = 1}^K \frac{\sqrt{m}(\overline{A}_{1,j} + \overline{A}_{2,j} - 1)}{\sqrt{\sigma_{1}^2 + \sigma_2^2}} = \sqrt{\frac{1}{mK}} \sum_{j=1}^K \underbrace{\Biggl\{\sum_{i \in I_j} \frac{A_{1,i} + A_{2,i} - 1}{\sqrt{\sigma_{1}^2 + \sigma_2^2}} \Biggr\}}_{\coloneqq  B_j} + o_{\mathcal{P}_0}(1),
\end{align*}
where $A_{1,i} + A_{2,i} - 1 \coloneqq  \mathds{1}\{\widehat{h}(V_i^{(1)}) = 1\} + r_X(X_i^{(2)}) \mathds{1}\{\widehat{h}(V_i^{(2)})= 2\} - 1$. Note that $B_1,\ldots,B_K$ are mutually independent conditional on $\widehat{h}$. As in the proof of \Cref{Theorem: Asymptotic Normality of Classification Accuracy}, we apply the conditional central limit theorem (\Cref{Lemma: conditional clt}) to the average of $B_1,\ldots,B_K$ conditional on $\widehat{h}$ under \Cref{Assumption: classifier}, which completes the proof of \Cref{Corollary: Asymptotic Normality of CV Accuracy}. 



\subsection{\texorpdfstring{Proof of \Cref{Theorem: Asymptotic Normality of Linear Time MMD}}{Proof of Theorem: Asymptotic Normality of Linear Time MMD}} \label{Section: Proof: Asymptotic Normality of linear time MMD}
The proof consists of two parts as in the proof of \Cref{Theorem: Asymptotic Normality of Classification Accuracy}. In the first part, we investigate the numerator of $\widehat{\mathrm{MMD}}^2_{\ell}$, i.e., $\overline{S}$, whereas in the second part, we show the consistency of the denominator to the population variance under \Cref{Assumption: linear-time MMD}. The proof is then completed by applying \Cref{Lemma: uniform Slutsky}(b). 

\bigskip 

\noindent \textbf{Analysis of the numerator.} Using the fact that a kernel can be expressed as an inner product of feature maps, $k(x,y) = \langle \psi(x), \psi(y) \rangle$, we can rewrite $\overline{S}$ as
\begin{align*}
	\overline{S}=\frac{1}{m} \sum_{i=1}^m \big\langle \psi\bigl(V_i^{(1)}\bigr)-\widehat{r}_X\bigl(X_i^{(2)}\bigr)  \psi\bigl( V_i^{(2)} \bigr), \ \psi\bigl( V_{i+m}^{(1)} \bigr)- \widehat{r}_X\bigl( X_{i+m}^{(2)} \bigr) \psi\bigl( V_{i+m}^{(2)} \bigr)\big\rangle.
\end{align*}
By adding and subtracting $r_X(X_i^{(2)}) \psi(V_i^{(2)})$ and $r_X(X_{i+m}^{(2)})\psi(V_{i+m}^{(2)})$, $\overline{S}$ can be written as the sum of the four terms given as:
\begin{align*}
&\mathbb{(I)}\coloneqq \frac{1}{m} \sum_{i=1}^m\underbrace{\big\langle\psi\bigl(V_i^{(1)} \bigr)-r_X\big(X_i^{(2)}\big)  \psi\bigl( V_i^{(2)} \bigr), \psi\bigl( V_{i+m}^{(1)} \bigr)-r_X\big(X_{i+m}^{(2)}\big) \psi\bigl( V_{i+m}^{(2)} \bigr)\big\rangle}_{\coloneqq S_i}, \\ 
&\mathbb{(II)} \coloneqq  \frac{1}{m} \sum_{i=1}^m\underbrace{\big\langle\psi\bigl( V_i^{(1)} \bigr)-r_X\bigl( X_i^{(2)} \bigr)  \psi\bigl( V_i^{(2)} \bigr), \psi\bigl( V_{i+m}^{(2)} \bigr)\big\rangle \cdot\big\{\widehat{r}_X\bigl( X_{i+m}^{(2)} \bigr)-r_X\big(X_{i+m}^{(2)}\big)\big\}}_{\coloneqq \widehat{S}_{i,a}}, \\
&\mathbb{(III)} \coloneqq  \frac{1}{m} \sum_{i=1}^m\underbrace{\big\langle \psi\bigl( V_i^{(2)} \bigr), \psi\bigl( V_{i+m}^{(1)} \bigr)-r_X\big(X_{i+m}^{(2)}\big) \psi\bigl( V_{i+m}^{(2)} \bigr)\big\rangle \cdot\big\{\widehat{r}_X\bigl(X_i^{(2)}\bigr)-r_X\big(X_i^{(2)}\big)\big\}}_{\coloneqq  \widehat{S}_{i,b}}, \\ 
&\mathbb{(IV)} \coloneqq  \frac{1}{m} \sum_{i=1}^m \underbrace{\big\langle \psi\bigl( V_i^{(2)} \bigr), \psi\bigl( V_{i+m}^{(2)} \bigr)\big\rangle\big\{\widehat{r}_X\bigl(X_i^{(2)}\bigr)-r_X\big(X_i^{(2)}\big)\big\} \cdot\big\{\widehat{r}_X\bigl( X_{i+m}^{(2)} \bigr)-r_X\big(X_{i+m}^{(2)}\big)\big\}}_{\coloneqq \widehat{S}_{i,c}}.
\end{align*}
The first term $\mathbb{(I)}$ does not involve an estimate of the density ratio and will be asymptotically Gaussian since it is the sum of i.i.d.~random variables under the null hypothesis. The other terms $\mathbb{(II)}$, $\mathbb{(III)}$, and $\mathbb{(IV)}$ are asymptotically negligible under the conditions of the theorem. Hence $\overline{S}$ will be dominated by $\mathbb{(I)}$. Let us analyze each term separately. 

\medskip 


\begin{enumerate}
	\item \textbf{Term $\mathbb{(I)}$.} Define $\mV_P[S_1] = \sigma_P^2$. Then under \Cref{Assumption: linear-time MMD}(a), \Cref{Lemma: Uniform CLT} yields
	\begin{align*}
		\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} |\mP_P(\sqrt{m} \sigma_P^{-1}\mathbb{(I)} \leq t) - \Phi(t)| \to 0.
	\end{align*} 
 \item \textbf{Terms $\mathbb{(II)}$ and $\mathbb{(III)}$.} We only analyze the term $\mathbb{(II)}$ since $\mathbb{(III)}$ can be handled in exactly the same way by symmetry. Under the null hypothesis, by the law of total expectation, it can be seen that the expectation of the summands of $\mathbb{(II)}$ is equal to zero:
 \begin{align*}
 	\mE\bigl[ \big\langle\psi\bigl( V_i^{(1)} \bigr)-r_X\bigl( X_i^{(2)} \bigr)  \psi\bigl( V_i^{(2)} \bigr), \psi\bigl( V_{i+m}^{(2)} \bigr)\big\rangle \cdot\big\{\widehat{r}_X\bigl( X_{i+m}^{(2)} \bigr)-r_X\big(X_{i+m}^{(2)}\big)\big\} \bigr] = 0,
 \end{align*}
 which leads to $\mE[\mathbb{(II)}] = 0$. On the other hand, the conditional second moment (or the conditional variance) of $\mathbb{(II)}$ given $D_b$ satisfies 
 \begin{align*}
 	\mE\bigl[\mathbb{(II)}^2 \given D_b \bigr]  & = \frac{1}{m} \mE\bigl[ \big\langle\psi\bigl( V_1^{(1)} \bigr)-r_X\bigl( X_1^{(2)} \bigr) \psi\bigl( V_1^{(2)} \bigr), \psi\bigl( V_{1+m}^{(2)} \bigr)\big\rangle^2 \cdot\big\{ \widehat{r}_X\bigl( X_{1+m}^{(2)} \bigr)-r_X\big(X_{1+m}^{(2)}\big)\big\}^2 \given D_b\bigr] \\ & \lesssim \frac{K^2}{m}\bigl(1 + \mE\bigl[ r_X\big(X_{1}^{(2)}\big)^2 \bigr] \bigr) \cdot \mE\bigl[ \big\{\widehat{r}_X\bigl( X_{1+m}^{(2)} \bigr)-r_X\big(X_{1+m}^{(2)}\big)\big\}^2 \given D_b \bigr],
 \end{align*}


 
 
 where we use the fact that $\langle \psi(x), \psi(y) \rangle = k(x,y)$, whose $\ell_\infty$ norm is uniformly bounded by the constant $K$. Therefore, under the condition that 
 \begin{align*}
 	\sup_{P \in \mathcal{P}_0}\mE_P\bigl[ \big\{\widehat{r}_X\bigl( X_{1+m}^{(2)} \bigr)-r_X\big(X_{1+m}^{(2)}\big)\big\}^2 \bigr] = o(m^{-1/2}) \quad \text{and} \quad \sup_{P \in \mathcal{P}_0} \mE_P \bigl[r_X\big(X_{1}^{(2)}\big)^2 \bigr] <\infty,
 \end{align*}
 Chebyshev's inequality yields $\mathbb{(II)} = o_{\mathcal{P}_0}(m^{-1/2})$ and similarly $\mathbb{(III)} = o_{\mathcal{P}_0}(m^{-1/2})$.
 	\item \textbf{Term $\mathbb{(IV)}$.} The fourth term $\mathbb{(IV)}$ can be written as
 	\begin{align*}
 		\mathbb{(IV)} = \frac{1}{m} \sum_{i=1}^m k\bigl(V_i^{(2)}, V_{i+m}^{(2)}\bigr)\big\{\widehat{r}_X\bigl(X_i^{(2)}\bigr)-r_X\big(X_i^{(2)}\big)\big\} \cdot\big\{\widehat{r}_X\bigl( X_{i+m}^{(2)} \bigr)-r_X\big(X_{i+m}^{(2)}\big)\big\}.
 	\end{align*}
 	Since the kernel is uniformly bounded and by the Cauchy--Schwarz inequality, we have
 	\begin{align*}
 		\lvert\mathbb{(IV)}\rvert & \leq K \bigg(\frac{1}{m} \sum_{i=1}^m \big\{\widehat{r}_X\bigl(X_i^{(2)}\bigr)-r_X\big(X_i^{(2)}\big)\big\}^2\bigg)^{1/2} \cdot \bigg(\frac{1}{m} \sum_{i=1}^m \big\{ \widehat{r}_X\bigl( X_{i+m}^{(2)} \bigr)-r_X\big(X_{i+m}^{(2)}\big)\big\}^2\bigg)^{1/2} \\
 		& \overset{(\star)}{=} o_{\mathcal{P}_0}(m^{-1/4}) o_{\mathcal{P}_0}(m^{-1/4}) = o_{\mathcal{P}_0}(m^{-1/2}),
 	\end{align*}
 	which follows by Markov's inequality along with the condition that 
 	\begin{align*}
 		\sup_{P \in \mathcal{P}_0}\mE_P\bigl[ \big\{\widehat{r}_X\bigl( X^{(2)} \bigr)-r_X\big(X^{(2)}\big)\big\}^2 \bigr] = o(m^{-1/2}), 
 	\end{align*}
    and step~($\star$) holds by \citet[][Lemma S5]{lundborg2022projected}.
 	Therefore it holds that $\mathbb{(IV)} = o_{\mathcal{P}_0}(m^{-1/2})$.
\end{enumerate}


Now combining the results establishes that 
\begin{align*}
	\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} |\mP_P(\sqrt{m} \sigma_P^{-1}\overline{S} \leq t) - \Phi(t)| \to 0.
\end{align*}

\bigskip 

\noindent \textbf{Consistency of the variance estimate.} Denoting 
\begin{align*}
	\widehat{\sigma}_P^2 \coloneqq  \frac{1}{m-1} \sum_{i=1}^m (\widehat{S}_i - \overline{S})^2,
\end{align*}
we would like to show that $\widehat{\sigma}_P^2/\sigma_P^2$ converges to one in probability, which further implies $\sigma_P / \widehat{\sigma}_P = 1 + o_{\mathcal{P}_0}(1)$ by \citet[][Lemma S7]{lundborg2022projected}. Since the test statistic $\widehat{\mathrm{MMD}}^2_{\ell}$ is scale-invariant, we may assume that $\sigma_P^2 = 1$ without loss of generality. Moreover, the preceding analysis ensures that $\overline{S} = o_{\mathcal{P}_0}(1)$. Therefore we only need to show $\frac{1}{m} \sum_{i=1}^m \widehat{S}_i^2$ converges to one in probability. To this end, observe 
\begin{align*}
    \bigg|\frac{1}{m} \sum_{i=1}^m \widehat{S}_i^2 -1 \bigg| & = \bigg|\frac{1}{m} \sum_{i=1}^m \bigl(S_i + \widehat{S}_{i,a} + \widehat{S}_{i.b} + \widehat{S}_{i,c} \bigr)^2 - 1 \bigg| \\
    & \leq \bigg| \frac{1}{m} \sum_{i=1}^m S_i^2 - 1  \bigg| + \bigg| \frac{1}{m} \sum_{i=1}^m \bigl( \widehat{S}_{i,a} + \widehat{S}_{i.b} + \widehat{S}_{i,c} \bigr)^2 \bigg| + 2\bigg| \frac{1}{m} \sum_{i=1}^m S_i\bigl( \widehat{S}_{i,a} + \widehat{S}_{i.b} + \widehat{S}_{i,c} \bigr) \bigg| \\
    & \leq \bigg| \frac{1}{m} \sum_{i=1}^m S_i^2 - 1  \bigg| + \bigg| \frac{1}{m} \sum_{i=1}^m \bigl( \widehat{S}_{i,a} + \widehat{S}_{i.b} + \widehat{S}_{i,c} \bigr)^2 \bigg| \\ 
    & \hskip 7.8em + 2\sqrt{\frac{1}{m} \sum_{i=1}^m S_i^2}\sqrt{\frac{1}{m} \sum_{i=1}^m \bigl( \widehat{S}_{i,a} + \widehat{S}_{i.b} + \widehat{S}_{i,c} \bigr)^2},
\end{align*}
where the last inequality follows by the Cauchy--Schwarz inequality. By the law of large numbers, $\frac{1}{m} \sum_{i=1}^m S_i^2$ converges to one in probability. Thus the proof amounts to showing that $\frac{1}{m} \sum_{i=1}^m \bigl( \widehat{S}_{i,a} + \widehat{S}_{i.b} + \widehat{S}_{i,c} \bigr)^2 = o_{\mathcal{P}_0}(1)$, which is implied by
\begin{align*}
    \frac{1}{m} \sum_{i=1}^m \widehat{S}_{i,a}^2= o_{\mathcal{P}_0}(1), ~~ \frac{1}{m} \sum_{i=1}^m \widehat{S}_{i,b}^2= o_{\mathcal{P}_0}(1) \quad \text{and} \quad \frac{1}{m} \sum_{i=1}^m \widehat{S}_{i,c}^2= o_{\mathcal{P}_0}(1).
\end{align*}
This can be done as the way how $\mathbb{(II)}$, $\mathbb{(III)}$, and $\mathbb{(IV)}$ are handled earlier along with Markov's inequality. This completes the proof. 

\subsection{\texorpdfstring{Proof of \Cref{Corollary: Asymptotic Normality of CV MMD}}{Proof of Corollary: Asymptotic Normality of CV MMD}}

For each $j \in \{1,\ldots,K\}$, the proof of \Cref{Theorem: Asymptotic Normality of Linear Time MMD} shows that $\sigma_{P}/\widehat{\sigma}_j = 1 + o_{\mathcal{P}_0}(1)$. Thus, by \Cref{Lemma: uniform Slutsky}(b), it is enough to show the asymptotic normality of  
\begin{align*}
	\frac{1}{K}\sum_{j = 1}^K \frac{\sqrt{n}\overline{S}_j}{\sigma}.
\end{align*}
Without loss of generality, denote the sample indices of $D_1,D_2,\ldots,D_K$ as
\begin{align*}
	I_1 = \biggl\{1,\ldots, \frac{2n}{K}\biggr\}, \ I_2 = \biggl\{\frac{2n}{K}+1,\ldots,\ \frac{4n}{K}\biggr\},\ldots, I_K =\biggl\{2n - \frac{2n}{K} + 1,\ldots, 2n\biggr\},
\end{align*}
and let the first $n/K$ elements of $I_j$ as $I_j'$ (e.g., $I_1' = \{1,\ldots, n/K\}$).  Then with $m' = n/K$, the proof of  \Cref{Theorem: Asymptotic Normality of Linear Time MMD} establishes that 
\begin{align*}
	\sum_{j=1}^K \overline{S}_j = \frac{K}{n}\sum_{j=1}^K \sum_{i \in I_j'} S_i + o_{\mathcal{P}_0}(n^{-1/2}),
\end{align*}
where
\begin{align*}
	S_i = \big\langle\psi\bigl(V_i^{(1)} \bigr)-r_X\big(X_i^{(2)}\big)  \psi\bigl( V_i^{(2)} \bigr), \psi\bigl( V_{i+m'}^{(1)} \bigr)-r_X\big(X_{i+m'}^{(2)}\big) \psi\bigl( V_{i+m'}^{(2)} \bigr)\big\rangle.
\end{align*}
Notably, $\sum_{i \in I_1'} S_i,\ldots,\sum_{i \in I_K'} S_i$ are mutually independent. Hence 
\begin{align*}
	\mV\biggl( \frac{K}{n}\sum_{j=1}^K \sum_{i \in I_j'} \sigma_P^{-1} S_i  \biggr) = \frac{K^2}{n^2} \cdot K \cdot \frac{n}{K} = \frac{K^2}{n}. 
\end{align*}
By the central limit theorem~(\Cref{Lemma: Uniform CLT}),
\begin{align*}
	\frac{1}{K} \sum_{j=1}^K \sum_{i \in I_j'} \frac{\sqrt{n}S_i}{\sigma_P} = \frac{1}{K}\sum_{j = 1}^K \frac{\sqrt{n}\overline{S}_j}{\sigma} + o_{\mathcal{P}_0}(1)
\end{align*}
converges to $N(0,1)$ as desired.
\subsection{\texorpdfstring{Proof of \Cref{theorem: Asymptotic Normality of Block MMD}}{Proof of theorem: Asymptotic Normality of Block wise MMD}}\label{Proof of theorem: Asymptotic Normality of Block wise MMD}
\revised{(To make unified version)}
The proof consists of two parts as in the proof of \Cref{Theorem: Asymptotic Normality of Classification Accuracy}. In the first part, we investigate the numerator of $\widehat{\mathrm{MMD}}^2_{B}$, i.e., $\bar{\eta}$, whereas in the second part, we show the consistency of the denominator to the population variance under \Cref{Assumption : Block wise MMD}. The proof is then completed by applying \Cref{Lemma: uniform Slutsky}(b). 

\paragraph{Analysis of the numerator.} All inner products and norms below are taken with respect to the reproducing kernel Hilbert space $\mathcal{H}_k$, but for notational simplicity we omit explicit reference to $\mathcal{H}_k$. 
Using the feature map representation $k(x,y)=\langle\psi(x),\psi(y)\rangle$, 
the block-averaged statistic $\bar{\eta}$ can be written as
\begin{align*}
\bar{\eta}
= \frac{1}{S} \sum_{b=1}^{S} \frac{B}{\binom{B}{2}}
\sum_{(i,j)\in I_b}
\big\langle \psi(V_i^{(1)}) - \widehat r_X(X_i^{(2)})\,\psi(V_i^{(2)}),\;
\psi(V_j^{(1)}) - \widehat r_X(X_j^{(2)})\,\psi(V_j^{(2)}) \big\rangle .
\end{align*}
For each block $b=1,\dots,S$, define
\begin{align*}
V_{B,(b)}
:= \bigg\| \frac{1}{B}\sum_{i=(b-1)B+1}^{bB}\psi(V_i^{(1)})
- \frac{1}{B}\sum_{i=(b-1)B+1}^{bB}\widehat r_X(X_i^{(2)})\,\psi(V_i^{(2)})
\bigg\|,
\end{align*}
that is, the Hilbert–space norm of the difference between 
the block-wise average embedding of the sample drawn from $P_{XY}^{(1)}$ 
and the reweighted block-wise average embedding of the sample drawn from $P_{XY}^{(2)}$. With this notation, we recall that
\begin{align*}
\bar{\eta}
= \frac{1}{S}\sum_{b=1}^{S}\hat{\eta}_{b},
\qquad 
\hat{\eta}_{b}
:= \bigg(\underbrace{B\,V_{B,(b)}^2 - \frac{1}{B}\sum_{i=(b-1)B+1}^{bB}\widehat{H}_{ii}}_{=:T_b}\bigg)\cdot\frac{B}{B-1},
\end{align*}
so that each block contribution $\hat{\eta}_b$ is expressed in terms of $T_b$ up to the factor $\tfrac{B}{B-1}$. We further decompose each block term into a population part and an estimation-error part. Specifically,
\begin{align*}
\sqrt{B}\,V_{B,(b)}
&= \Bigg\|\underbrace{\frac{1}{\sqrt{B}}\sum_{i=(b-1)B+1}^{bB}
\bigl\{\psi(V_i^{(1)}) - r_X(X_i^{(2)})\,\psi(V_i^{(2)})\bigr\}}_{=: \I_{b}}
+ \underbrace{\frac{1}{\sqrt{B}}\sum_{i=(b-1)B+1}^{bB}
\psi(V_i^{(2)})\{r_X(X_i^{(2)})-\widehat r_X(X_i^{(2)})\}}_{=: \II_{b}}
\Bigg\| \\
&= \sqrt{\|\I_{b}\|^2 + \|\II_{b}\|^2
+ 2\langle \I_{b}, \II_{b}\rangle}.
\end{align*}
By construction of $\widehat H$ and $H$, the diagonal correction can be written as
\begin{align*}
  \frac{1}{B}\sum_{i=(b-1)B+1}^{bB}\widehat H_{ii}
  &= \frac{1}{B}\sum_{i=(b-1)B+1}^{bB} H_{ii} \\
  &\quad + \frac{1}{B}\sum_{i=(b-1)B+1}^{bB}
      \bigl\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\bigr\}^2 \, k(V_i^{(2)},V_i^{(2)}) \\
  &\quad - \frac{2}{B}\sum_{i=(b-1)B+1}^{bB}
      \bigl\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\bigr\}\, k(V_i^{(1)},V_i^{(2)}).
\end{align*}
Combining these expressions, $T_b$ admits the decomposition
\begin{align*}
T_b
&= \|\I_b\|^2 - \frac{1}{B}\sum_{i=(b-1)B+1}^{bB} H_{ii}  \\
&\quad + \|\II_b\|^2 + 2\langle \I_b,\II_b\rangle \\
&\quad - \frac{1}{B}\sum_{i=(b-1)B+1}^{bB}
      \{\,\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\,\}^2 k(V_i^{(2)},V_i^{(2)}) \\
&\quad + \frac{2}{B}\sum_{i=(b-1)B+1}^{bB}
      \{\,\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\,\}\, k(V_i^{(1)},V_i^{(2)}) \\
&=: T_{1,b}+T_{2,b}+T_{3,b}+T_{4,b}.
\end{align*}
For convenience, we define block-averaged quantities
\begin{align*}
T_j := \frac{1}{\sqrt{S}}\sum_{b=1}^S T_{j,b},
\quad j\in\{1,2,3,4\},
\end{align*}
so that
\begin{align*}
\sqrt{S}\,\bar{\eta}
= \frac{B}{B-1}\cdot(T_1+T_2+T_3+T_4).
\end{align*}
Since $\hat{\eta}_b = T_b \cdot \tfrac{B}{B-1}$, the difference 
$\hat{\eta}_b - T_b = \tfrac{T_b}{B-1}$ is $o_{\mathcal P_0}(1)$ as $B\to\infty$,
provided that $T_b=O_{\mathcal P_0}(1)$ (as will be shown below). 
Thus $\hat{\eta}_b$ and $T_b$ are asymptotically equivalent, 
and it suffices to analyze $T_b$. 
In particular, establishing that $T_2,T_3,T_4=o_{\mathcal P_0}(1)$ will imply
\begin{align*}
\sqrt{S}\,\bar{\eta} = T_1 + o_{\mathcal{P}_0}(1),
\end{align*}
so that the asymptotic distribution of $\sqrt{S}\,\bar{\eta}$ is fully determined by the leading term $T_1$.

\begin{enumerate}
% Term T_1
\item \textbf{Term $T_{1}$.} 
Recall that
\begin{align*}
T_{1,b} 
&= \|\I_b\|^2 - \frac{1}{B}\sum_{i=1}^B H_{ii} \\
&= (B-1) \cdot \frac{2}{B(B-1)}\sum_{(i,j)\in I_{b}} H_{ij} 
\end{align*}
Hence $T_{1,b}$ coincides with the order-two symmetric $U$-statistic based on the kernel $H$, scaled by a factor of $(B-1)$. Under the null, we have $\mE_{P}[T_{1,1}]=0$. 
Standard $U$-statistic theory then gives 
\begin{align*}
\mE_{P}[T_{1,1}^2]
= \frac{2(B-1)}{B}\,\mE_{P}[H_{12}^2].
\end{align*}
For $B \geq 2$, the prefactor satisfies $(B-1)/B \geq 1/2$. 
Hence, by Assumption~\ref{Assumption : Block wise MMD}(a),
\begin{align*}
    \inf_{P\in\mathcal{P}_{0}} \mE_P[T_{1,1}^2] 
    \;\geq\; c_{1}.
\end{align*}
This provides a uniform positive lower bound on the variance of $T_{1,1}$, independent of $B$. 

For higher moments, a direct expansion of $T_{1,1}$ would introduce explicit dependence on $B$. Instead, by Lemma~\ref{Lemma : moment bound T1}, the decoupled statistic
\begin{align*}
T_{1,1}^{\mathrm{dec}}
= \frac{1}{B}\sum_{i=1}^B \sum_{j=1}^B H(Z_i^{(1)},Z_j^{(2)})
\end{align*}
satisfies
\begin{align*}
\mE\big[|T_{1,1}^{\mathrm{dec}}|^{2+\delta}\big]
\;\leq\; c_{\delta}\, \mE[|H_{12}|^{2+\delta}],
\end{align*}
where $c_\delta$ is independent of $B$. 
Classical decoupling inequalities 
\citep[see][]{delapena1995decoupling,delapena1999decoupling}
then ensure that the same bound holds for the original $U$-statistic $T_{1,1}$, 
up to a universal constant. By Assumption~\ref{Assumption : Block wise MMD}(a), there exist constants $c_{1}',c_{2}'>0$,
depending only on $(c_{1},c_{2},\delta)$, such that
\begin{align*}
\inf_{P\in\mathcal{P}_{0}}\mE_{P}[T_{1,1}^2] \geq c_{1}', 
\qquad 
\sup_{P\in\mathcal{P}_{0}}\mE_{P}[|T_{1,1}|^{2+\delta}] \leq c_{2}'.
\end{align*}
Defining $\mV_P[T_{1,1}]=\sigma^2_P$, Lemma~\ref{Lemma: Uniform CLT} applies and yields
\begin{align*}
\lim_{M\to\infty}\;\sup_{P\in\mathcal{P}_{0}}
    \sup_{t\in\mathbb{R}}
    \Big| \mP_{P}\big(\sigma_P^{-1}T_{1}\leq t\big) - \Phi(t) \Big|
    = 0.
\end{align*}
Since the number of blocks $S=\lfloor M/B\rfloor$ diverges as $M\to\infty$, the central limit theorem follows.

% Term T_2
\item \textbf{Term $T_{2}$.}  
Recall that
\begin{align*} 
T_{2} &= \frac{1}{\sqrt{S}}\sum_{b=1}^{S} T_{2,b} 
= \frac{1}{\sqrt{S}}\sum_{b=1}^{S}\|\II_b\|^2 
+ \frac{2}{\sqrt{S}}\sum_{b=1}^{S}\langle \I_b,\II_b\rangle. 
\end{align*}

We first control the cross term. By the Cauchy--Schwarz inequality, 
\begin{align*} 
\bigg| \frac{1}{\sqrt{S}}\sum_{b=1}^{S}\langle \I_b,\II_b\rangle \bigg| 
&\leq \sqrt{S}\;\bigg(\frac{1}{S}\sum_{b=1}^S \|\I_b\|^2\bigg)^{1/2} 
\bigg(\frac{1}{S}\sum_{b=1}^S \|\II_b\|^2\bigg)^{1/2}. 
\end{align*}
Under the null hypothesis, $\|\I_b\|^2$ is centered 
(i.e.\ $\mathbb{E}[\|\I_b\|^2]=0$), and by 
\Cref{Assumption : Block wise MMD}(a) it has finite variance. 
Hence Chebyshev’s inequality yields
\begin{align*}
\frac{1}{S}\sum_{b=1}^S \|\I_b\|^2 
= o_{\mathcal{P}_0}(S^{-1/2}).
\end{align*}

Next, consider the error term $\II_b$, given by
\begin{align*}
\II_b
= \frac{1}{\sqrt{B}}\sum_{i=(b-1)B+1}^{bB} 
\psi(V_i^{(2)}) \,\big\{r_X(X_i^{(2)})-\hat r_X(X_i^{(2)})\big\}.
\end{align*}
Applying Cauchy--Schwarz and the reproducing property 
$\langle\psi(x), \psi(y)\rangle=k(x,y)$ with $\|k\|_\infty \leq K$, 
we obtain
\begin{align*}
\|\II_b\|_{\mathcal H_k}^2
&\leq K^2 \cdot B \cdot 
\bigg(\frac{1}{B}\sum_{i=(b-1)B+1}^{bB}
   \{r_X(X_i^{(2)})-\hat r_X(X_i^{(2)})\}^2\bigg).
\end{align*}
By \Cref{Assumption : Block wise MMD}(b),
\begin{align*}
\sup_{P \in \mathcal{P}_0} 
\mathbb{E}_P\big[\{\widehat{r}_X(X^{(2)})-r_X(X^{(2)})\}^2\big]
= o(N^{-1/2}).
\end{align*}

Combining the above bounds, we deduce that the cross term satisfies
\begin{align*}
\bigg| \frac{1}{\sqrt{S}}\sum_{b=1}^{S}\langle \I_b,\II_b\rangle \bigg|
&\stackrel{(i)}{=} o_{\mathcal{P}_{0}}(S^{1/4}) \cdot o_{\mathcal{P}_{0}}(B^{1/2}N^{-1/4}) \\
&\stackrel{(ii)}{=} o_{\mathcal{P}_{0}}\Big(\frac{M^{(1+\gamma)/4}}{N^{1/4}}\Big),
\end{align*}
where step $(i)$ applies \citet[][Lemma~S5]{lundborg2022projected} together with Markov's inequality, 
and step $(ii)$ follows from the definition $S=\lfloor M/B \rfloor$ and Assumption~\ref{Assumption : Block wise MMD}(d). A similar argument yields for the quadratic term
\begin{align*}
\frac{1}{\sqrt{S}}\sum_{b=1}^{S}\|\II_b\|_{\mathcal H_k}^2
= o_{\mathcal P_0}\Big(\frac{M^{(1+\gamma)/2}}{N^{1/2}}\Big).
\end{align*}
Therefore, we conclude that
\begin{align*}
T_2
= o_{\mathcal P_0}\Big(\frac{M^{(1+\gamma)/4}}{N^{1/4}} \Big),
\end{align*}
and finally, \Cref{Assumption : Block wise MMD}(e) ensures 
$T_2=o_{\mathcal P_0}(1)$.





\item \textbf{Terms $T_{3}$ and $T_{4}$.} Recall that
\begin{align*}
    T_{3} 
    &= -\frac{1}{\sqrt{S}}\sum_{b=1}^{S}\frac{1}{B}\sum_{i=(b-1)B+1}^{bB}
      \bigl\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\bigr\}^2 
      \,k(V_i^{(2)},V_i^{(2)}), \\
    T_{4} 
    &= \frac{1}{\sqrt{S}}\sum_{b=1}^{S} \frac{2}{B}\sum_{i=(b-1)B+1}^{bB}
      \bigl\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\bigr\}\,
      k(V_i^{(1)},V_i^{(2)}).
\end{align*}

Since the kernel is uniformly bounded with $\|k\|_\infty \le K$, 
each block contribution satisfies
\begin{align*}
    |T_{3,b}| 
    &\leq K \cdot \frac{1}{B}\sum_{i=(b-1)B+1}^{bB}
       \bigl\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\bigr\}^2, \\
    |T_{4,b}| 
    &\leq K \cdot \Big(\frac{1}{B}\sum_{i=(b-1)B+1}^{bB}
       \bigl\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\bigr\}^2 \Big)^{1/2}.
\end{align*}

By Markov's inequality and the condition
\begin{align*}
\sup_{P \in \mathcal{P}_0} 
\mathbb{E}_P\!\big[(\widehat{r}_X(X^{(2)})-r_X(X^{(2)}))^2\big]
= o(N^{-1/2}),
\end{align*}
Together with the block definition $S=\lfloor M/B \rfloor$ 
and Assumption~\ref{Assumption : Block wise MMD}(d), this yields
\begin{align*}
T_{3} = o_{\mathcal{P}_0}\!\left(\frac{M^{(1-\gamma)/2}}{N^{1/2}}\right),
\qquad
T_{4} = o_{\mathcal{P}_0}\!\left(\frac{M^{(1-\gamma)/2}}{N^{1/4}}\right).
\end{align*}
Finally, Assumption~\ref{Assumption : Block wise MMD}(e) ensures that 
both remainder terms are asymptotically negligible:
\begin{align*}
T_{3}=o_{\mathcal{P}_0}(1),
\qquad 
T_{4}=o_{\mathcal{P}_0}(1).
\end{align*}
\end{enumerate}
Combining the bounds for all four terms, we conclude that
\begin{align*}
\lim_{M\wedge N\rightarrow \infty}\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} \left| \mathbb{P}_P\left( \sqrt{S} \, \sigma_P^{-1} \bar{\eta} \leq t \right) - \Phi(t) \right| = 0.    
\end{align*}
\paragraph{Consistency of the variance estimate.} Denoting 
\begin{align*}
    \hat{\sigma}_{P}^{2}:=\frac{1}{S-1}\sum_{b=1}^{S}\big(\hat{\eta}_{b}-\bar{\eta}\big)^2,
\end{align*}
We aim to demonstrate that the ratio $\hat{\sigma}^2_{P}/\sigma^2_{P}$ converges to one in probability, which, in turn, implies $\hat{\sigma}_{P}/\sigma_{P}=1+o_{\mathcal{P}_{0}}(1)$ following \citep[Lemma 7]{lundborg2022projected}. Given that the test statistic $\widehat{\text{MMD}}^{2}_{B}$ is scale-invariant, we can assume $\sigma^2_{P}=1$ without any loss of generality. Additionally, the preceding analysis confirms that $\bar{\eta}=o_{\mathcal{P}_{0}}(1)$. Consequently, it suffices to show that $\frac{1}{S}\sum_{b=1}^{S}\hat{\eta}_{b}^2$ converges to one in probability. Recalling the block-level definition $\hat\eta_b = \tfrac{B}{B-1}T_b$, this reduces to 
\begin{align*}
    \frac{1}{S} \sum_{b=1}^S \hat{\eta}_b^2=\Big(\frac{B}{B-1}\Big)^2 \cdot \frac{1}{S} \sum_{b=1}^S T_b^2 .
\end{align*}
so it suffices to show that $\tfrac{1}{S}\sum_{b=1}^S T_b^2$ converges to one in probability. To establish this, note that:
\begin{align*}
    \bigg|\frac{1}{S}\sum_{b=1}^{S}T_{b}^2-1\bigg| &= \bigg|\frac{1}{S}\sum_{b=1}^{S}(T_{1,b}+T_{2,b}+T_{3,b}+T_{4,b})^2-1\bigg| \\
    &\leq \bigg|\frac{1}{S}\sum_{b=1}^{S}{T^2_{1,b}}-1\Bigg| + \Bigg|\frac{1}{S}\sum_{b=1}^{S}(T_{2,b}+T_{3,b}+T_{4,b})^2\Bigg| + 2\Bigg|\frac{1}{S}\sum_{b=1}^{S}T_{1,b}(T_{2,b}+T_{3,b}+T_{4,b})\Bigg| \\
    &\leq \bigg|\frac{1}{S}\sum_{b=1}^{S}{T_{1,b}^2}-1\Bigg| + \Bigg|\frac{1}{S}\sum_{b=1}^{S}(T_{2,b}+T_{3,b}+T_{4,b})^2\Bigg| \\
    &\hskip 7.8em +2\sqrt{\frac{1}{S}\sum_{b=1}^{S}T_{1,b}^2}\sqrt{\frac{1}{S}\sum_{b=1}^{S}(T_{2,b}+T_{3,b}+T_{4,b})^2},
\end{align*}
where the last inequality follows from the Cauchy--Schwarz inequality. 
Applying the law of large numbers, we see that $\frac{1}{S}\sum_{b=1}^{S}T_{1,b}^{2}$ converges to one in probability. 
Consequently, the proof reduces to establishing that 
\begin{align*}
\frac{1}{S}\sum_{b=1}^{S}(T_{2,b}+T_{3,b}+T_{4,b})^2=o_{\mathcal{P}_{0}}(1).
\end{align*}
This in turn follows from showing
\begin{align*}
\frac{1}{S}\sum_{b=1}^{S}T_{2,b}^2=o_{\mathcal{P}_{0}}(1), 
\quad 
\frac{1}{S}\sum_{b=1}^{S}T_{3,b}^2=o_{\mathcal{P}_{0}}(1), 
\quad 
\frac{1}{S}\sum_{b=1}^{S}T_{4,b}^2=o_{\mathcal{P}_{0}}(1).
\end{align*}

These results can be established using the similar techniques as for $T_{2}$, $T_{3}$, and $T_{4}$:

\begin{enumerate}
    \item \textbf{Step 1. } Recall that
    \begin{align*}
        T_{2,b} = \|\II_b\|^2 + 2\langle \I_b,\II_b\rangle.
    \end{align*}
    By the inequality $(a+b)^2 \leq 2(a^2+b^2)$ for $a,b \geq 0$, it follows that
    \begin{align*}
        T_{2,b}^2 \;\leq\; 2\|\II_b\|^4 + 8\langle \I_b,\II_b\rangle^2.
    \end{align*}
    \paragraph{(a) The $\|\II_b\|^4$ term.} We first control the term $\|\II_b\|^4$. From the definition of $\II_b$ and the boundedness of the kernel, we obtain
    \begin{align*}
        \|\II_b\|^4 
        &\leq K^4 \cdot B^2 
        \bigg(\frac{1}{B}\sum_{i=(b-1)B+1}^{bB}
           \{r_X(X_i^{(2)})-\hat r_X(X_i^{(2)})\}^2\bigg)^2.
    \end{align*}
    Applying \citet[Lemma S5]{lundborg2022projected} and Markov’s inequality, together with the condition
    \begin{align*}
    \sup_{P\in \mathcal P_0}\mathbb{E}_P\!\left[(\hat r_X(X^{(2)})-r_X(X^{(2)}))^2\right] 
    = o(N^{-1/2}),
    \end{align*}
    we deduce
    \begin{align*}
    \|\II_b\|^4 = o_{\mathcal P_0}(B^2/N).
    \end{align*}
    Under Assumption~\ref{Assumption : Block wise MMD}(e), this implies $\|\II_b\|^4=o_{\mathcal P_0}(1).$ Moreover by Markov's inequality $\tfrac{1}{S}\sum_{b=1}^{S}\|\II_{b}\|^4=o_{\mathcal P_0}(1).$
    \paragraph{(b) The $\langle \I_b,\II_b\rangle^2$ term.}
    For the second contribution, we first apply the Cauchy--Schwarz inequality:
    \begin{align*}
    \langle \I_b,\II_b\rangle^2 \;\leq\; \|\I_b\|^2 \|\II_b\|^2.
    \end{align*}
    Since $\|\II_b\|^2 \leq \sum_{b=1}^S \|\II_b\|^2$, it follows that
    \begin{align*}
    \frac{1}{S}\sum_{b=1}^S \langle \I_b,\II_b\rangle^2
    &\leq \frac{1}{S}\sum_{b=1}^S \|\I_b\|^2 \|\II_b\|^2 \\
    &\leq \Big(\frac{1}{S}\sum_{b=1}^S \|\I_b\|^2\Big)\cdot \Big(\sum_{b=1}^S \|\II_b\|^2\Big).
    \end{align*}
    
    By Assumption~\ref{Assumption : Block wise MMD}(a), $\|\I_b\|^2$ has mean zero and finite variance. Hence, by Chebyshev’s inequality,
    \begin{align*}
    \frac{1}{S}\sum_{b=1}^S \|\I_b\|^2 = o_{\mathcal P_0}(S^{-1/2}).
    \end{align*}
    Moreover, Assumption~\ref{Assumption : Block wise MMD}(b) gives
    \begin{align*}
    \sum_{b=1}^S \|\II_b\|^2 
    = S \cdot \frac{1}{S}\sum_{b=1}^S \|\II_b\|^2 
    = S \cdot o_{\mathcal P_0}(BN^{-1/2}).
    \end{align*}
    
    Combining these bounds together with Assumption~\ref{Assumption : Block wise MMD}(e), we conclude that the entire cross term is of order $o_{\mathcal P_0}(1)$. Combining $(1)$ and $(2)$, we conclude that
    \begin{align*}
    \frac{1}{S}\sum_{b=1}^{S} T_{2,b}^2 = o_{\mathcal P_0}(1).
    \end{align*}

    \item \textbf{Step 2. } For $\tfrac{1}{S}\sum_{b=1}^{S}T_{3,b}^2$ and $\tfrac{1}{S}\sum_{b=1}^{S}T_{4,b}^2$, 
    we proceed in the same way as in \textbf{Step~1}. 
    In particular, for $\tfrac{1}{S}\sum_{b=1}^{S}T_{4,b}^2$ we apply Jensen's inequality, 
    while for $\tfrac{1}{S}\sum_{b=1}^{S}T_{3,b}^2$ we directly use the same bounding technique as in \textbf{Step~1}. 
    Together with Markov's inequality, these bounds imply that both averages vanish:
    \begin{align*}
    \frac{1}{S}\sum_{b=1}^{S}T_{3,b}^2 = o_{\mathcal P_0}(1),
    \qquad 
    \frac{1}{S}\sum_{b=1}^{S}T_{4,b}^2 = o_{\mathcal P_0}(1).   
    \end{align*}
\end{enumerate}

Combining these results with Markov’s inequality completes the proof.

% By adding and subtracting ${r}_X(X_i^{(2)}) \psi(V_i^{(2)})$ and ${r}_X(X_{j}^{(2)}) \psi(V_{j}^{(2)})$, $\sqrt{S}\bar{\eta}$ can be written as the sum of the four terms given as:
% \begin{align*}
% &\I := \frac{1}{\sqrt{S}} \sum_{b=1}^{S} \underbrace{B\cdot \binom{B}{2}^{-1} \sum_{(i,j) \in I_{b}} \langle \psi(V_i^{(1)}) - r_X(X_i^{(2)})\psi(V_i^{(2)}), \psi(V_{j}^{(1)}) - r_X(X_{j}^{(2)})\psi(V_{j}^{(2)}) \rangle}_{:={\eta}_b} \\
% &\II := \frac{1}{\sqrt{S}} \sum_{b=1}^{S} \underbrace{B\cdot\binom{B}{2}^{-1} \sum_{(i,j) \in I_{b}} \langle \psi(V_i^{(1)}) - r_X(X_i^{(2)})\psi(V_i^{(2)}), \psi(V_{j}^{(2)}) \rangle \cdot \{ \hat{r}_X(X_{j}^{(2)}) - r_X(X_{j}^{(2)}) \}}_{:=\hat{\eta}_{b,1}} \\
% &\III := \frac{1}{\sqrt{S}} \sum_{b=1}^{S} \underbrace{B\cdot\binom{B}{2}^{-1} \sum_{(i,j) \in I_{b}} \langle \psi(V_i^{(2)}), \psi(V_{j}^{(1)}) - r_X(X_{j}^{(2)})\psi(V_{j}^{(2)}) \rangle \cdot \{ \hat{r}_X(X_i^{(2)}) - r_X(X_i^{(2)}) \}}_{:=\hat{\eta}_{b,2}} \\
% &\IV := \frac{1}{\sqrt{S}} \sum_{b=1}^{S} \underbrace{B\cdot\binom{B}{2}^{-1} \sum_{(i,j) \in I_{b}} \langle \psi(V_i^{(2)}), \psi(V_{j}^{(2)}) \rangle \{ \hat{r}_X(X_i^{(2)}) - r_X(X_i^{(2)}) \} \cdot \{ \hat{r}_X(X_{j}^{(2)}) - r_X(X_{j}^{(2)}) \}}_{:=\hat{\eta}_{b,3}}
% \end{align*}
% Our goal is to show that, asymptotically, $\sqrt{S}\,\bar{\eta} = \I + o_{\mathcal{P}_{0}}(1)$ holds.

% \begin{enumerate}
% \item \textbf{Term $\I$.} Since $\mE_{P}[\eta_1]=0$, standard $U$-statistic theory yields
% \begin{align*}
% \mE_{P}(\eta_1^2)
% = \frac{2B}{B-1}\,\mE_{P}[H_{12}^2].
% \end{align*}
% $\tfrac{2B}{B-1}\ge 2$ for all $B>1$. Therefore Assumption~\ref{Assumption : Block wise MMD}(a) implies
% \begin{align*}
%     \inf_{P\in\mathcal{P}_{0}} \mE_P(\eta_1^2) 
%     \geq 2c.
% \end{align*}
% This gives a uniform positive lower bound on the variance of $\eta_1$ that does not depend on $B$. For higher moments, direct expansion of $\eta_1$ would introduce explicit dependence on $B$. 
% To avoid this, we consider the decoupled version
% \begin{align*}
% \eta_1^{\mathrm{dec}}
% = \frac{2}{B-1}\sum_{i=1}^B\sum_{j=1}^B H\bigl(Z_i^{(1)},Z_j^{(2)}\bigr),
% \end{align*}
% where $\{Z_i^{(1)}\}_{i=1}^B$ and $\{Z_j^{(2)}\}_{j=1}^B$ are two independent copies of i.i.d. samples from $P$. Here, decoupling means that we evaluate $H$ on two independent sample sets rather than pairing points from the same set. Applying Lemma~\ref{Lemma : moment bound U stat} with $p=2+\delta$ gives terms of order $B^p$ and $B^{p/2}$ from the inner sum, while the $(B-1)^{-1}$ normalization contributes $(B-1)^{-p}$. Together these yield $(B/(B-1))^p \leq 2^p$ for $B \geq 2$, so the dependence on $B$ can be absorbed into a constant depending only on $p$ and the kernel moments. Explicitly,
% \begin{align*}
% \mE|\eta_1^{\mathrm{dec}}|^{2+\delta} 
% &\leq C_{\delta}\max\Big\{(\mE [H_{12}^2])^{1+\delta/2},\; \mE\big[|H_{12}|^{2+\delta}\big]\Big\},
% \end{align*}
% where $C_{\delta}$ depends only on $\delta$ and not on $B$. 
% Finally, by classical decoupling inequalities 
% \citep[see][]{delapena1995decoupling,delapena1999decoupling}, 
% the same bound applies to the original $\eta_1$ up to a universal constant.
% By \Cref{Assumption : Block wise MMD}~(a), we can find constants $c',C'>0$, depending only on $(c,C,\delta)$, such that
% \begin{align*}
%     \inf_{P\in\mathcal{P}_{0}}\mE_{P}[\eta_1^2] \geq c', 
%     \qquad 
%     \sup_{P\in\mathcal{P}_{0}}\mE_{P}[|\eta_1|^{2+\delta}] \leq C'.
% \end{align*}
% Define $\mV_P[\eta_1]=\sigma^2_P$. Then under \Cref{Assumption : Block wise MMD}~(a), \Cref{Lemma: Uniform CLT} yields
% \begin{align*}
% \lim_{n\to\infty}\;\sup_{P\in\mathcal{P}_{0}}
%     \sup_{t\in\mathbb{R}}
%     \Big| \mP_{P}\big(\sigma_P^{-1}\I\leq t\big) - \Phi(t) \Big|
%     = 0.
% \end{align*}
% \noindent
% Note that the number of blocks $S=\lfloor M/B\rfloor$ increases with $n$, so the central limit theorem applies in the regime $M\to\infty$ as $n\to\infty$.


% In this term, the population density ratio $r_X$ is assumed to be known. Let $\mathcal{Z} := (\mathcal{X} \times \mathcal{Y}) \times (\mathcal{X} \times \mathcal{Y})$, and define the product measure $P_Z := \mathbb{P}_{XY}^{(1)} \otimes \mathbb{P}_{XY}^{(2)}$. Define
%     \begin{align*}
%         \widetilde{H}(Z_{i},Z_{j}) &\coloneqq \langle \varphi(Z_{i}) - \mu_{P_{Z}}, \varphi(Z_{j}) - \mu_{P_{Z}} \rangle_{\mathcal{H}_{k}} \\
%         &= H(W_i, W_j) - \mathbb{E}_Z[H(W_i, Z)] - \mathbb{E}_Z[H(Z, W_j)] + \mathbb{E}_{Z, Z'}[H(Z, Z')],
%     \end{align*}
% where $\varphi(W_i) = \psi(V_i^{(1)}) - r_X(X_i^{(2)})\, \psi(V_i^{(2)})$ and $\mu_{P_{Z}}$ is the mean embedding of $P_{Z}$. Then $\eta_{b}$ can be written as
% \begin{align*}
%     \eta_{b} = B\cdot \frac{1}{B(B-1)} \sum_{i\neq j}^{B} \widetilde{H}(W_i, W_j),
% \end{align*}
% where the summation has mean zero. Note in particular that the U-statistic based on $\widetilde{H}(W_i, W_j)$ is degenerate in the sense that
% \begin{align*}
%     \mathbb{E}_Z[\widetilde{H}(Z, v)] &= \mathbb{E}[H(Z, v)] - \mathbb{E}_{Z,Z'}[H(Z,Z')] - \mathbb{E}_Z[H(Z, v)] + \mathbb{E}_{Z,Z'}[H(Z,Z')] = 0.
% \end{align*}
% Consider the operator $T_{\widetilde{H}}: L_2(P_Z) \to \mathcal{H}_k$ defined by
% \begin{align*}
%     (T_{\widetilde{H}} g)(z) := \int_{\mathcal{Z}} \widetilde{H}(z, z') g(z') \, dP_Z(z').
% \end{align*}
% By Theorem VI.23 of \cite{reed1980functional}, this operator is Hilbert–Schmidt and hence compact if and only if the kernel $H$ is square-integrable with respect to $P_Z$, i.e., $H \in L_2(\mathcal{Z} \times \mathcal{Z}, P_Z \times P_Z).$ This allows us to express $\widetilde{H}$ via its spectral decomposition:
% \begin{align*}
%   \widetilde{H}(z,z') = \sum_{\ell=1}^\infty \lambda_\ell \Psi_\ell(Z)\Psi_\ell(Z'),
% \end{align*}
% where the eigenfunctions $\{\Psi_\ell\}_{\ell\geq 1}$ and eigenvalues $\{\lambda_\ell\}_{\ell \geq 1}$ satisfy
% \begin{align*}
% \int_{\mathcal{Z}} \widetilde{H}(Z, Z') \Psi_\ell(Z) \, dP_Z(Z) &= \lambda_\ell \Psi_\ell(Z'), \\
% \int_{\mathcal{Z}} \Psi_\ell(Z) \Psi_j(Z) \, dP_Z(Z) &= \delta_{\ell j},
% \end{align*}
% with convergence in $L_2(\mathcal{Z} \times \mathcal{Z}, P_Z \times P_Z)$. Moreover, by Theorem VI.22 of \cite{reed1980functional}, since $T_{\widetilde{H}}$ is Hilbert–Schmidt, we have $\sum_{\ell=1}^\infty \lambda_\ell^2 < \infty$.

% Because $\widetilde{H}$ is degenerate, we further obtain
% \begin{align*}
% \lambda_\ell \cdot \mathbb{E}_{Z'} \Psi_\ell(Z') 
% = \int_{\mathcal{Z}} \mathbb{E}_{z'} H(Z, Z') \Psi_\ell(Z) \, dP_Z(Z) = 0,
% \end{align*}
% implying
% \begin{align*}
%     \mathbb{E}_Z \Psi_\ell(Z) = 0 \quad \text{for all } \ell.
% \end{align*}
% That is, the eigenfunctions $\{\Psi_\ell\}$ are mean-zero and pairwise uncorrelated.

% We now use these results to derive the asymptotic distribution of $\eta_{b}$. First,
% \begin{align*}
% \frac{1}{B} \sum_{i=1}^{B} \sum_{j \neq i}^{B} H(W_i, W_j) &= \frac{1}{B} \sum_{i=1}^{B} \sum_{j \neq i}^{B} \sum_{\ell=1}^{\infty} \lambda_\ell \Psi_\ell(W_i) \Psi_\ell(W_j) \\
% &= \frac{1}{B} \sum_{\ell=1}^{\infty} \lambda_\ell \left( \left( \sum_{i=1}^B \Psi_\ell(W_i) \right)^2 - \sum_{i=1}^B \Psi_\ell^2(W_i) \right) \\
% &\xrightarrow{D} \sum_{\ell=1}^{\infty} \lambda_\ell (a_\ell^2 - 1),
% \end{align*}
% where $a_\ell \sim \mathcal{N}(0, 1)$ are i.i.d., and the final relation denotes convergence in distribution, as established in \citet[Section 5.5.2]{serfling1980approximation}. By Assumption~\ref{Assumption : Block wise MMD}(a), the kernel $H_{12}$ satisfies
% \begin{align*}
% \inf_{P\in\mathcal P_0}\mE_P[H_{12}^2] \ge c>0, 
% \qquad 
% \sup_{P\in\mathcal P_0}\mE_P[|H_{12}|^{2+\delta}] \le C <\infty.    
% \end{align*}
% Since $\widetilde H$ inherits these bounds, it follows that for every block size $B$,
% \begin{align*}
% \text{Var}(\eta_b) = \frac{2B}{B-1}\,\mE_P[\widetilde H(Z_1,Z_2)^2] \;\ge\; \frac{2B}{B-1}\,c > 0,
% \qquad
% \mE_P[|\eta_b|^{2+\delta}] < C',    
% \end{align*}
% uniformly over $P\in\mathcal P_0$. 
% Thus $\{\eta_b\}_{b=1}^S$ is an i.i.d.\ sequence with mean zero, finite $(2+\delta)$-moment, and variance uniformly bounded away from zero. 
% By Lemma~\ref{Lemma: Uniform CLT}, we obtain
% \begin{align*}
% \lim_{n\to\infty}\sup_{P \in \mathcal P_0}\sup_{t\in\mathbb R}
% \Big|
% \mP_P\!\Big(\sqrt{S}\sigma_{P}^{-1}\bar{\eta} \le t\Big) - \Phi(t)
% \Big| = 0,
% \end{align*}
% where $\sigma_P^2 := \lim_{B\to\infty}\text{Var}(\eta_b) = 2\mE_P[\widetilde H(Z_1,Z_2)^2]$.
% \textcolor{red}{(explain clearly $\to 0$ happens as $n \to \infty$ or $S \to \infty$)}
%     \item \textbf{Terms $\II$ and $\III$.}  
%     \textcolor{red}{(revise)}We focus on the term $\II$, since $\III$ can be handled in exactly the same way by symmetry. Let $\widehat{\eta}_{b,1}(i,j)$ denote the $(i,j)$-th pairwise component of $\widehat{\eta}_{b,1}$, which represents the interaction between samples $i$ and $j$ within the $b$-th block. Under the null hypothesis, the law of total expectation implies that the expectation of each such component is zero:
%     \begin{align*}
%     \mathbb{E}[\hat{\eta}_{b,1}(i,j) \mid D_b]
%     &= \big\{ \widehat{r}_X(X_j^{(2)}) - r_X(X_j^{(2)}) \big\}
%     \cdot \mathbb{E}\big[
%     \big\langle \psi(V_i^{(1)}) - r_X(X_i^{(2)}), \psi(V_i^{(2)}), \psi(V_j^{(2)}) \big\rangle \mid D_b \big] \\
%     &= 0.
%     \end{align*}
% Therefore, $\mathbb{E}[\hat{\eta}_{b,1} \mid D_b] = 0$, which in turn implies that $\mathbb{E}[\II] = 0$. 
% Let 
% \begin{align*}
%  A_{ij} := \bigl\langle 
%     \psi(V_i^{(1)}) - r_X(X_i^{(2)})\, \psi(V_i^{(2)}),\, 
%     \psi(V_j^{(2)}) 
% \bigr\rangle, 
% \quad 
% B_{j} := \widehat{r}_{X}(X_{j}^{(2)}) - r_{X}(X_{j}^{(2)}).   
% \end{align*}
% On the other hand, the conditional second moment of $\II$ can be written as
% \begin{align*}
% \mathrm{Var}[\II \mid D_{b}] 
% = \frac{1}{S} \sum_{b=1}^{S} \mathrm{Var}[\hat{\eta}_{b,1} \mid D_{b}],    
% \end{align*}
% since the blocks are independent. Without loss of generality, consider the case $b = 1$.
%     \begin{align*}
%         \mE[\hat{\eta}_{1,1}^2\mid D_{b}] &= \mE\bigg[B^2 \cdot \frac{1}{B^2(B-1)^2}\Big(\sum_{i\neq j}^{B}A_{ij}B_{j}\Big)^2\;\Big | \;D_{b}\bigg],
%     \end{align*}
% where $\Big(\sum_{i\neq j}^{B}A_{ij}B_{j}\Big)^2$ decomposed as four terms :
% \begin{align*}
% \Big( \sum_{i \neq j}^B A_{ij} B_j \Big)^2
% &= \Big( \sum_{i \neq j}^B A_{ij} B_j \Big)
%    \Big( \sum_{k \neq \ell}^B A_{k\ell} B_\ell \Big) \\
% &= \sum_{i \neq j}^B \sum_{k \neq \ell}^B A_{ij} A_{k\ell} B_j B_\ell \\
% &= \sum_{i \neq j} A_{ij}^2 B_j^2 
%    && \text{(Case 1: $i = k,\ j = \ell$)} \\
% &\quad + \sum_{i} \sum_{j \neq \ell} \sum_{\substack{\ell \neq i \\ \ell \neq j}}
%    A_{ij} A_{i\ell} B_j B_\ell 
%    && \text{(Case 2: $i = k,\ j \neq \ell$)} \\
% &\quad + \sum_{j} \sum_{i \neq j} \sum_{\substack{k \neq j \\ k \neq i}}
%    A_{ij} A_{kj} B_j^2 
%    && \text{(Case 3: $i \neq k,\ j = \ell$)} \\
% &\quad + \sum_{\substack{i \neq k , j \neq \ell \\ i \neq j , k \neq \ell}}
%    A_{ij} A_{k\ell} B_j B_\ell 
%    && \text{(Case 4: $i \neq k,\ j \neq \ell$)} \, .
% \end{align*}
% Since Case 4 ensures that all indices are distinct, $A_{ij}B_{j}$ and $A_{k\ell}B_{\ell}$ are conditionally independent given $D_{b}$, which implies that their conditional expectation is zero. Therefore,
% \begin{align*}
%     \mathbb{E}[\hat{\eta}_{1,1}^2\mid D_{b}] 
%     &= B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot 
%        \mathbb{E}\!\bigg[ \sum_{i \neq j} A_{ij}^2 B_j^2 \,\Big|\, D_b \bigg] 
%        && \text{(Case 1)} \\
%     &\quad + B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot 
%        \mathbb{E}\!\bigg[ \sum_{i} \sum_{j \neq \ell} \sum_{\substack{\ell \neq i \\ \ell \neq j}}
%           A_{ij} A_{i\ell} B_j B_\ell \,\Big|\, D_b \bigg] 
%           && \text{(Case 2)} \\
%     &\quad + B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot 
%        \mathbb{E}\!\bigg[ \sum_{j} \sum_{i \neq j} \sum_{\substack{k \neq j \\ k \neq i}}
%           A_{ij} A_{kj} B_j^2 \,\Big|\, D_b \bigg] && \text{(Case 3)} \,.
% \end{align*}
% First, consider Case 1:
% \begin{align*}
%      B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot 
%        \mathbb{E}\!\bigg[ \sum_{i \neq j} A_{ij}^2 B_j^2 \,\Big|\, D_b \bigg] &=  \frac{B(B-1)}{(B-1)^2} \cdot 
%        \mathbb{E}\!\bigg[\frac{1}{B(B-1)} \sum_{i \neq j} A_{ij}^2 B_j^2 \,\Big|\, D_b \bigg] \\
%        &= \frac{B}{B-1}\cdot \frac{1}{B(B-1)}\sum_{i \neq j}\mE\Big[A_{ij}^2 B_{j}^2 \mid D_{b} \Big] \\
%        &\stackrel{(a)}{\lesssim} K^2 \Big( 1+\mE\big[r_{X}\big(X_{1}^{(2)}\big)^2\big]\Big)\cdot \mE\Big[\big\{\widehat{r}_{X}\big(X_{1}^{(2)}\big)-r_{X}\big(X_{1}^{(2)}\big)\big\}^2 \mid D_{b}\Big] \\
%        &= \revised{o_{\mathcal{P}_0}(N^{-1/2})},
% \end{align*}
% where $(a)$ follows from the fact, also used in \Cref{Section: Proof: Asymptotic Normality of linear time MMD}, that $\langle \psi(x), \psi(y) \rangle = k(x, y)$, with the kernel $k$ uniformly bounded by the constant $K$, together with the condition 
%  \begin{align*}
%  	\sup_{P \in \mathcal{P}_0}\mE_P\bigl[ \big\{\widehat{r}_X\bigl( X_{1+m}^{(2)} \bigr)-r_X\big(X_{1+m}^{(2)}\big)\big\}^2 \bigr] = o(N^{-1/2}) \quad \text{and} \quad \sup_{P \in \mathcal{P}_0} \mE_P \bigl[r_X\big(X_{1}^{(2)}\big)^2 \bigr] <\infty,
%  \end{align*}

% For Case 2 :
% \begin{align*}
% & B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot 
%    \mathbb{E}\bigg[ \sum_{i} \sum_{j \neq i} \sum_{\substack{\ell \neq i \\ \ell \neq j}}
%        A_{ij} A_{i\ell} B_j B_\ell \;\Big|\; D_b \bigg] \\
% &= B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot B(B-1)(B-2) \cdot 
%    \mathbb{E}\bigg[ \frac{1}{B(B-1)(B-2)} 
%        \sum_{i} \sum_{j \neq i} \sum_{\substack{\ell \neq i \\ \ell \neq j}}
%        A_{ij} A_{i\ell} B_j B_\ell \;\Big|\; D_b \bigg] \\
% &= \frac{B(B-2)}{B-1} \cdot \frac{1}{B(B-1)(B-2)} 
%    \sum_{i} \sum_{j \neq i} \sum_{\substack{\ell \neq i \\ \ell \neq j}}
%    \mathbb{E}\bigg[ A_{ij} A_{i\ell} B_j B_\ell \;\Big|\; D_b \bigg] \\
% &= \frac{B(B-2)}{B-1} \cdot \frac{1}{B(B-1)(B-2)} 
%    \sum_{i} \sum_{j \neq i} \sum_{\substack{\ell \neq i \\ \ell \neq j}}
%    \mathbb{E}\Big[ A_{ij} A_{i\ell} \Big] \cdot 
%    \mathbb{E}\Big[ B_j B_\ell \;\Big|\; D_b \Big] \\
% &\lesssim B \cdot \sqrt{K^2 \Big(1 + \mE\Big[r_{X}\big(X_{1}^{(2)}\big)^2\Big]\Big)\cdot \Big(1 + \mE\Big[r_{X}\big(X_{2}^{(2)}\big)^2\Big]\Big)} \cdot  \frac{1}{B(B-1)(B-2)} 
%        \sum_{i} \sum_{j \neq i} \sum_{\substack{\ell \neq i \\ \ell \neq j}}\mathbb{E}\big[|B_j B_\ell|\;|\; D_b \big],
% \end{align*}
% where we use the simple inequality $|xy| \leq (x^2+y^2)/2$, which implies
% \begin{align*}
% & B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot 
%    \mathbb{E}\bigg[ \sum_{i} \sum_{j \neq i} \sum_{\substack{\ell \neq i \\ \ell \neq j}}
%        A_{ij} A_{i\ell} B_j B_\ell \;\Big|\; D_b \bigg] \\
% & \lesssim B \cdot \sqrt{K^2 \Big(1 + \mE\Big[r_{X}\big(X_{1}^{(2)}\big)^2\Big]\Big)\cdot \Big(1 + \mE\Big[r_{X}\big(X_{2}^{(2)}\big)^2\Big]\Big)} \cdot \frac{1}{B}\sum_{j=1}^{B}\mE[B_{j}^2 \mid D_{b}] \\ & = \revised{o_{\mathcal{P}_{0}}(B\cdot N^{-1/2})}.
% \end{align*}
% For the last case, Case 3, we apply a similar approach as above:
% \begin{align*}
% & B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot 
%    \mathbb{E}\!\bigg[ \sum_{j} \sum_{i \neq j} \sum_{\substack{k \neq j \\ k \neq i}}
%        A_{ij} A_{kj} B_j^2 \;\Big|\; D_b \bigg] \\
% &= B^2 \cdot \frac{1}{B^2 (B-1)^2} \cdot B(B-1)(B-2) \cdot
%    \mathbb{E}\!\bigg[ \frac{1}{B(B-1)(B-2)} 
%        \sum_{j} \sum_{i \neq j} \sum_{\substack{k \neq j \\ k \neq i}}
%        A_{ij} A_{kj} B_j^2 \;\Big|\; D_b \bigg] \\
% &= \frac{B(B-2)}{B-1} \cdot 
%    \mathbb{E}\!\bigg[ \frac{1}{B(B-1)(B-2)} 
%        \sum_{j} \sum_{i \neq j} \sum_{\substack{k \neq j \\ k \neq i}}
%        A_{ij} A_{kj} B_j^2 \;\Big|\; D_b \bigg] \\
% &= \frac{B(B-2)}{B-1} \cdot 
%    \frac{1}{B(B-1)(B-2)} 
%        \sum_{j} \sum_{i \neq j} \sum_{\substack{k \neq j \\ k \neq i}}
%        \mathbb{E}\!\bigg[ A_{ij} A_{kj} \bigg] \cdot 
%        \mathbb{E}\!\big[ B_j^2 \mid D_b \big] \\
% &\lesssim B \cdot 
%    \sqrt{K^2 \Big(1 + \mE\Big[r_{X}\big(X_{1}^{(2)}\big)^2\Big]\Big)\cdot \Big(1 + \mE\Big[r_{X}\big(X_{2}^{(2)}\big)^2\Big]\Big)} \cdot
%    \frac{1}{B}\sum_{j=1}^{B}\mE[B_{j}^2 \mid D_{b}] \\ & = \revised{o_{\mathcal{P}_{0}}(B\cdot N^{-1/2})}.
% \end{align*}
% Combining the results of Cases 1–3 implies that $\hat{\eta}_{b,1} = \revised{o_{\mathcal{P}_{0}}(B\cdot N^{-1/2})}$ for $b=1,\ldots,S$. Therefore, under Assumption~\ref{Assumption : Block wise MMD}(d), the conditional variance of $\II$ satisfies
% \begin{align*}
% \mathrm{Var}[\II \mid D_b] 
% = \frac{1}{S}\sum_{b=1}^{S}\mathrm{Var}[\hat{\eta}_{b,1} \mid D_b] 
% = \revised{o_{\mathcal{P}_{0}}(B\cdot N^{-1/2})}.
% \end{align*}
% By Chebyshev’s inequality, it then follows that $\II = o_{\mathcal{P}_0}(1)$ and similarly $\III = o_{\mathcal{P}_0}(1)$.



% \item \textbf{Term $\IV$.} The final term $\IV$ is defined as
% \begin{align*}
%     \IV := \sqrt{S} \cdot \frac{1}{S} \sum_{b=1}^{S} \underbrace{B \cdot \frac{2}{B(B-1)} \sum_{(i,j) \in I_{b}} k(V_{i}^{(2)},V_{j}^{(2)}) \{ \hat{r}_X(X_i^{(2)}) - r_X(X_i^{(2)}) \} \cdot \{ \hat{r}_X(X_{j}^{(2)}) - r_X(X_{j}^{(2)}) \}}_{\hat{\eta}_{b,3}}.
% \end{align*}
% Since the kernel $k$ is uniformly bounded by $K$, we have \textcolor{red}{(the first inequality is true?)}
% \begin{align*}
%     |\hat{\eta}_{b,3}| 
%     &\leq K\cdot B\cdot  \bigg| \frac{2}{B(B-1)} \sum_{(i,j) \in I_b} \{\widehat{r}_{X}(X_{i}^{(2)}) - r_{X}(X_{i}^{(2)})\} \cdot \{\widehat{r}_{X}(X_{j}^{(2)}) - r_{X}(X_{j}^{(2)})\} \bigg| \\
%     &\leq \frac{2KB}{B(B-1)} \sum_{(i,j) \in I_b} \big| \widehat{r}_{X}(X_{i}^{(2)}) - r_{X}(X_{i}^{(2)}) \big| \cdot \big| \widehat{r}_{X}(X_{j}^{(2)}) - r_{X}(X_{j}^{(2)}) \big| \\
%     &\stackrel{(i)}{\leq} \frac{KB}{B(B-1)} \bigg( \sum_{i=(b-1)B+1}^{bB} \big| \widehat{r}_X(X_i^{(2)}) - r_X(X_i^{(2)}) \big| \bigg)^2 \\
%     &\stackrel{(ii)}{\leq}B\cdot \frac{K \cdot B^2}{B(B-1)} \cdot \frac{1}{B} \sum_{i=(b-1)B+1}^{bB} \big\{ \widehat{r}_X(X_i^{(2)}) - r_X(X_i^{(2)}) \big\}^2,
% \end{align*}
% where $(i)$ uses the inequality $\sum_{i<j} |a_i||a_j| \leq \frac{1}{2} \left(\sum_i |a_i|\right)^2$, and $(ii)$ follows from Jensen's inequality.

% Thus, for each block $b$, we obtain the bound
% \begin{align*}
%     |\hat{\eta}_{b,3}| \leq \frac{KB \cdot B^2}{B(B-1)} \cdot \frac{1}{B} \sum_{i=(b-1)B+1}^{bB} \big\{ \widehat{r}_{X}(X_{i}^{(2)}) - r_{X}(X_{i}^{(2)}) \big\}^2 =: \Delta_b.
% \end{align*}

% By \Cref{Assumption : Block wise MMD}(c)
% \begin{align*}
%   \sup_{P \in \mathcal{P}_0} \mathbb{E}_P \big[ \{ \widehat{r}_X(X^{(2)}) - r_X(X^{(2)}) \}^2 \big] = \revised{o(N^{-1/2})},
% \end{align*}
% we can apply Markov’s inequality to conclude that $\Delta_b = \revised{o(B\cdot N^{-1/2})}$ uniformly over $b$.

% Since $\IV = \sqrt{S} \cdot \frac{1}{S} \sum_{b=1}^{S} \hat{\eta}_{b,3}$, another application of Markov’s inequality yields
% \begin{align*}
%     \IV &= \sqrt{S} \cdot B\cdot \revised{o_{\mathcal{P}_{0}}(B\cdot N^{-1/2})}) \\
%     &= \left\lfloor \frac{M}{B} \right\rfloor^{1/2} \cdot B\cdot \revised{o_{\mathcal{P}_{0}}(B\cdot N^{-1/2})}) \\
%     &= \revised{o_{\mathcal{P}_{0}}((n-N)^{(1+2\gamma)/2}\cdot N^{-1/2})},
% \end{align*}
% where the final equality holds under Assumption~\ref{Assumption : Block wise MMD}(d). This term vanishes provided that $N$ is sufficiently large, i.e., when $(n-N)^{1+2\gamma} \ll N$, which in turn implies $\IV = o_{\mathcal{P}_{0}}(1)$.
% \end{enumerate}

% Combining the bounds for all four terms, we conclude that
% \begin{align*}
% \lim_{n\rightarrow \infty}\sup_{P \in \mathcal{P}_0} \sup_{t \in \mathbb{R}} \left| \mathbb{P}_P\left( \sqrt{S} \, \sigma_P^{-1} \bar{\eta} \leq t \right) - \Phi(t) \right| = 0,    
% \end{align*}

% \paragraph{Consistency of the variance estimate.} Denoting 
% \begin{align*}
%     \hat{\sigma}_{P}^{2}:=\frac{1}{S-1}\sum_{b=1}^{S}\big(\hat{\eta}_{b}-\bar{\eta}\big)^2,
% \end{align*}
% We aim to demonstrate that the ratio $\hat{\sigma}^2_{P}/\sigma^2_{P}$ converges to one in probability, which, in turn, implies $\hat{\sigma}_{P}/\sigma_{P}=1+o_{\mathcal{P}_{0}}(1)$ following \citep[Lemma 7]{lundborg2022projected}. Given that the test statistic $\widehat{\text{MMD}}^{2}_{B}$ is scale-invariant, we can assume $\sigma^2_{P}=1$ without any loss of generality. Additionally, the preceding analysis confirms that $\bar{\eta}=o_{\mathcal{P}_{0}}(1)$. Consequently, it suffices to show that $\frac{1}{S}\sum_{b=1}^{S}\hat{\eta}_{b}^2$ \textcolor{red}{(typo)} converges to one in probability. To establish this, note that:
% \begin{align*}
%     \bigg|\frac{1}{S}\sum_{b=1}^{S}\hat{\eta}_{b}^2-1\bigg| &= \bigg|\frac{1}{S}\sum_{b=1}^{S}(\eta_{b}+\hat{\eta}_{b,1}+\hat{\eta}_{b,2}+\hat{\eta}_{b,3})^2-1\bigg| \\
%     &\leq \bigg|\frac{1}{S}\sum_{b=1}^{S}{\eta}^{2}_{b}-1\Bigg| + \Bigg|\frac{1}{S}\sum_{b=1}^{S}(\hat{\eta}_{b,1}+\hat{\eta}_{b,2}+\hat{\eta}_{b,3})^2\Bigg| + 2\Bigg|\frac{1}{S}\sum_{b=1}^{S}\eta_{b}(\hat{\eta}_{b,1}+\hat{\eta}_{b,2}+\hat{\eta}_{b,3})\Bigg| \\
%     &\leq \bigg|\frac{1}{S}\sum_{b=1}^{S}{\eta}^{2}_{b}-1\Bigg| + \Bigg|\frac{1}{S}\sum_{b=1}^{S}(\hat{\eta}_{b,1}+\hat{\eta}_{b,2}+\hat{\eta}_{b,3})^2\Bigg| \\
%     &\hskip 7.8em +2\sqrt{\frac{1}{S}\sum_{b=1}^{S}\eta_{b}^{2}}\sqrt{\frac{1}{S}\sum_{b=1}^{S}(\hat{\eta}_{b,1}+\hat{\eta}_{b,2}+\hat{\eta}_{b,3})^2},
% \end{align*}
% where the last inequality is a consequence of the Cauchy–Schwarz inequality. Applying the law of large numbers, we see that $\frac{1}{S}\sum_{b=1}^{S}\eta_{b}^{2}$ converges to one in probability. Consequently, the proof reduces to establishing that $\frac{1}{S}\sum_{b=1}^{S}(\hat{\eta}_{b,1}+\hat{\eta}_{b,2}+\hat{\eta}_{b,3})^2=o_{\mathcal{P}_{0}}(1)$. \textcolor{red}{(is this trivial?)} This follows from showing that
% \begin{align*}
% \frac{1}{S}\sum_{b=1}^{S}\hat{\eta}^2_{b,1}=o_{\mathcal{P}_{0}}(1), \quad \frac{1}{S}\sum_{b=1}^{S}\hat{\eta}^2_{b,2}=o_{\mathcal{P}_{0}}(1), \quad \frac{1}{S}\sum_{b=1}^{S}\hat{\eta}^2_{b,3}=o_{\mathcal{P}_{0}}(1).
% \end{align*}
% These results follow from the same techniques used for $\II$, $\III$, and $\IV$, together with Markov’s inequality, thereby concluding the proof.

\subsection{\texorpdfstring{Proof of \Cref{cor:cv_block_MMD}}{Proof of Corollary: Asymptotic Normality of CV-Block MMD}}
For each $j \in \{1, \ldots, K\}$, the proof of Theorem~\ref{theorem: Asymptotic Normality of Block MMD} establishes that $\widehat{\sigma}_j / \sigma_P = 1 + o_{\mathcal{P}_0}(1)$. Therefore, by \Cref{Lemma: uniform Slutsky}, it suffices to show the asymptotic normality of the standardized statistic
\begin{align*}
    \frac{1}{K} \sum_{j=1}^K \frac{\sqrt{S} \, \bar{\eta}_j}{\sigma_P},
\end{align*}
where $\bar{\eta}_j$ denotes the average of block-level statistics in fold $j$.
Let the full dataset of size $n$ be evenly divided into $S := \frac{n}{B}$ disjoint blocks of size $B$, and assume that $S$ is divisible by $K$. We assign the blocks to $K$ disjoint subsets, denoted $\mathcal{B}_1, \ldots, \mathcal{B}_K$, such that each fold $j$ contains exactly $S/K$ blocks, i.e., $|\mathcal{B}_j| = S/K$.
Each block $b \in \{1, \ldots, S\}$ consists of a set of index pairs $(i, j)$, denoted $I_b \subset [B] \times [B]$, and the corresponding block-level statistic is defined as
\begin{align*}
    \hat{\eta}_b := B\cdot \binom{B}{2}^{-1} \sum_{(i,j) \in I_b} \big\langle \psi(V_i^{(1)}) - \widehat{r}_X(X_i^{(2)}) \psi(V_i^{(2)}),\; \psi(V_j^{(1)}) - \widehat{r}_X(X_j^{(2)}) \psi(V_j^{(2)}) \big\rangle.
\end{align*}
Then, for each fold $j$, we define the block-averaged statistic as
\begin{align*}
    \bar{\eta}_j := \frac{K}{S}\sum_{b \in \mathcal{B}_j} \eta_b.
\end{align*}
Summing over all folds yields
\begin{align*}
    \sum_{j=1}^K \bar{\eta}_j 
    = \frac{K}{S} \sum_{j=1}^K \sum_{b \in \mathcal{B}_j} \eta_b
    = \frac{K}{S} \sum_{b=1}^{S} \eta_b + o_{\mathcal{P}_0}(S^{-1/2}),
\end{align*}
where we use the fact that the collection $\{\mathcal{B}_j\}$ partitions the $S$ blocks.
Importantly, $\sum_{b\in\mathcal{B}_{1}}\eta_{b},\ldots,\sum_{b\in\mathcal{B}_{K}}\eta_{b}$ are mutually independent. Hence
\begin{align*}
    \mathrm{Var} \bigg( \frac{K}{S} \sum_{j=1}^K \sum_{b \in \mathcal{B}_j} \frac{\eta_b}{\sigma_{P}} \bigg)
    = \frac{K^2}{S^2}  \cdot K \cdot \frac{S}{K} = \frac{K^2}{S},
\end{align*}
Finally, applying the central limit theorem (\Cref{Lemma: Uniform CLT}), we obtain
\begin{align*}
    \frac{1}{K} \sum_{j=1}^K \frac{\sqrt{S} \, \bar{\eta}_j}{\sigma_P}
    = \frac{1}{K} \sum_{j=1}^K \sum_{b \in \mathcal{B}_j} \frac{\sqrt{S} \, \eta_b}{\sigma_P} + o_{\mathcal{P}_0}(1),
\end{align*}
converges to $N(0,1)$ as desired.

\subsection{\texorpdfstring{Proof of \Cref{Theorem : Asymptotic distribution of quadratic time MMD}}{Proof of Theorem: Asymptotic Distribution of Quadratic-Time MMD}}\label{Proof of Theorem: Asymptotic Distribution of Quadratic-Time MMD}
Similar to the block-wise decomposition in proof of \Cref{theorem: Asymptotic Normality of Block MMD} (\Cref{Proof of theorem: Asymptotic Normality of Block wise MMD}), we have
\begin{align*}
    \widehat{\mathrm{MMD}}_u^2 
    &= \frac{M}{M-1}V_{M}^2 
       - \frac{1}{M(M-1)}\sum_{i=1}^{M}\widehat{H}(W_{i},W_{i}), \\
V_{M} 
    &= \Bigg\| \frac{1}{M}\sum_{i=1}^{M}\psi(V_i^{(1)}) 
       - \frac{1}{M}\sum_{i=1}^{M}\widehat r_X(X_i^{(2)})\,\psi(V_i^{(2)})\Bigg\|.
\end{align*}

To analyze the leading term, we decompose $\sqrt{M}V_M$ into 
the population and estimation-error parts:
\begin{align*}
\sqrt{M}\,V_M 
&= \Bigg\|\underbrace{\frac{1}{\sqrt{M}}\sum_{i=1}^M 
  \{\psi(V_i^{(1)}) - r_X(X_i^{(2)})\,\psi(V_i^{(2)})\}}_{=: \I}
 + \underbrace{\frac{1}{\sqrt{M}}\sum_{i=1}^M 
  \psi(V_i^{(2)})\{r_X(X_i^{(2)})-\widehat r_X(X_i^{(2)})\}}_{=: \II}\Bigg\| \\
&= \sqrt{\|\I\|^2 + \|\II\|^2 + 2\langle \I,\II\rangle}.
\end{align*}

The diagonal correction expands as
\begin{align*}
  \frac{1}{M}\sum_{i=1}^{M}\widehat H(W_i,W_i)
  &= \frac{1}{M}\sum_{i=1}^{M} H(W_i,W_i) 
   + \frac{1}{M}\sum_{i=1}^{M}
      \{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\}^2 k(V_i^{(2)},V_i^{(2)}) \\
  &\quad - \frac{2}{M}\sum_{i=1}^{M}
      \{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\}\, k(V_i^{(1)},V_i^{(2)}).
\end{align*}

Combining the above, we obtain the decomposition
\begin{align*} 
\frac{M-1}{M}\cdot M\cdot \widehat{\mathrm{MMD}}_u^2 
&= \|\I\|^2 - \frac{1}{M}\sum_{i=1}^M H(W_i,W_i) \\ 
&\quad + \|\II\|^2 + 2\langle \I,\II\rangle \\ 
&\quad - \frac{1}{M}\sum_{i=1}^M \{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\}^2 k(V_i^{(2)},V_i^{(2)}) \\ 
&\quad + \frac{2}{M}\sum_{i=1}^M \{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\}\,k(V_i^{(1)},V_i^{(2)}) \\ 
&=: T_{1}+T_{2}+T_{3}+T_{4}.
\end{align*}
Since $\tfrac{M-1}{M}\to 1$ as $M\to \infty$, the prefactor does not affect the limiting distribution by the uniform Slutsky lemma (\Cref{Lemma: uniform Slutsky}). Hence, in what follows we focus on the decomposition into $T_1,T_2,T_3,T_4$, where $T_1$ provides the leading stochastic component and $T_2,T_3,T_4$ are asymptotically negligible.

\begin{enumerate}
\item \textbf{Term $T_1$.} Recall that
\begin{align*}
    \|\I\|^2 - \frac{1}{M}\sum_{i=1}^{M} H(W_{i},W_{i})
    \;=\;
    \frac{1}{M}\sum_{i \ne j} H(W_{i},W_{j}).
\end{align*}
Under the null hypothesis $H = \widetilde{H}_{n}$, 
Assumption~\ref{Assumption : quadratic time MMD}(a) together with 
Lemma~\ref{lemma:degenerate_ustat_berryessen} ensures that, 
\begin{align*}
    \lim_{M \to \infty}
    \sup_{P \in \mathcal{P}_{n}^{(0)}}
    \sup_{t \in \mathbb{R}}
    \Big|
        \mathbb{P}_{P}\big(T_1 \leq t\big)
        -
        \mathbb{P}_{P}\big(G_n \leq t\big)
    \Big|
    = 0,
\end{align*}
where the limiting distribution is given by
\begin{align*}
    G_n 
    = \sum_{\ell=1}^{\infty} 
      \lambda_{\ell,n}\,(a_\ell^2 - 1),
    \qquad 
    \{a_\ell\}_{\ell \ge 1} \;\iid\; N(0,1).
\end{align*}
The same reasoning applies to $T_{2}$, $T_{3}$, and $T_{4}$, 
following arguments analogous to those in the proof of 
Theorem~\ref{theorem: Asymptotic Normality of Block MMD}.

\item \textbf{Term $T_{2}$.} 
By definition,
\begin{align*}
T_{2} = \|\II\|^2 + 2\langle \I,\II\rangle.
\end{align*}
By the Cauchy--Schwarz inequality, $| \langle \I,\II\rangle | \;\leq\; \|\I\|\,\|\II\|,$ so it suffices to show that $\|\I\| = O_{\mathcal{P}}(1)$ and $\|\II\| = o_{\mathcal{P}}(1)$. To handle $\|\I\|$, note that
\begin{align*}
\|\I\|^2 
= \frac{1}{M}\sum_{i\neq j}\widetilde{H}_{n}(W_{i},W_{j})
 + \frac{1}{M}\sum_{i=1}^{M}\widetilde{H}_{n}(W_{i},W_{i}).
\end{align*}
The off-diagonal term is $O_{\mathcal{P}}(1)$ by the analysis of $T_1$.  
Furthermore, by Assumption~\ref{Assumption : quadratic time MMD}(b) and the law of large numbers,  
the diagonal term converges to a bounded quantity under $P\in\mathcal{P}^{(0)}_n$.  
Therefore, $\|\I\| = O_{\mathcal{P}}(1)$.

Turning to $\|\II\|$, we have
\begin{align*}
\|\II\|
&= \bigg\|\frac{1}{\sqrt{M}}\sum_{i=1}^M 
   \psi(V_i^{(2)})\{r_X(X_i^{(2)}) - \hat r_X(X_i^{(2)})\}\bigg\| \\
&\leq M^{1/2}\,
   \Big(\frac{1}{M}\sum_{i=1}^M \|\psi(V_i^{(2)})\|^2\Big)^{1/2}
   \Big(\frac{1}{M}\sum_{i=1}^M \{r_X(X_i^{(2)}) - \hat r_X(X_i^{(2)})\}^2\Big)^{1/2},
\end{align*}
where the inequality follows from Cauchy--Schwarz inequality. 
The first factor is bounded by a constant since the kernel $k$ is uniformly bounded 
(\Cref{Assumption : quadratic time MMD}(d)), 
while the second factor is $o(N^{-1/4})$ by \Cref{Assumption : quadratic time MMD}(c). 
Thus,
\begin{align*}
\|\II\| = o_{\mathcal P}\Big(\frac{M^{1/2}}{N^{1/4}}\Big).
\end{align*}
By \Cref{Assumption : quadratic time MMD}(e), the growth rates of $M$ and $N$ further imply $\|\II\| = o_{\mathcal P}(1),$
and in turn, by \citet[Lemma S5]{lundborg2022projected}, $\|\II\|^2 = o_{\mathcal{P }}(1).$ Combining these bounds, both terms in $T_{2}$ vanish asymptotically:
\begin{align*}
T_{2} = \|\II\|^2 + 2\langle \I,\II\rangle
= o_{\mathcal P}(1).
\end{align*}
\item \textbf{Terms $T_{3}$ and $T_{4}$.} 
Following the arguments in proof of \Cref{theorem: Asymptotic Normality of Block MMD} (\Cref{Proof of theorem: Asymptotic Normality of Block wise MMD}), 
we readily obtain
\begin{align*}
 T_{3} = o_{\mathcal P}(1), 
\qquad 
T_{4} = o_{\mathcal P}(1).   
\end{align*}
\end{enumerate}
Now combinig the results establishes that
\begin{align*}
\lim_{M \to \infty} \;
\sup_{P \in \mathcal{P}_{n}^{(0)}}
\sup_{t \in \mathbb{R}}
\Big|
\mathbb{P}_{P}
\big(M \cdot \widehat{\mathrm{MMD}}_{u}^2 \leq t\big)
- 
\mathbb{P}_{P}(G_n \leq t)
\Big| = 0.
\end{align*}










% \subsection{\texorpdfstring{Proof of \Cref{Theorem : Asymptotic distribution of quadratic time MMD}}{Proof of Theorem: Asymptotic Distribution of Quadratic-Time MMD}}
% \label{Proof of theorem : Asymptotic distribution of quadratic time MMD}
% Similar decomposition with \Cref{Proof of theorem: Asymptotic Normality of Block wise MMD} we have the exact relation
% \begin{align*}
%     \widehat{\mathrm{MMD}}_u^2 
%     = \frac{M}{M-1}V_{M}^2 - \frac{1}{M(M-1)}\sum_{i=1}^{M}\widehat{H}(Z_{i},Z_{i}),
% \end{align*}
% where 
% \begin{align*}
% V_{M} = \bigg\| \frac{1}{M}\sum_{i=1}^{M}\psi(V_i^{(1)}) 
% - \frac{1}{M}\sum_{i=1}^{M}\widehat r_X(X_i^{(2)})\,\psi(V_i^{(2)})\bigg\|_{\mathcal H_k}.    
% \end{align*}
% Multiplying both sides by $M$ yields
% \begin{align*}
%     M\cdot \widehat{\mathrm{MMD}}_u^2
%     &= M  V_M^2 + \bigg(\frac{M}{M-1}-1\bigg)\cdot MV_M^2 - \frac{M}{M-1}\cdot\frac{1}{M}\sum_{i=1}^M \widehat H(W_i,W_i).
% \end{align*}

% This decomposition highlights that the main stochastic contribution arises from $M V_M^2$, 
% while the diagonal term provides a deterministic centering. 
% Accordingly, we proceed as follows to establish the asymptotic distribution of $M\cdot\widehat{\mathrm{MMD}}_u^2$.
% \begin{itemize}
%     \item\textbf{Step 1.} the fluctuation of $M V_M^2$, which converges to an infinite sum of chi-squared random variables and is therefore stochastically bounded.
%     \item\textbf{Step 2.} the diagonal average $\frac{1}{M}\sum_{i=1}^M \widehat H(W_i,W_i)$, which converges to a finite deterministic value depending on the kernel operator.
% \end{itemize}
% \paragraph{Step 1.}To analyze the first step, it is convenient to study $\sqrt{M}V_M$, since
% \begin{align*}
% M V_M^2 = \big(\sqrt{M}V_M\big)^2.    
% \end{align*}
% Instead of analyzing $M V_M^2$ directly, we first consider $\sqrt{M}V_M$ to isolate the effect of density ratio estimation. This linearization shows that the additional error from $\widehat r_X-r_X$ is asymptotically negligible under \Cref{Assumption : quadratic time MMD}(c)–(d). 
% Equivalently, our goal is to establish that, asymptotically,
% \begin{align*}
% \sqrt{M}\,V_M
% = \Big\| \frac{1}{\sqrt{M}} \sum_{i=1}^M \psi(V_i^{(1)}) 
% - \frac{1}{\sqrt{M}} \sum_{i=1}^M r_X(X_i^{(2)})\, \psi(V_i^{(2)}) \Big\|_{\mathcal H_k},    
% \end{align*}
% so that the contribution from the density ratio estimator disappears in the limit. To make this precise, we decompose
% \begin{align*}
% \sqrt{M}\,V_M 
% &= \Big\| \underbrace{\frac{1}{\sqrt{M}}\sum_{i=1}^M \{\psi(V_i^{(1)}) - r_X(X_i^{(2)})\,\psi(V_i^{(2)})\}}_{\I}
% + \underbrace{\frac{1}{\sqrt{M}}\sum_{i=1}^M \psi(V_i^{(2)})\{r_X(X_i^{(2)})-\widehat r_X(X_i^{(2)})\}}_{\II}\Big\|_{\mathcal H_k} \\
% &= \sqrt{\|\I\|_{\mathcal H_k}^2 + \|\II\|_{\mathcal H_k}^2 + 2\langle \I,\II\rangle_{\mathcal H_k}}.
% \end{align*}
% Hence the problem reduces to showing
% \begin{enumerate}
%     \item $\|\I\|_{\mathcal H_k}$ is stochastically bounded.
%     \item $\|\II\|_{\mathcal H_k} = o_{\mathcal P_{n}^{(0)}}(1)$.
% \end{enumerate}
% The first condition guarantees that the main term does not diverge, 
% while the second ensures that the error term vanishes. 
% Together, they also imply that the cross term $2\langle \I,\II\rangle_{\mathcal H_k}$ is asymptotically negligible, Consequently, we obtain
% \begin{align*}
%  M V_M^2 = \|\I\|_{\mathcal H_k}^2 + o_{\mathcal P_{n}^{(0)}}(1),   
% \end{align*}
% which provides the desired reduction for analyzing the asymptotic distribution of $M V_{M}^2$.
% \begin{enumerate}
% % Term I
% \item \textbf{Term $\I$.} 
% Recall that $\varphi(W) = \psi(V^{(1)}) - r_X(X^{(2)})\,\psi(V^{(2)})$ and that 
% $\widetilde H_n$ denotes the centered kernel introduced in 
% \eqref{equation : centered kernel}. 
% Define $\I = \frac{1}{\sqrt{M}}\sum_{i=1}^M \varphi(W_i)$.
% Then
% \begin{align*}
%     \|\I\|_{\mathcal H_{k}}^2
%     &= \frac{1}{M}\Big\langle \sum_{i=1}^M \varphi(W_i),\, \sum_{j=1}^M \varphi(W_j)\Big\rangle_{\mathcal H_{k}} \\
%     &= \frac{1}{M}\sum_{i,j=1}^M H(W_i,W_j),
% \end{align*}
% where $H(W_i,W_j) = \langle \varphi(W_i), \varphi(W_j)\rangle_{\mathcal H_{k}}$. Using the definition of $\widetilde H_n$, under the null hypothesis we may rewrite
% \begin{align*}
% \|\I\|_{\mathcal H_{k}}^2
% = \frac{1}{M}\sum_{i,j=1}^M \widetilde H_n(W_i,W_j),    
% \end{align*}
% so that $\frac{1}{M}\|\I\|^2$ is a V-statistic with kernel $\widetilde H_n$.
% By the spectral decomposition, we may write
% \begin{align*}
%     \|\I\|_{\mathcal{H}_{k}}^{2}
%     &= \frac{1}{M}\sum_{i,j=1}^{M}\sum_{\ell=1}^\infty 
%        \lambda_{\ell,n}^{(P_{n})}\,\Psi_{\ell,n}^{(P_{n})}(W_{i})\Psi_{\ell,n}^{(P_{n})}(W_{j}) \\
%     &= \sum_{\ell=1}^{\infty}\lambda_{\ell,n}^{(P_{n})}
%        \bigg(\frac{1}{\sqrt{M}}\sum_{i=1}^{M}\Psi_{\ell,n}^{(P_{n})}(W_{i})\bigg)^2 .
% \end{align*}
% By \Cref{Assumption : quadratic time MMD}(a), the sequence of eigenvalues is uniformly square–summable, so that the series above is well defined. 
% Next, under \Cref{Assumption : quadratic time MMD}(b), for each fixed $\ell$ and fixed distribution $P_{n}$ the central limit theorem implies that $\frac{1}{\sqrt{M}}\sum_{i=1}^{M}\Psi_{\ell,n}^{(P_{n})}(W_{i})$
% converges in distribution to a standard Gaussian. By the continuous mapping theorem, the squared terms converge in distribution to $\chi^2(1)$ random variables, and hence
% \begin{align*}
%     \sum_{\ell=1}^{\infty}\lambda_{\ell,n}^{(P_{n})}
%        \bigg(\frac{1}{\sqrt{M}}\sum_{i=1}^{M}\Psi_{\ell,n}^{(P_{n})}(W_{i})\bigg)^2
% \end{align*}
% converges to $\sum_{\ell \geq 1}\lambda_{\ell,n}^{(P_{n})} a_\ell^2$, where $a_\ell \sim N(0,1)$. 
% For each fixed $P_{n}$, this establishes pointwise convergence in distribution of $\|\I\|_{\mathcal{H}_{k}}^{2}$. 
% To strengthen this result to uniform convergence of the distribution functions over $t \in \mathbb{R}$, we invoke Polya's theorem \citep[Theorem 2.6.1]{lehmann2004elements}, which states that convergence in distribution implies uniform convergence of the associated distribution functions when the limit distribution is continuous. 
% Consequently, for any sequence $\{P_{n}:n \geq 1\}$ with $P_{n}\in\mathcal{P}_{n}^{(0)}$, we obtain
% \begin{align*}
% \lim_{n \to \infty} 
% \sup_{t \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_{n}}
% \big(\|\I\|_{\mathcal{H}_{k}}^{2} \leq t\big)
% - 
% \mathbb{P}\big(\widetilde{G}_n^{(P_n)} \leq t\big)
% \Big|=0,
% \end{align*}
% where
% \begin{align*}
%  \widetilde{G}_n^{(P_n)} \;\sim\; \sum_{\ell=1}^{\infty} 
% \lambda_{\ell,n}^{(P_{n})}\,a_\ell^2,
% \qquad \{a_\ell\}_{\ell \geq 1} \stackrel{\text{i.i.d.}}{\sim} N(0,1).   
% \end{align*}

% \item \textbf{Term $\II$.}  
% By the definition of the Hilbert norm and the Cauchy--Schwarz inequality,
% \begin{align*}
% \Big\|\frac{1}{\sqrt{M}} \sum_{i=1}^M 
% \psi(V_i^{(2)}) \cdot \big\{r_X(X_i^{(2)}) - \hat{r}_X(X_i^{(2)})\big\}\Big\|
% &\leq 
% \sqrt{M}\,\Big(\frac{1}{M} \sum_{i=1}^M \|\psi(V_i^{(2)})\|^2\Big)^{1/2}
% \\ &\quad\quad\cdot
% \bigg(\frac{1}{M} \sum_{i=1}^M \big\{r_X(X_i^{(2)}) - \hat{r}_X(X_i^{(2)})\big\}^2\bigg)^{1/2} .
% \end{align*}
% Under \Cref{Assumption : quadratic time MMD}(d), the kernel 
% $k(\cdot,\cdot) = \langle \psi(\cdot), \psi(\cdot)\rangle$ is uniformly bounded in $\mathcal{H}_k$, so the first factor is bounded by a constant $K > 0$. 
% For the second factor, the convergence rate of the density ratio estimator implies
% \begin{align*}
%     \sqrt{M}\,\Bigg(\frac{1}{M} \sum_{i=1}^M \big(r_X(X_i^{(2)}) - \hat{r}_X(X_i^{(2)})\big)^2\Bigg)^{1/2} 
% = o_{\mathcal P_{n}^{(0)}}\Big(\frac{M^{1/2}}{N^{1/4}}\Big).
% \end{align*}
% Hence,
% \begin{align*}
%   \Big\|\frac{1}{\sqrt{M}} \sum_{i=1}^M 
% \psi(V_i^{(2)}) \cdot \big\{r_X(X_i^{(2)}) - \hat{r}_X(X_i^{(2)})\big\}\Big\|
% = o_{\mathcal P_{n}^{(0)}}\Big(\frac{M^{1/2}}{N^{1/4}}\Big).  
% \end{align*}
% By \Cref{Assumption : quadratic time MMD}(e) ($M^2 \ll N$), it follows that
% \begin{align*}
%  \|\II\|^2 = o_{\mathcal{P}_{0}}(1).   
% \end{align*}


% \item \textbf{Term $\langle \I,\II\rangle_{\mathcal{H}_{k}}$.} 
% By the Cauchy--Schwarz inequality,
% \begin{align*}
% \big|\langle \I, \II \rangle\big|
% &\leq \|\I\| \cdot \|\II\|.
% \end{align*}
% Since $\|\I\| = O_{\mathcal{P}_0}(1)$ and 
% $\|\II\| = o_{\mathcal{P}_0}(1)$, we conclude
% \begin{align*}
% \langle \I, \II \rangle = o_{\mathcal{P}_0}(1).    
% \end{align*}
% \end{enumerate}
% Combining the above results, we establish that for any sequence $\{P_{n}:n \geq 1\}$ with $P_{n}\in\mathcal{P}_{n}^{(0)}$,
% \begin{align*}
% \lim_{n \to \infty} 
% \sup_{t \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_{n}}
% \big(MV_M^2 \leq t\big)
% - 
% \mathbb{P}\big(\widetilde{G}_n^{(P_n)} \leq t\big)
% \Big|=0,
% \end{align*}
% where
% \begin{align*}
%  \widetilde{G}_n^{(P_n)} \;\sim\; \sum_{\ell=1}^{\infty} 
% \lambda_{\ell,n}^{(P_{n})}\,a_\ell^2,
% \qquad \{a_\ell\}_{\ell \geq 1} \stackrel{\text{i.i.d.}}{\sim} N(0,1).   
% \end{align*}
% \paragraph{Step 2.}By the construction of $\widehat{H}$ and $H$, we can write
% \begin{align*}
%   \frac{1}{M}\sum_{i=1}^{M}\widehat{H}(Z_{i},Z_{i})
%   &= \frac{1}{M}\sum_{i=1}^{M}H(Z_{i},Z_{i})
%   + \frac{1}{M}\sum_{i=1}^{M}\big\{\widehat{r}_{X}(X_{i}^{(2)})-r_{X}(X_{i}^{(2)})\big\}^2 k(V_{i}^{(2)},V_{i}^{(2)}) \\
%   &\quad - \frac{2}{M}\sum_{i=1}^{M}\big\{\widehat{r}_{X}(X_{i}^{(2)})-r_{X}(X_{i}^{(2)})\big\} k(V_{i}^{(1)},V_{i}^{(2)}).
% \end{align*}
% Under the null hypothesis, and under \Cref{Assumption : quadratic time MMD}(c) and (d), and using the above analysis, we obtain
% \begin{align*}
% \frac{1}{M}\sum_{i=1}^{M}\widehat{H}(W_{i},W_{i})
% = \frac{1}{M}\sum_{i=1}^{M}\widetilde{H}_{n}(Z_{i},Z_{i})
% + o_{\mathcal{P}_{n}^{(0)}}(1).
% \end{align*}
% Furthermore, by the spectral decomposition, we have
% \begin{align*}
%     \frac{1}{M}\sum_{i=1}^{M}\widehat{H}(W_{i},W_{i})
%     = \sum_{\ell=1}^{\infty}\lambda^{(P_{n})}_{\ell,n}
%     \Big(\frac{1}{M}\sum_{i=1}^{M}\big(\Psi_{\ell,n}^{(P_{n})}(W_{i})\big)^2\Big)+ o_{\mathcal{P}_{n}^{(0)}}(1).
% \end{align*}
% Finally, under \Cref{Assumption : quadratic time MMD}(a) and (b), this expression converges to $\sum_{\ell=1}^{\infty}\lambda^{(P_{n})}_{\ell,n}.$
% \medskip
% Recall that
% \begin{align*}
%     M\cdot \widehat{\mathrm{MMD}}_u^2
%     &= M  V_M^2 
%     + \underbrace{\bigg(\frac{M}{M-1}-1\bigg)\cdot MV_M^2}_{(a)} 
%     - \underbrace{\frac{M}{M-1}\cdot\frac{1}{M}\sum_{i=1}^M \widehat H(W_i,W_i)}_{(b)}.
% \end{align*}
% Combining the results from \textbf{Step 1} and \textbf{Step 2}, we observe the following.  
% Since $MV_{M}^2$ converges to a limiting distribution, it is stochastically bounded, which implies that $(a)=o_{\mathcal{P}_{n}^{(0)}}(1)$.  
% Moreover, $(b)$ converges to $\sum_{\ell=1}^{\infty}\lambda^{(P_{n})}_{\ell,n}$.  
% Therefore, we conclude that
% \begin{align*}
% \lim_{n \to \infty} 
% \sup_{t \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_{n}}
% \big(M\cdot \widehat{\mathrm{MMD}}_u^2 \leq t\big)
% - 
% \mathbb{P}\big(G_n^{(P_n)} \leq t\big)
% \Big|=0,
% \end{align*}
% where
% \begin{align*}
%  G_n^{(P_n)} \sim \sum_{\ell=1}^{\infty} 
% \lambda_{\ell,n}^{(P_{n})}\,(a_\ell^2-1),
% \qquad \{a_\ell\}_{\ell \geq 1} \stackrel{\text{i.i.d.}}{\sim} N(0,1).   
% \end{align*}
% This is sufficient to establish the uniform result
% \begin{align*}
% \lim_{n \to \infty} 
% \sup_{P_{n}\in\mathcal{P}_{n}^{(0)}}
% \sup_{t \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_{n}}
% \big(M\cdot \widehat{\mathrm{MMD}}_u^2 \leq t\big)
% - 
% \mathbb{P}\big(G_n^{(P_n)} \leq t\big)
% \Big|=0.
% \end{align*}
% To see this, select a sequence $P_{n}^{\prime}$ such that, for all $n$, we have
% \begin{align*}
% \sup_{t \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_{n}^{\prime}}
% \big(M\cdot \widehat{\mathrm{MMD}}_u^2 \leq t\big)
% - 
% \mathbb{P}\big(G_n^{(P_n)} \leq t\big)
% \Big|
% &\leq 
% \sup_{P_{n}\in\mathcal{P}_{n}^{(0)}} 
% \sup_{t \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_{n}}
% \big(M\cdot \widehat{\mathrm{MMD}}_u^2 \leq t\big)
% - 
% \mathbb{P}\big(G_n^{(P_n)} \leq t\big)
% \Big| \\
% &\leq 
% \sup_{t \in \mathbb{R}}
% \Big|
% \mathbb{P}_{P_{n}^{\prime}}
% \big(M\cdot \widehat{\mathrm{MMD}}_u^2 \leq t\big)
% - 
% \mathbb{P}\big(G_n^{(P_n)} \leq t\big)
% \Big|
% + \frac{1}{n}.
% \end{align*}
% Since both the leftmost and rightmost terms converge to zero, it follows that the middle term also converges to zero, as required.  
% This completes the proof of \Cref{Theorem : Asymptotic distribution of quadratic time MMD}.


% \subsection{\texorpdfstring{Proof of \Cref{thm:residual-mmd-fixed k P null}}{Proof of theorem: fixed k P null}}
% Recall that $R(v) = g(v) - m(v)$ and define the residual kernel
% \begin{align*}
% k^{\mathrm{res}}(v,v') := \langle R(v), R(v')\rangle_{\cH}.    
% \end{align*} 
% Using this notation, the quadratic-time statistic based on $\hat r_X$ can be written as
% \begin{align*}
% U_n^{\mathrm{res}}(\hat r_X)
% &:= \frac{1}{n(n-1)} \sum_{i\neq j} k_{\mathrm{res}}(V_i^{(1)},V_j^{(1)})
% + \frac{1}{n(n-1)} \sum_{i\neq j} \hat r_X(X_i^{(2)})\hat r_X(X_j^{(2)})\,k_{\mathrm{res}}(V_i^{(2)},V_j^{(2)}) \\
% &\quad - \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n \hat r_X(X_i^{(2)})\,k_{\mathrm{res}}(V_i^{(2)},V_j^{(1)})
% - \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n \hat r_X(X_j^{(2)})\,k_{\mathrm{res}}(V_i^{(1)},V_j^{(2)}).
% \end{align*}
% After some algebraic simplification, this can be equivalently expressed as
% \begin{align*}
% U_n^{\mathrm{res}}(\hat r_X)
% &= \Bigg\|\frac{1}{n}\sum_{i=1}^n R(V_i^{(1)}) - \frac{1}{n}\sum_{i=1}^n \hat r_X(X_i^{(2)}) R(V_i^{(2)})\Bigg\|_\cH^2 \\
% &\quad - \frac{1}{n(n-1)}\Bigg(\sum_{i=1}^n \|R(V_i^{(1)})\|^2 + \sum_{j=1}^n \|\hat r_X(X_j^{(2)})R(V_j^{(2)})\|^2\Bigg).
% \end{align*}
% Introducing the empirical means
% \begin{align*}
% \mu_{V^{(1)},n}^R := \frac{1}{n}\sum_{i=1}^n R(V_i^{(1)}),
% \qquad
% \mu_{V^{(2)},n}^{R,\hat r} := \frac{1}{n}\sum_{i=1}^n \hat r_X(X_i^{(2)})R(V_i^{(2)}),
% \end{align*}
% this expression can be rewritten as
% \begin{align*}
% U_n^{\mathrm{res}}(\widehat{r}_{X})
% &= \|\mu_{V^{(1)},n}^R - \mu_{V^{(2)},n}^{R,\widehat{r}}\|^2
% + \frac{1}{n-1}\Big(\|\mu_{V^{(1)},n}^R\|^2 + \|\mu_{V^{(2)},n}^{R,\widehat{r}}\|^2\Big) \\
% &\quad - \frac{1}{n(n-1)}\Big(\sum_{i=1}^n \|R(V_i^{(1)})\|^2 + \sum_{i=1}^n \|\widehat{r}_X(X_i^{(2)})R(V_i^{(2)})\|^2\Big).
% \end{align*}
% Let $U_n^{\mathrm{res}}(r_X)$ denote the corresponding oracle statistic.  
% Taking the difference between the plug-in and oracle versions yields
% \begin{align*}
% U_n(\widehat{r}_X) - U_n(r_X)
% &= \|\mu_{V^{(1)},n}^R - \mu_{V^{(2)},n}^{R,\hat r}\|^2 
%  - \|\mu_{V^{(1)},n}^R - \mu_{V^{(2)},n}^{R,r}\|^2 \\
% &\quad + \frac{1}{n-1}\Big(\|\mu_{V^{(2)},n}^{R,\hat r}\|^2 - \|\mu_{V^{(2)},n}^{R,r}\|^2\Big) \\
% &\quad - \frac{1}{n(n-1)}\sum_{i=1}^n \big(\hat r_X(X_i^{(2)})^2 - r_X(X_i^{(2)})^2\big)\,\|R(V_i^{(2)})\|^2.
% \end{align*}
% For convenience, write $\Delta(x) := \hat r_X(x) - r_X(x)$ and define
% \begin{align*}
% A_n := \mu_{V^{(2)},n}^{R,\hat r} - \mu_{V^{(2)},n}^{R,r}
% = \frac{1}{n}\sum_{i=1}^n \Delta(X_i^{(2)})R(V_i^{(2)}).
% \end{align*}
% Applying the Hilbert space identity
% \begin{equation}\label{eq:norm-identity}
%     \|a-b\|^2 - \|a-c\|^2 = \|b-c\|^2 + 2\langle a-c,\, c-b\rangle,
% \end{equation}
% with $a = \mu_{V^{(1)},n}^R$, $b = \mu_{V^{(2)},n}^{R,\hat r}$, and $c = \mu_{V^{(2)},n}^{R,r}$, we obtain
% \begin{align*}
% U_n(\hat r) - U_n(r)
% &= \underbrace{\|A_n\|^2}_{\I}
% - \underbrace{2\langle\mu_{V^{(1)},n}^R - \mu_{V^{(2)},n}^{R,r},\, A_n\rangle}_{\II} \\
% &\quad + \underbrace{\frac{1}{n-1}\Big(\|\mu_{V^{(2)},n}^{R,\hat r}\|^2 - \|\mu_{V^{(2)},n}^{R,r}\|^2\Big)}_{\III} \\
% &\quad - \underbrace{\frac{1}{n(n-1)} \sum_{i=1}^n \big(\hat r_X(X_i^{(2)})^2 - r_X(X_i^{(2)})^2\big)\,\|R(V_i^{(2)})\|^2}_{\IV}.
% \end{align*}

% The purpose of this decomposition is to isolate the effect of the estimation error 
% $\hat r_X - r_X$.  
% Our goal is to show that
% \begin{align*}
%    n\cdot\big\{U_n(\hat r) - U_n(r)\big\} \;\to\; 0, 
% \end{align*}
% so that the plug-in statistic $U_n(\widehat{r}_X)$ shares the same asymptotic distribution as the oracle statistic $U_n(r_X)$.


% \begin{enumerate}
% % Term \I
%     \item \textbf{Term $\I$.} Recall 
%     \begin{align*}
%         A_n=\frac{1}{n}\sum_{i=1}^{n}Z_{i}, 
%         \qquad W_i=\Delta\big(X_i^{(2)}\big) R\big(V_i^{(2)}\big).
%     \end{align*}
%     Conditional on $D_b$, the variables $\{W_i\}_{i=1}^n$ are i.i.d. Hence, by the law of total expectation,
%     \begin{align*}
%         \mE[\|A_n\|^2] 
%         &= \mE\!\left[\,\mE[\|A_n\|^2 \mid D_b]\,\right].
%     \end{align*}
%     Applying Jensen’s inequality inside the conditional expectation gives
%     \begin{align*}
%         \mE[\|A_n\|^2 \mid D_b] 
%         &\leq \mE[\|Z_1\|^2 \mid D_b] \\
%         &= \mE\!\left[\Delta(X_{1}^{(2)})^2 \,\|R(V_{1}^{(2)})\|^2 \,\Big|\, D_b\right].
%     \end{align*}
%     Therefore,
%     \begin{align*}
%         \mE[\|A_n\|^2] 
%         &\leq \mE\!\left[\Delta(X_{1}^{(2)})^2 \,\|R(V_{1}^{(2)})\|^2\right].
%     \end{align*}
%     Finally, by Markov’s inequality,
%     \begin{align*}
%         \|A_n\|^2 
%         = O_p\!\left(\mE\!\left[\Delta(X_{1}^{(2)})^2 \,\|R(V_{1}^{(2)})\|^2 \right]\right).
%     \end{align*}
%     % Term \II
%     \item \textbf{Term $\II$.} 
%     \begin{align*}
%         \II = -2\langle \mu_{V^{(1)},n}^R - \mu_{V^{(2)},n}^{R,r},\, A_n\rangle.
%     \end{align*}
%     By the Cauchy--Schwarz inequality,
%     \begin{align*}
%         |\II| \;\leq\; 2\,\|\mu_{V^{(1)},n}^R - \mu_{V^{(2)},n}^{R,r}\| \cdot \|A_n\|.
%     \end{align*}
%     The first factor is the difference between two empirical mean embeddings, which is of order $O_p(n^{-1/2})$ under the finite second moment assumption.
%     The second factor satisfies $\|A_n\| = O_p\!\big(\sqrt{\mE[\Delta^2\|R\|^2]}\big)$, as established in the analysis of Term~$\I$.
%     Combining these bounds along with Markov's inequality, we obtain
%     \begin{align*}
%         \II = O_p\Big(n^{-1/2}\cdot \sqrt{\mE[\Delta(X_{1}^{(2)})^2\|R(V_{1}^{(2)})\|^2]}\,\Big).
%     \end{align*}
    
% % Term \III
% \item \textbf{Term $\III$.} Observe that
% \begin{align*}
%     \mu_{V^{(2)}, n}^{R, \hat r}
%     = \mu_{V^{(2)}, n}^{R, r} + A_n,
% \end{align*}
% so that
% \begin{align*}
%     \|\mu_{V^{(2)}, n}^{R, \hat r}\|^2
%     &= \|\mu_{V^{(2)}, n}^{R, r} + A_n\|^2 \\
%     &= \|\mu_{V^{(2)}, n}^{R, r}\|^2 
%       + 2\langle \mu_{V^{(2)}, n}^{R, r}, A_n\rangle
%       + \|A_n\|^2.
% \end{align*}
% Therefore,
% \begin{align*}
%     \III 
%     &= \frac{1}{n-1}\Big(2\langle \mu_{V^{(2)}, n}^{R, r}, A_n\rangle + \|A_n\|^2\Big).
% \end{align*}

% As shown in Term~$\I$, $\|A_n\|^2 = O_p\!\big(\mE[\Delta(X_{1}^{(2)})^2\|R(V_{1}^{(2)})\|^2]\big)$. 
% Moreover, since $\|\mu_{V^{(2)}, n}^{R, r}\| = O_p(1)$ under the finite second moment,
% \begin{align*}
%     \langle \mu_{V^{(2)}, n}^{R, r}, A_n\rangle
%     = O_p\!\big(\|A_n\|\big) 
%     = O_p\!\Big(\sqrt{\mE[\Delta_{1}(X^{(2)})^2\|R(V_{1}^{(2)})\|^2]}\Big).
% \end{align*}

% Hence by Markov's inequality,
% \begin{align*}
%     \III
%     = O_p\!\left(\frac{1}{n}\mE[\Delta(X_{1}^{(2)})^2\|R(V_{1}^{(2)})\|^2]\right).
% \end{align*}


% % Term \IV
% \item \textbf{Term $\IV$.} Using the identity $(a^2-b^2) = 2a(a-b) - (a-b)^2$, we write
% \begin{align*}
%     \IV &= -\frac{1}{n(n-1)} \sum_{i=1}^n 
%     \big(\hat r_X(X_i^{(2)})^2 - r_X(X_i^{(2)})^2\big)\,\|R(V_i^{(2)})\|^2 \\
%     &= -\frac{2}{n(n-1)}\sum_{i=1}^n r(X_i^{(2)})\,\Delta(X_i^{(2)})\,\|R(V_i^{(2)})\|^2
%        +\frac{1}{n(n-1)}\sum_{i=1}^n \Delta(X_i^{(2)})^2 \,\|R(V_i^{(2)})\|^2 \\
%     &=: T_{1,n} + T_{2,n}.
% \end{align*}

% For $T_{2,n}$, 
% \begin{align*}
%     |T_{2,n}| 
%     &\leq \frac{1}{n-1}\cdot \frac{1}{n}\sum_{i=1}^n 
%     \Delta(X_i^{(2)})^2 \|R(V_i^{(2)})\|^2.
% \end{align*}
% Taking expectations,
% \begin{align*}
%     \mathbb{E}[|T_{2,n}|] 
%     &\leq \frac{1}{n-1}\,\mathbb{E}\!\left[\Delta(X^{(2)})^2 \|R(V^{(2)})\|^2\right].
% \end{align*}
% Hence, by Markov’s inequality,
% \begin{align*}
%     |T_{2,n}| 
%     = O_p\!\left(\frac{1}{n}\,\mathbb{E}\!\left[\Delta(X^{(2)})^2 \|R(V^{(2)})\|^2\right]\right).
% \end{align*}

% For $T_{1,n}$, by the Cauchy--Schwarz inequality,
% \begin{align*}
%     |T_{1,n}| &\leq \frac{2}{n-1}\Bigg|\frac{1}{n}\sum_{i=1}^n 
%     r(X_i^{(2)})\,\Delta(X_i^{(2)})\,\|R(V_i^{(2)})\|^2\Bigg| \\
%     &\leq \frac{2}{n-1}\cdot 
%     \Bigg(\frac{1}{n}\sum_{i=1}^n r(X_i^{(2)})^2\|R(V_i^{(2)})\|^2\Bigg)^{1/2}
%     \cdot
%     \Bigg(\frac{1}{n}\sum_{i=1}^n \Delta(X_i^{(2)})^2\,\|R(V_i^{(2)})\|^2\Bigg)^{1/2}.
% \end{align*}
% Hence, provided that $\mathbb{E}[r_X(X^{(2)})^2\|R(V^{(2)})\|^2] < \infty$, 
% we can apply Markov’s inequality to obtain
% \begin{align*}
%     |T_{1,n}| 
%     = O_p\!\left(\frac{1}{n}\,\sqrt{\mathbb{E}\!\left[\Delta(X_{1}^{(2)})^2 \|R(V_{1}^{(2)})\|^2\right]}\right).
% \end{align*}
% Combining this bound with the result for $T_{2,n}$ and applying Markov’s inequality once more, 
% we conclude that
% \begin{align*}
%     \IV 
%     = O_p\!\left(\frac{1}{n}\,\mathbb{E}\!\left[\Delta(X_{1}^{(2)})^2 \|R(V_{1}^{(2)})\|^2\right]\right).
% \end{align*}

% \end{enumerate}
% Combining the above bounds, we obtain
% \begin{align*}
% n\cdot\big\{U_n(\hat r) - U_n(r)\big\}
% = n \cdot \mE\big[\Delta(X_{1}^{(2)})^2 \|R(V_{1}^{(2)})\|^2\big] + O_p(1).
% \end{align*}
% In particular, under the condition
% \begin{align*}
% \mE\big[\Delta(X^{(2)})^2 \|R(V^{(2)})\|_{\cH}^2\big] = o\Big(\frac{1}{n}\Big),
% \end{align*}
% the leading expectation term vanishes asymptotically. That is, the difference between the two statistics is asymptotically negligible. In particular,
% \begin{align*}
%     n \cdot U_n(\widehat{r}_{X}) 
%     \;\;\stackrel{D}{\longrightarrow}\;\;
%     \sum_{\ell \geq 1} \lambda_\ell \,(Z_\ell^2 - 1),
%     \qquad Z_\ell \iid N(0,1),
% \end{align*}
% where $\{\lambda_\ell\}_{\ell \geq 1}$ are the eigenvalues of the integral operator 
% associated with the residual kernel $\bar H^{(r_X)}$. This limit has the form of an infinite sum of centered chi-squared variables and coincides with the oracle distribution under the true density ratio $r_X$.

% \subsection{\texorpdfstring{Proof of \Cref{thm:residual-mmd-varying-kernel}}{Proof of theorem: fixed P changing k null}}
% As in the proof of \Cref{thm:residual-mmd-fixed k P null}, we first control the difference
% \begin{align*}
% n\{U_n(\hat r_X)-U_n(r_X)\}
% = n \cdot \mE\!\big[\Delta(X^{(2)})^2 \|R_n(V^{(2)})\|^2\big] + O_p(1).
% \end{align*}
% By Assumption~3, the expectation term is $o(1)$, which implies
% \begin{align*}
% n\cdot U_n(\hat r_X) = n\cdot U_n(r_X) + o_p(1).
% \end{align*}
% Thus it suffices to analyze the limiting distribution of the oracle statistic $U_n(r_X)$.
% For each $n$, the residual kernel $\bar H_n^{(r_X)}$ admits the eigen-expansion
% \begin{align*}
% \bar H_n^{(r_X)}(z,z')
% = \sum_{\ell=1}^\infty \lambda_{\ell,n}\,\Psi_{\ell,n}(z)\Psi_{\ell,n}(z'),
% \end{align*}
% with orthonormal eigenfunctions $\{\Psi_{\ell,n}\}$ in $L^2(P_Z)$ and eigenvalues $\{\lambda_{\ell,n}\}$.  
% Substituting this into the quadratic form of the $U$-statistic gives the exact identity
% \begin{align*}
% n\,U_n(r_X)
% = \frac{n}{n-1}\sum_{\ell=1}^\infty \lambda_{\ell,n}
% \Bigg\{\Big(\tfrac{1}{\sqrt n}\sum_{i=1}^n \Psi_{\ell,n}(W_i)\Big)^2
% - \tfrac{1}{n}\sum_{i=1}^n \Psi_{\ell,n}(W_i)^2\Bigg\}.
% \end{align*}
% By the law of large numbers, the second term inside the braces converges to $1$ in probability, and by a triangular-array multivariate CLT, for each fixed $L$ the vector
% \begin{align*}
% \Big(\tfrac{1}{\sqrt n}\sum_{i=1}^n \Psi_{\ell,n}(W_i)\Big)_{\ell=1}^L
% \end{align*}
% converges in distribution to $(Z_1,\dots,Z_L)$ with $Z_\ell\iid N(0,1)$.  
% Consequently, for any fixed $L$,
% \begin{align*}
% \sum_{\ell=1}^L \lambda_{\ell,n}\Bigg\{\Big(\tfrac{1}{\sqrt n}\sum_{i=1}^n \Psi_{\ell,n}(W_i)\Big)^2 - 1\Bigg\}
% \;\Rightarrow\; \sum_{\ell=1}^L \lambda_{\ell,n}(Z_\ell^2-1).
% \end{align*}
% Since $\lambda_{\ell,n}\to \lambda_\ell^\star$ for each $\ell$ under Assumption~2, the right-hand side converges further to $\sum_{\ell=1}^L \lambda_\ell^\star(Z_\ell^2-1)$.
% To extend from the truncated sum to the full expansion, we require uniform tail control.  
% Assumption~2 together with the uniform Hilbert--Schmidt bound
% \begin{align*}
% \sup_{n \geq 1} \sum_{\ell=1}^\infty \lambda_{\ell,n}^2 < \infty
% \end{align*}
% ensures that
% \begin{align*}
% \sup_{n \geq 1} \,\mathrm{Var}\Bigg(\sum_{\ell>L}\lambda_{\ell,n}
% \Big\{\Big(\tfrac{1}{\sqrt n}\sum_{i=1}^n \Psi_{\ell,n}(W_i)\Big)^2 
% - \tfrac{1}{n}\sum_{i=1}^n \Psi_{\ell,n}(W_i)^2\Big\}\Bigg)
% \;\longrightarrow\; 0
% \end{align*}
% as $L\to\infty$. By Chebyshev’s inequality, the tail then vanishes in probability uniformly over $n$. Combining these arguments, we conclude that
% \begin{align*}
% n\cdot U_n(r_X) \;\Rightarrow\; \sum_{\ell=1}^\infty \lambda_\ell^\star(Z_\ell^2-1).
% \end{align*}
% Finally, recalling that $n\{U_n(\hat r_X)-U_n(r_X)\}=o_p(1)$, Slutsky’s theorem implies
% \begin{align*}
% n\cdot U_n(\hat r_X) \;\Rightarrow\; \sum_{\ell=1}^\infty \lambda_\ell^\star(Z_\ell^2-1),
% \end{align*}
% which establishes the claim.
\subsection{\texorpdfstring{Proof of \Cref{thm:wild-bootstrap-consistency}}{Proof of theorem: Wild Bootstrap Consistency with Estimated Kernel}}
\label{Proof of proposition: multiplier bootstrap consistency with estimated ratio}
Define the oracle multiplier bootstrap statistic
\begin{align*}
    \mathrm{MMD}^{2}_{\mathrm{Boot}}
    := \frac{1}{M(M-1)} \sum_{i \neq j} \xi_i \xi_j \, H(W_i, W_j),
\end{align*}
where $H$ uses the true density ratio $r_X$ instead of the estimator $\widehat r_X$.  
For comparison, define the oracle U-statistic
\begin{align*}
    U_M(H) := \frac{1}{M(M-1)} \sum_{i \neq j} H(W_i, W_j).
\end{align*}
We decompose
\begin{align*}
    M \cdot \widehat{\mathrm{MMD}}^2_{\mathrm{Boot}} - M \cdot U_M(H)
    &= \underbrace{\Big( M \cdot \widehat{\mathrm{MMD}}^2_{\mathrm{Boot}} - M \cdot \mathrm{MMD}^2_{\mathrm{Boot}} \Big)}_{=:\Delta_1} 
    + \underbrace{\Big( M \cdot \mathrm{MMD}^2_{\mathrm{Boot}} - M \cdot U_M(H) \Big)}_{=:\Delta_2}.
\end{align*}
We show that $\Delta_1$ is asymptotically negligible, while $\Delta_2$ is asymptotically consistent. so that conditional on $\mathbb{W}_M=(W_{1},\ldots,W_{M})$, $M\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{Boot}}$ and $M\cdot U_M(H)$ share the same asymptotic distribution under the null hypothesis. This establishes the consistency of the multiplier bootstrap for approximating the null distribution.
\begin{enumerate}
\item\textbf{Term $\Delta_1$.}  
We first analyze term~$\Delta_1$, whose goal is to show
\begin{align*}
    \Delta_1 = o_{\mathcal{P}}(1).
\end{align*}
We begin by expressing the multiplier bootstrap version of the quadratic-time MMD statistic as
\begin{align*}
    \widehat{\mathrm{MMD}}_{\text{Boot}}^2 
    = \frac{M}{M-1}\big(V_{M}^{\text{(Boot)}}\big)^2 
      - \frac{1}{M(M-1)}\sum_{i=1}^{M}\xi_i\,\widehat{H}(W_i,W_i),
\end{align*}
where
\begin{align*}
V_{M}^{\text{(Boot)}} 
    := \Bigg\| \frac{1}{M}\sum_{i=1}^{M}\psi(V_i^{(1)}) 
       - \frac{1}{M}\sum_{i=1}^{M}\xi_i\,\widehat r_X(X_i^{(2)})\,\psi(V_i^{(2)})\Bigg\|.
\end{align*}
Following the decomposition in 
\Cref{Theorem : Asymptotic distribution of quadratic time MMD} 
(\Cref{Proof of Theorem: Asymptotic Distribution of Quadratic-Time MMD}), 
we separate the population and estimation-error components:
\begin{align*}
\sqrt{M}\,V_M^{\text{(Boot)}} 
&= 
\Bigg\|
\underbrace{\frac{1}{\sqrt{M}}\sum_{i=1}^M \xi_i
  \big\{\psi(V_i^{(1)}) - r_X(X_i^{(2)})\,\psi(V_i^{(2)})\big\}}_{=:(\mathbf{I})}
 + 
\underbrace{\frac{1}{\sqrt{M}}\sum_{i=1}^M \xi_i
  \psi(V_i^{(2)})\big\{r_X(X_i^{(2)})-\widehat r_X(X_i^{(2)})\big\}}_{=:(\mathbf{II})}
\Bigg\| \\
&= \sqrt{\|(\mathbf{I})\|^2 + \|(\mathbf{II})\|^2 + 2\langle (\mathbf{I}),(\mathbf{II})\rangle}.
\end{align*}
The diagonal correction term expands as
\begin{align*}
  \frac{1}{M}\sum_{i=1}^{M}\xi_i^2\,\widehat H(W_i,W_i)
  &= \frac{1}{M}\sum_{i=1}^{M} \xi_i^2\, H(W_i,W_i) 
   + \frac{1}{M}\sum_{i=1}^{M}\xi_i^2
      \big\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\big\}^2 k(V_i^{(2)},V_i^{(2)}) \\
  &\quad - \frac{2}{M}\sum_{i=1}^{M}\xi_i^2
      \big\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\big\}\, k(V_i^{(1)},V_i^{(2)}).
\end{align*}
Combining the above expressions gives
\begin{align*}
\frac{M-1}{M}\Big(M\widehat{\mathrm{MMD}}^2_{\mathrm{Boot}} - M\mathrm{MMD}^2_{\mathrm{Boot}}\Big)
&= \|(\mathbf{II})\|^2 + 2\langle (\mathbf{I}),(\mathbf{II})\rangle \\ 
&\quad - \frac{1}{M}\sum_{i=1}^M \xi_i^2\,\big\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\big\}^2 k(V_i^{(2)},V_i^{(2)}) \\ 
&\quad + \frac{2}{M}\sum_{i=1}^M \xi_i^2\,\big\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\big\}\,k(V_i^{(1)},V_i^{(2)}) \\ 
&=: T_{1}+T_{2}+T_{3}.
\end{align*}
Since $(M-1)/M \to 1$, the prefactor does not affect the asymptotic behavior by the uniform Slutsky lemma (\Cref{Lemma: uniform Slutsky}).  
It thus suffices to show
\begin{align*}
T_{1} = o_{\mathcal{P}}(1), 
\quad 
T_{2} = o_{\mathcal{P}}(1), 
\quad 
T_{3} = o_{\mathcal{P}}(1).
\end{align*}
\noindent\textbf{Term $T_{1} . $}
By the Cauchy--Schwarz inequality, 
$|\langle (\mathbf{I}),(\mathbf{II})\rangle| \leq \|(\mathbf{I})\|\|(\mathbf{II})\|$, 
so it suffices to show $\|(\mathbf{I})\| = O_{\mathcal{P}}(1)$ and 
$\|(\mathbf{II})\| = o_{\mathcal{P}}(1)$.
From the definition,
\begin{align*}
\|(\mathbf{I})\|^2 = \frac{1}{M}\sum_{i,j=1}^{M}\xi_i\xi_j\,\widetilde{H}_n(W_i,W_j)
= \sum_{\ell=1}^{\infty}\lambda_{\ell,n}
\Big(\frac{1}{\sqrt{M}}\sum_{i=1}^{M}\xi_i\Psi_{\ell,n}(W_i)\Big)^2.    
\end{align*}
Let $\{a_\ell\}_{\ell\ge1}$ be i.i.d.\ standard Gaussian random variables. Then
\begin{align*}
\|(\mathbf{I})\|^2 
= 
\underbrace{\sum_{\ell=1}^{\infty}\lambda_{\ell,n}a_\ell^2
\Big(\tfrac{1}{M}\sum_{i=1}^{M}\Psi_{\ell,n}^2(W_i)-1\Big)}_{\Delta_1}
+ 
\underbrace{\sum_{\ell=1}^{\infty}\lambda_{\ell,n}(a_\ell^2-1)}_{\Delta_2}
+ 
\sum_{\ell=1}^{\infty}\lambda_{\ell,n}.    
\end{align*}
Here, $\Delta_1$ and $\Delta_2$ capture all stochastic fluctuations, 
while $\sum_{\ell}\lambda_{\ell,n}$ is deterministic for each $n$ and serves only as a centering constant, even under a triangular array.


For $\Delta_1$, note that $\mathbb{E}[\Delta_1]=0$. Hence,
\begin{align*}
    \mV[\Delta_1] 
    &= \mathbb{E}\big[\mV[\Delta_1 \mid \mathbb{W}_M]\big] 
    + \mV\big[\mathbb{E}[\Delta_1 \mid \mathbb{W}_M]\big].
\end{align*}
Since $\mV[a_{\ell}^2]=2$, we obtain
\begin{align*}
    \mathbb{E}\big[\mV[\Delta_1 \mid \mathbb{W}_M]\big] 
    &= \mathbb{E}\bigg[2\sum_{\ell=1}^{\infty}\lambda_{\ell,n}^2
    \Big(\frac{1}{M}\sum_{i=1}^{M}\Psi_{\ell,n}^2(W_i)-1\Big)^2 \bigg] \\
    &= 2\sum_{\ell=1}^{\infty}\lambda_{\ell,n}^2
    \mathbb{E}\bigg[\Big(\frac{1}{M}\sum_{i=1}^{M}\Psi_{\ell,n}^2(W_i)-1\Big)^2\bigg] \\
    &\stackrel{(i)}{=} 2\sum_{\ell=1}^{\infty}\lambda_{\ell,n}^2
    \mV\Big[\frac{1}{M}\sum_{i=1}^{M}\Psi_{\ell,n}^2(W_i)\Big] \\
    &= 2\sum_{\ell=1}^{\infty}\lambda_{\ell,n}^2
    \frac{1}{M}\mV[\Psi_{\ell,n}^2(W_1)] \\
    &\stackrel{(ii)}{\leq} \frac{2}{M}
\mathbb{E}\Big[\sum_{\ell=1}^{\infty}\lambda_{\ell,n}^2\Psi_{\ell,n}^4(W_1)\Big] 
    = \frac{2}{M}\mathbb{E}\big[\widetilde{H}_{n}(W_1,W_1)^2\big],
\end{align*}
where $(i)$ follows from $\mathbb{E}[\tfrac{1}{M}\sum_{i=1}^{M}\Psi_{\ell,n}^2(W_i)] = 1$,  
and $(ii)$ uses the orthonormality of $\{\Psi_{\ell,n}\}_{\ell\ge1}$.

Similarly,
\begin{align*}
   \mV\big[\mathbb{E}[\Delta_1 \mid \mathbb{W}_M]\big] 
   &= \mV\bigg[\sum_{\ell=1}^{\infty}\lambda_{\ell,n}
    \Big(\frac{1}{M}\sum_{i=1}^{M}\Psi_{\ell,n}^2(W_i)-1\Big)\bigg] \\
    &= \sum_{\ell=1}^{\infty}\lambda_{\ell,n}^2
    \mV\Big[\frac{1}{M}\sum_{i=1}^{M}\Psi_{\ell,n}^2(W_i)\Big] \\
    &\leq \frac{1}{M}\mathbb{E}\big[\widetilde{H}_{n}(W_1,W_1)^2\big],
\end{align*}
where the final inequality follows from the same reasoning as above.
We conclude that 
\begin{align}\label{eq:delta_bound}
    \mV[\Delta_1] 
    \leq \frac{3}{M}\,\mathbb{E}\big[\widetilde{H}_{n}(W_1,W_1)^2\big].
\end{align}
\revised{(Should we need?) }Assuming $\mathbb{E}[\widetilde{H}_{n}(W_1,W_1)^2] < \infty$, 
Chebyshev's inequality yields $\Delta_{1}=o_{\mathcal{P}_{n}^{(0)}}(1)$.

For $\Delta_2$, note that $\mathbb{E}[\Delta_2]=0$, and
\begin{align*}
    \mV[\Delta_2] 
    = 2\sum_{\ell=1}^{\infty}\lambda_{\ell,n}^2,
\end{align*}
with
\begin{align*}
    \sup_{P_{n}\in\mathcal{P}_{n}^{(0)}}
    \sum_{\ell=1}^{\infty}\lambda_{\ell,n}^2 < c.
\end{align*}
Hence, by Chebyshev's inequality, $\Delta_2 = O_{\mathcal{P}_{n}^{(0)}}(1)$.
Combining these results, we have 
$\|\I\|^2 = O_{\mathcal{P}_{n}^{(0)}}(1)$, and thus $\|\I\| = O_{\mathcal{P}_{n}^{(0)}}(1)$.

Turning to $\|\II\|$, we obtain
\begin{align*}
\|\II\|
&\le M^{1/2}
\Big(\tfrac{1}{M}\sum_{i=1}^M \|\psi(V_i^{(2)})\|^2\Big)^{1/2}
\Big(\tfrac{1}{M}\sum_{i=1}^M 
\xi_i^2\{r_X(X_i^{(2)}) - \widehat r_X(X_i^{(2)})\}^2\Big)^{1/2}.
\end{align*}
The first factor is bounded by a constant 
(by \Cref{Assumption : quadratic time MMD}(c)), 
while the second factor equals 
$o(N^{-1/4})$ by \Cref{Assumption : quadratic time MMD}(b).  
Hence
\[
\|\II\| = o_{\mathcal{P}_n^{(0)}}(M^{1/2}/N^{1/4}).
\]
By \Cref{Assumption : quadratic time MMD}(d), 
this implies $\|\II\| = o_{\mathcal{P}_n^{(0)}}(1)$, and by 
\citet[Lemma~S5]{lundborg2022projected}, $\|\II\|^2 = o_{\mathcal{P}_n^{(0)}}(1)$.
Therefore,
\[
T_1 = \|\II\|^2 + 2\langle \I,\II\rangle = o_{\mathcal{P}_n^{(0)}}(1).
\]
Because the kernel $k$ is uniformly bounded by $K$, we have
\begin{align*}
    \mathbb{E}[T_{2}] 
    &\le K\,\mathbb{E}\Big[\tfrac{1}{M}\sum_{i=1}^M 
    \{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\}^2\Big], \\
    \mV[T_{3}] 
    &\le 8K\,\mathbb{E}\big[\{\widehat r_X(X_i^{(2)})-r_X(X_i^{(2)})\}^2\big].
\end{align*}
Applying Markov’s inequality to $T_{2}$ and Chebyshev’s inequality to $T_{3}$, we obtain
\begin{align*}
    T_{2} = o_{\mathcal{P}_n^{(0)}}(N^{-1/2})
    \quad \text{and} \quad
    T_{3} = o_{\mathcal{P}_n^{(0)}}(N^{-1/4}),
\end{align*}
so both terms vanish asymptotically.




    \item\textbf{Term $\I$. }
\revised{Need to revised}
\end{enumerate}






\subsection{\texorpdfstring{Proof of \Cref{thm:wild-bootstrap-consistency}}{Proof of Proposition: Wild Bootstrap Consistency with Estimated Kernel}}
\label{Proof of proposition: Wild bootstrap consistency with estimated ratio}
\begin{proof}
Define the wild bootstrap statistic with the known density ratio, denoted by $\mathrm{MMD}^{2}_{\mathrm{wild}}$, as
\begin{align*}
    \mathrm{MMD}^{2}_{\mathrm{wild}} 
    := \frac{1}{M(M-1)} \sum_{i \neq j} \xi_i \xi_j \, H(W_i, W_j),
\end{align*}
where $H$ is the counterpart of $\widehat{H}$ obtained by using the true density ratio $r_X$ instead of its estimator $\widehat{r}_X$. 
For comparison, we also define the oracle U-statistic
\begin{align*}
    U_M(H) := \frac{1}{M(M-1)} \sum_{i \neq j} H(W_i, W_j).
\end{align*}
We decompose the difference between the wild bootstrap statistic with the estimated kernel 
and the oracle U-statistic:
\begin{align*}
    M \cdot\widehat{\mathrm{MMD}}^2_{\mathrm{wild}} - M \cdot U_M(H)
    &= \underbrace{\Big( M \cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}} - M \cdot \mathrm{MMD}^2_{\mathrm{wild}} \Big)}_{\I} 
    + \underbrace{\Big( M \cdot \mathrm{MMD}^2_{\mathrm{wild}} - M \cdot U_M(H) \Big)}_{\II}.
\end{align*}
We show that $\I$ is asymptotically negligible, while $\II$ is asymptotically consistent. so that $M\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}}$ and $M\cdot U_M(H)$ share the same asymptotic distribution under the null. This establishes the consistency of the wild bootstrap for approximating the null distribution.
\begin{enumerate}
    \item\textbf{Term $\II$. }We first analyze term $\II$, which plays a key role in establishing consistency. 
Recall the definition of the wild bootstrap statistic:
\begin{align*}
\mathrm{MMD}^2_{\mathrm{wild}}
= \frac{1}{M(M-1)} \sum_{i \neq j} \xi_i \xi_j \,\widetilde{H}_n(W_i,W_j),
\end{align*}
where $\{W_i\}_{i=1}^M$ are i.i.d.\ samples and $\{\xi_i\}_{i=1}^M$ are independent bootstrap multipliers. 
Under the null hypothesis we may replace $H$ with $\widetilde{H}_n$. 
By the spectral decomposition of $\widetilde{H}_n$, we obtain
\begin{align*}
M \cdot \mathrm{MMD}^2_{\mathrm{wild}}
= \frac{M}{M-1}\sum_{\ell=1}^\infty \lambda_{\ell,n}^{(P_n)}
\left\{
\Big(\frac{1}{\sqrt{M}} \sum_{i=1}^M \xi_i \Psi_{\ell,n}^{(P_n)}(W_i)\Big)^2
- \frac{1}{M}\sum_{i=1}^M \xi_i^2 \,(\Psi_{\ell,n}^{(P_n)}(W_i))^2
\right\}.
\end{align*}

Conditional on $\mathbb{W}=\{W_1,\dots,W_M\}$, the linear terms have mean zero and variance 
$\frac{1}{M}\sum_{i=1}^M (\Psi_{\ell,n}^{(P_n)}(W_i))^2$. 
By the conditional central limit theorem for multipliers, for each fixed $\ell$ the normalized sum 
converges in distribution to a centered Gaussian random variable, so the leading quadratic form converges to a weighted sum of independent $\chi^2_1$ variables with weights $\{\lambda_{\ell,n}^{(P_n)}\}$. 
The diagonal correction term converges in probability to $1$ by the law of large numbers.

Therefore, conditionally on $\mathbb{W}$ and for any sequence $\{P_{n}\}_{n \geq 1}$ with $P_{n} \in \mathcal{P}_{n}^{(0)}$, the asymptotic distribution of 
$M \cdot \mathrm{MMD}^2_{\mathrm{wild}}$ is given by
\begin{align*}
\sum_{\ell=1}^\infty \lambda_{\ell,n}^{(P_n)}(a_\ell^2-1),
\qquad a_\ell \stackrel{\text{i.i.d.}}{\sim} N(0,1),
\end{align*}
which coincides with the null limiting distribution of the quadratic-time MMD U-statistic. 
Since the limiting distribution is continuous, Polya’s theorem implies uniform convergence of the corresponding distribution functions. In particular,
\begin{align*}
\lim_{n\to\infty}
\sup_{P_n \in \mathcal{P}_n^{(0)}} \sup_{x \in \mathbb{R}}
\Big|
\mathbb{P}_{P_n}\!\big(M \cdot \mathrm{MMD}^2_{\mathrm{wild}} \le x \mid \mathbb{W}\big)
-
\mathbb{P}_{P_n}\big(M \cdot U_M(H) \le x\big)
\Big|=0.
\end{align*}
This establishes the conditional and uniform consistency of the wild bootstrap.
    \item\textbf{Term $\I$. }
Similar to \Cref{Proof of Theorem: Asymptotic Distribution of Quadratic-Time MMD}, we define
\begin{align*}
    V_M^{\text{(wild)}} 
    := \bigg\|\frac{1}{M}\sum_{i=1}^M \xi_i \psi(V_i^{(1)})
    - \frac{1}{M}\sum_{i=1}^M \xi_i\,\widehat{r}_X(X_i^{(2)})\psi(V_i^{(2)}) \bigg\|_{\mathcal H_k}.
\end{align*}
A direct expansion yields
\begin{align*}
    M\cdot \widehat{\mathrm{MMD}}_{\text{wild}}^2
    = M\big(V_M^{\text{(wild)}}\big)^2
    + \Big(\frac{M}{M-1}-1\Big)\cdot M\big(V_M^{\text{(wild)}}\big)^2
    - \frac{M}{M-1}\cdot \frac{1}{M}\sum_{i=1}^M \xi_i^2 \widehat H(W_i,W_i).
\end{align*}

As before, the main stochastic contribution comes from $M(V_M^{\text{(wild)}})^2$, while the last two terms act as centering and vanish asymptotically.  
Decomposing $\sqrt{M}V_M^{\text{(wild)}}$ into
\begin{align*}
T_{1} := \frac{1}{\sqrt{M}}\sum_{i=1}^M \xi_i \big\{\psi(V_i^{(1)})-r_X(X_i^{(2)})\psi(V_i^{(2)})\big\}, 
\quad
T_{2} := \frac{1}{\sqrt{M}}\sum_{i=1}^M \xi_i\, \psi(V_i^{(2)})\{r_X(X_i^{(2)})-\widehat r_X(X_i^{(2)})\},
\end{align*}
we obtain 
\begin{align*}
M(V_M^{\text{(wild)}})^2 = \|T_{1}\|_{\mathcal H_k}^2 + o_{\mathcal{P}_{n}^{(0)}}(1).    
\end{align*}
Conditional on $\mathbb{Z}$, the structure of $T_{1}$ coincides with that of 
Term~$\II$ analyzed in the earlier step. 
Hence, the same spectral decomposition and CLT arguments apply, and $\|T_{1}\|_{\mathcal H_k}^2$ converges in distribution to an infinite weighted sum of independent $\chi^2_1$ random variables. The remainder $T_{2}$ has mean zero, and its conditional variance is negligible under \Cref{Assumption : quadratic time MMD}(c)–(e). 
By Chebyshev’s inequality, it follows that $T_{2}=o_{\mathcal{P}_{n}^{(0)}}(1).$ Finally, since the diagonal correction term satisfies $\xi_i^2=1$, the similar argument as in \Cref{Proof of Theorem: Asymptotic Distribution of Quadratic-Time MMD} applies. 
Therefore, the difference between the wild bootstrap statistic based on $\widehat H$ and its oracle counterpart based on $H$ is asymptotically negligible conditional on $\mathbb{W}$.     
\end{enumerate}
Combining the analyses of Term~$\I$ and Term~$\II$, we conclude that for any sequence $\{P_n\}_{n\ge 1}$ with $P_n \in \mathcal{P}_n^{(0)}$,
\begin{align*}
    \lim_{n \to \infty}
    \sup_{P_n \in \mathcal{P}_{n}^{(0)}}
    \sup_{x \in \mathbb{R}}
    \Big|
    \mathbb{P}_{P_n}\big(M\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}} \leq x \mid \mathbb{W}\big)
    -
    \mathbb{P}_{P_n}\big(G_{n}^{(P_{n})} \leq x\big)
    \Big| = 0,
\end{align*}
where
\begin{align*}
G_{n}^{(P_{n})} \;\sim\; \sum_{\ell=1}^\infty \lambda_{\ell,n}^{(P_n)}(a_\ell^2-1),
\qquad a_\ell \stackrel{\text{i.i.d.}}{\sim} N(0,1).
\end{align*}
That is, the plug-in wild bootstrap statistic $M \cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}}$ converges, conditional on $\mathbb{W}$, to the same sum of infinitely weighted chi-squared random variables as the oracle statistic. Consequently, the wild bootstrap procedure based on the estimated density ratio is conditionally consistent for approximating the null distribution of the quadratic-time MMD statistic.

\end{proof}

\subsection{\texorpdfstring{Proof of \Cref{prop:bootstrap-quantile-consistency}}{Proof of proposition: bootstrap quantile consistency}}
We adopt the proof of Proposition 1 of \textcolor{blue}{Lee et al. (2025)}




% \subsection{\texorpdfstring{Proof of \Cref{thm:fixed k p consistent}}{Proof of theorem: fixed P k consistency}}
% To prove consistency, fix $P:=P_{XY}^{(1)}$ and $Q:=P_{XY}^{(2)}$ and observe that
% \begin{align*}
%     \mE_{P,Q}[1-\Psi(\mV^{(1)}, \mV^{(2)})] 
%     &= \mP_{P,Q}\!\left[n \cdot U_{n}^{\mathrm{res}}(\widehat{r}_{X}) \leq t_{1-\alpha}\right] \\
%     &= \mP_{P,Q}\!\Big[\underbrace{nU_{n}^{\mathrm{res}}(r_{X})}_{=:A_{n}}
%       + \underbrace{n\big(U_{n}^{\mathrm{res}}(\widehat{r}_{X})-U_{n}^{\mathrm{res}}(r_{X})\big)}_{=:B_{n}}
%       \leq t_{1-\alpha}\Big].
% \end{align*}
% For any $\epsilon>0$, this yields the decomposition
% \begin{align}\label{eq : power}
%     \mE_{P,Q}[1-\Psi(\mV^{(1)}, \mV^{(2)})]
%     \leq \mP_{P,Q}[A_{n}\leq t_{1-\alpha} + \epsilon]
%     + \mP_{P,Q}[|B_{n}| \geq \epsilon].
% \end{align}
% Under the condition
% \begin{align*}
%     \mE\!\left[\Delta(X^{(2)})^2 \,\|R(V^{(2)})\|_\cH^2\right] 
%     = o\!\left(\frac{1}{n}\right),
% \end{align*}
% the term $B_{n} := n\big(U_{n}^{\mathrm{res}}(\widehat{r}_{X}) - U_{n}^{\mathrm{res}}(r_{X})\big)$ 
% is negligible in probability. Hence for any $\epsilon > 0$,
% \begin{align*}
%     \mP_{P,Q}[\,|B_{n}|\geq \epsilon\,] \;\longrightarrow\; 0 \quad \text{as } n\to\infty.
% \end{align*}
% It sufficies to show that first term of \eqref{eq : power} converges to zero to complete the proof. Recall the wild bootstrap statistic defined in \eqref{equation : wild bootstrap MMD}. Here we focus instead on the version based on the density ratio $r_X$.
% By construction, $\mE[W_i]=0$ and $\mE[W_iW_j]=0$ for $i\neq j$. Hence,
% \begin{align*}
% \mE\Big[n\cdot\widehat{\mathrm{MMD}}^2_{\mathrm{wild}} \given \mathbb{Z}_n\Big] = 0.    
% \end{align*}
% For the variance, symmetry of the kernel implies that only index pairings 
% $(i,j)=(k,\ell)$ or $(i,j)=(\ell,k)$ survive. This gives
% \begin{align*}
% \mV\!\left(n\cdot\widehat{\mathrm{MMD}}^2_{\mathrm{wild}} \,\middle|\, \mathbb{Z}_n\right)
% = \frac{4}{(n-1)^2}\sum_{i<j}\big(\widetilde H^{(r_X)}(W_i,W_j)\big)^2.    
% \end{align*}
% If
% \begin{align*}
%  \mE\big[r_X(X^{(2)})^4 \,\|R(V^{(2)})\|_\cH^4\big] < \infty,     
% \end{align*}
% then by Cauchy–Schwarz,
% \begin{align*}
% \mE\!\left[(\widetilde H^{(r_X)}(Z_1,Z_2))^2\right]<\infty,
% \end{align*}
% and therefore
% \begin{align*}
% \mE\!\left[\mV\!\left(n\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}} \,\middle|\, \mathbb Z_n\right)\right] 
% = \frac{2n}{n-1}\,\mE\!\left[(\widetilde H^{(r_X)}(Z_1,Z_2))^2\right]
% \;\le\; C,
% \end{align*}
% for some constant $C<\infty$ and all $n\ge 2$. Fix $M>0$. By conditional Chebyshev’s inequality,
% \begin{align*}
% \mP\!\left(\big|n\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}}\big|>M \,\middle|\, \mathbb Z_n\right)
% \;\le\; \frac{\mV\!\left(n\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}} \,\middle|\, \mathbb Z_n\right)}{M^2}.    
% \end{align*}
% Taking expectations and using the variance bound, we obtain
% \begin{align*}
%  \mP\!\left(\big|n\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}}\big|>M\right)
% \;\le\; \frac{C}{M^2}.   
% \end{align*}
% Thus, for any $\epsilon>0$, one can choose $M$ sufficiently large so that
% \begin{align*}
% \mP\!\left(\big|n\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}}\big|>M\right) < \epsilon.
% \end{align*}
% By the definition of the conditional $(1-\alpha)$ quantile $t_{1-\alpha}(\mathbb Z_n)$,
% if for some finite $M>0$ we have
% \begin{align*}
% \mP(|n\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}}|\le M \mid \mathbb Z_n) \;\ge\; 1-\alpha,
% \end{align*}
% then necessarily
% \begin{align*}
% t_{1-\alpha}(\mathbb Z_n)\;\le\; M.
% \end{align*}
% Moreover, Chebyshev’s inequality shows that for any $\epsilon>0$ one can choose 
% $M<\infty$ large enough so that
% \begin{align*}
% \mP\!\left(\mP(|n\cdot \widehat{\mathrm{MMD}}^2_{\mathrm{wild}}|>M \mid \mathbb Z_n)>\alpha\right) < \epsilon.
% \end{align*}
% In other words, the quantile is always bounded by some sufficiently large but finite constant $M$ (with probability at least $1-\epsilon$). Hence
% \begin{align*}
% t_{1-\alpha}(\mathbb Z_n) = O_p(1).
% \end{align*}
% so that for any fixed $\epsilon>0$,
% \begin{align*}
% \frac{t_{1-\alpha}+\epsilon}{n} = o_p(1).    
% \end{align*}
% Let the event $\mathcal{E}_n := \left\{ \frac{t_{1-\alpha}+\epsilon}{n} \;\geq\; \tfrac{\gamma_{\mathrm{res}}^2}{2} \right\}$. Then $\mP(\mathcal{E}_n) \;\to\; 0.$
% Now observe that
% \begin{align*}
% \Big\{ U_{n}^{\mathrm{res}}(r_X) \leq \tfrac{t_{1-\alpha}+\epsilon}{n}\Big\}
% \subseteq 
% \Big\{ U_{n}^{\mathrm{res}}(r_X) \leq \tfrac{\gamma_{\mathrm{res}}^2}{2}\Big\} \;\cup\; \mathcal{E}_n,
% \end{align*}
% so that
% \begin{align*}
%     \mP_{P,Q}\!\Big[U_{n}^{\mathrm{res}}(r_X) \leq \tfrac{t_{1-\alpha} + \epsilon}{n}\Big] 
%     &\leq \mP_{P,Q}\!\Big[U_{n}^{\mathrm{res}}(r_X) \leq \tfrac{\gamma_{\mathrm{res}}^2}{2}\Big] 
%        + \mP(\mathcal{E}_n).
% \end{align*}
% Since $\mE[U_{n}^{\mathrm{res}}(r_X)] = \gamma_{\mathrm{res}}^2$, Chebyshev’s inequality yields
% \begin{align*}
%     \mP_{P,Q}\!\Big[U_{n}^{\mathrm{res}}(r_X) \leq \tfrac{\gamma_{\mathrm{res}}^2}{2}\Big]
%     &\leq \mP_{P,Q}\!\Big[\,|U_{n}^{\mathrm{res}}(r_X)-\gamma_{\mathrm{res}}^2| \geq \tfrac{\gamma_{\mathrm{res}}^2}{2}\,\Big] \\
%     &\leq \frac{4}{\gamma_{\mathrm{res}}^4}\,\mV\!\big[U_{n}^{\mathrm{res}}(r_X)\big].
% \end{align*}
% Therefore,
% \begin{align*}
%     \mP_{P,Q}\!\Big[U_{n}^{\mathrm{res}}(r_X) \leq \tfrac{t_{1-\alpha} + \epsilon}{n}\Big] 
%     \leq \frac{4}{\gamma_{\mathrm{res}}^4}\,\mV\!\big[U_{n}^{\mathrm{res}}(r_X)\big] 
%        + \mP(\mathcal{E}_n).
% \end{align*}
% Finally, since $\mP(\mathcal{E}_n)\to 0$, the bound above implies that 
% $\mV\!\big[U_{n}^{\mathrm{res}}(r_X)\big] \to 0$. 
% As $\gamma_{\mathrm{res}}^2$ is a fixed positive constant under the alternative, 
% it suffices to control the variance, which completes the argument.

% Since $U_{n}^{\mathrm{res}}(r_X)$ is a second-order U-statistic, its variance can be analyzed via the Hoeffding decomposition \citep{serfling1980approximation}
% \begin{align*}
% \mV(U_n(h)) 
% = \frac{4}{n}\,\mV\big(\mE[h(V_1,V_2)\mid V_1]\big) 
% + O\Big(\frac{1}{n^2}\Big).
% \end{align*}
% Applying this to the residual kernel $\widetilde H^{(r_X)}$ gives
% \begin{align*}
% \mV\big(U_{n}^{\mathrm{res}}(r_X)\big) 
% = \frac{4}{n}\,\mV(h_1(V)) + O\Big(\tfrac{1}{n^2}\Big),
% \end{align*}
% where $h_1(V)=\mE[\widetilde H^{(r_X)}(V,V')\mid V]$. 
% Since $h_1(V)$ involves terms of the form $r_X(X^{(2)})\|R(V^{(2)})\|_{\cH}^2$, we can bound
% \begin{align*}
% \mV(h_1(V)) 
% \;\lesssim\; \mE\left[r_X(X^{(2)})^4 \,\|R(V^{(2)})\|_{\cH}^4\right].
% \end{align*}
% Hence
% \begin{align*}
% \mV\big(U_{n}^{\mathrm{res}}(r_X)\big) 
% = O\!\left(\frac{1}{n}\,\mE\!\left[r_X(X^{(2)})^4 \,\|R(V^{(2)})\|_{\cH}^4\right]\right).
% \end{align*}
% Thus, a finite fourth moment condition,
% $\mE[r_X(X^{(2)})^4\|R(V^{(2)})\|_\cH^4] < \infty$,
% is sufficient to guarantee that the variance converges to zero.



\subsection{\texorpdfstring{Proof of \Cref{Example : Stable case}}{Proof of Example : Stable case}}\label{Appendix : Proof of Example : Stable case}


\noindent In this section, we aim to present a detailed analysis of the asymptotic equivalence between the GCM statistic $T_n$ and its counterpart $\tilde{T}_n$ constructed using $\{(\tilde{X}_i,\tilde{Y}_i,\tilde{Z}_i)\}_{i=1}^n$. Throughout this section, we assume that $Y$ has a finite second moment and that $Y \independent Z \given X$, i.e., the null hypothesis holds. Let
\begin{align*}
	T_n = \frac{\frac{1}{\sqrt{n}}\sum_{i=1}^n R_i}{\big\{ \frac{1}{n} \sum_{i=1}^n R_i^2 - \bigl(\frac{1}{n} \sum_{r=1}^n R_r\bigr)^2  \big\}^{1/2}} \coloneqq  \frac{\nu_{R}}{\hat{\sigma}_{R}},
\end{align*}
and $\tilde{T}_n \coloneqq  {\nu}_{\tilde{R}}/\hat{\sigma}_{\tilde{R}}$. Let $\sigma_{\tilde{R}}^2 > 0$ denote the variance of $\{\tilde{Y} - f(\tilde{X})\}\{\tilde{Z} - g(\tilde{X})\}$ where $(\tilde{X},\tilde{Y},\tilde{Z}) $ is a random draw from the joint distribution $P_{XYZ}$. We begin with an upper bound for $|T_n - \tilde{T}_n|$: 
\begin{align*}
	\Big|\frac{\nu_{R}}{\hat{\sigma}_{R}}-\frac{{\nu}_{\tilde{R}}}{\hat{\sigma}_{\tilde{R}}} \Big| &\leq
	\Big|\frac{\nu_{R}}{\hat{\sigma}_{R}}-\frac{{\nu}_{\tilde{R}}}{\hat{\sigma}_{R}}\Big|+\Big| \frac{{\nu}_{\tilde{R}}}{\hat{\sigma}_{R}}-\frac{{\nu}_{\tilde{R}}}{\hat{\sigma}_{\tilde{R}}} \Big| \\
	&\leq \frac{1}{\hat{\sigma}_{R}} \lvert \nu_{R}-{\nu}_{\tilde{R}}\rvert + \frac{\lvert {\nu}_{\tilde{R}}\rvert}{( \hat{\sigma}_{R}+\hat{\sigma}_{\tilde{R}})\hat{\sigma}_{R}\hat{\sigma}_{\tilde{R}}}\big| \hat{\sigma}_{R}^2-\hat{\sigma}_{\tilde{R}}^2\big|,
\end{align*}
from which the proof boils down to showing the convergence of the following four terms to zero in probability: (a) $\nu_R - \nu_{\tilde{R}}$, (b) $\hat{\sigma}_{R}^2-\hat{\sigma}_{\tilde{R}}^2$, (c) $\hat{\sigma}_R^2 - \sigma_{\tilde{R}}^2$ and (d) $\hat{\sigma}_{\tilde{R}}^2 - \sigma_{\tilde{R}}^2$. Under these convergence results, the asymptotic equivalence follows by the continuous mapping theorem along with the fact that $\nu_{\tilde{R}}$ is stochastically bounded by the central limit theorem. In what follows, we establish convergence of (a), (b), (c), and (d) to zero in probability in order. 

\medskip 

\begin{enumerate}
    \item \textbf{Term (a): $\nu_R - \nu_{\tilde{R}}$.} Starting with the term (a), the difference between $\nu_R$ and $\nu_{\tilde{R}}$ can be written as
\begin{align*}
	{\frac{1}{\sqrt{n}} \sum_{i=1}^n R_i - \frac{1}{\sqrt{n}} \sum_{i=1}^n \tilde{R}_i} = \underbrace{\frac{1}{\sqrt{n}} \sum_{i= n_1 + 1}^{\bar{n}_1} (R_i - \tilde{R}_i) \cdot \mathds{1}(\bar{n}_1 > n_1)}_{\coloneqq \Delta_{1}} + \underbrace{\frac{1}{\sqrt{n}} \sum_{i= \bar{n}_1 + 1}^{n_1} (R_i - \tilde{R}_i) \cdot \mathds{1}(\bar{n}_1 \leq n_1)}_{\coloneqq  \Delta_{2}}.
\end{align*}
Remark that $Z_i$ is a fixed constant for a given index $i$, which allows us to show that $\mE[R_i] = 0$ for any $i \in [n]$. For example, when $i=1$, $Z_1$ equals $1$ (since $X_{1} = X_{1}^{(1)}$) and thus the law of total expectation yields
\begin{align*}
	\mE[R_1] = \mE[\{1-g(X_{1})\}\{Y_{1}-f(X_{1})\}] =\mathbb{E}\big[\{1-g(X_{1})\}\, \mathbb{E}[\{Y_{1}-f(X_{1})\} \given X_1]\big] = 0,
\end{align*}
where we recall $f(X_1) = \mE[Y_1\given X_1]$. It also follows that $\mE[\tilde{R}_i] = 0$ for any $i \in [n]$ under the null hypothesis. This together with the law of total expectation shows that 
\begin{align*}
	\mE{[\Delta_1]} = \mE{\Bigg[\frac{1}{\sqrt{n}}\sum_{i=n_{1}+1}^{\bar{n}_1}\mathds{1}(\bar{n}_1 > n_1)\mE{\big[ R_i - \tilde{R}_i\given \bar{n}_1\big]} \Bigg]} = 0,
\end{align*}
and similarly $\mE{[\Delta_2]} = 0$. Thus, we have $\mE[\nu_R - \nu_{\tilde{R}}] = 0$. 

Now consider the variance of $\nu_R - \nu_{\tilde{R}}$. Since $\mE[\nu_R - \nu_{\tilde{R}}] = 0$ and $\mE[\Delta_1\Delta_2] =0$, we have 
\begin{align*}
	\mV[\nu_R - \nu_{\tilde{R}}] = \mV[\Delta_1] + \mV[\Delta_2]. 
\end{align*} 
For $\text{Var}[\Delta_1]$, we have
\begin{align*}
	\operatorname{Var}[\Delta_1] & =\mathbb{E}\bigl[\operatorname{Var}\big\{\Delta_1 \given \bar{n}_1\big\}\big]+\operatorname{Var}\big[\underbrace{\mathbb{E}\big\{\Delta_1 \given \bar{n}_1\big\}}_{=0}\big] \\
	&=
	\mE{\bigg[
		\operatorname{Var}\bigg\{\frac{1}{\sqrt{n}} \sum_{i= n_1 + 1}^{\bar{n}_1} (R_i - \tilde{R}_i) \cdot \mathds{1}(\bar{n}_1 > n_1) \;\Big|\; \bar{n}_1\bigg\}
		\bigg]} \\
	&=
	\mE{\bigg[\frac{1}{n}\sum_{i= n_1 + 1}^{\bar{n}_1}\mathds{1}(\bar{n}_1 > n_1)\;\mE{\big\{\big(R_{i}-\tilde{R}_{i} \big)^2 \; \big|\; \bar{n}_1\big\}} \bigg]}\\
	& \overset{\mathrm{(i)}}{\leq} 2 
	\mE{\bigg[\frac{\bar{n}_1-n_1}{n}\cdot\frac{1}{\bar{n}_1-n_1}\sum_{i=n_1+1}^{\bar{n}_1}\mE{\bigl(R_{i}^2+\tilde{R}_{i}^2 \given \bar{n}_1 \bigr)}\bigg]} \overset{\mathrm{(ii)}}{\leq} 
	\frac{4}{n} \mathbb{E}\left[\left|\bar{n}_1-n_1\right|\right] \mV(Y_1) \overset{\mathrm{(iii)}}{\leq} \frac{2}{\sqrt{n}} \mV(Y_1), 
\end{align*}
where (i) follows from the inequality $(x - y)^2 \leq 2x^2 + 2y^2$ and (ii) uses the law of total variance along with the fact that $R_i^2 \leq \{Y_i - f(X_i)\}^2$ and $\tilde{R}_i^2 \leq \{\tilde{Y}_i - f(\tilde{X}_i)\}^2$ since $Z_i, \tilde{Z}_i \in \{1,2\}$. For the last inequality (iii), we use $\mathbb{E}[|\bar{n}_1 - n_1|] \leq \sqrt{n}/2$. The same bound holds for $\mV[\Delta_2]$ and thus 
\begin{align*}
	\mV[\nu_R - \nu_{\tilde{R}}] \leq \frac{4}{\sqrt{n}} \mV(Y_1). 
\end{align*}
Combining the results with Chebyshev's inequality now shows that $\nu_R - \nu_{\tilde{R}}$ converges to zero in probability. 
\item \textbf{Term (b): $\hat{\sigma}_{R}^2-\hat{\sigma}_{\tilde{R}}^2$.} We next aim to show that 
\begin{align*}
	\hat{\sigma}_{R}^2-\hat{\sigma}_{\tilde{R}}^2 = \bigg\{ \frac{1}{n} \sum_{i=1}^n R_i^2 - \Big(\frac{1}{n} \sum_{r=1}^n R_r\Big)^2 \bigg\} - \bigg\{ \frac{1}{n} \sum_{i=1}^n \tilde{R}_i^2 - \Big(\frac{1}{n} \sum_{r=1}^n \tilde{R}_r\Big)^2  \bigg\}
\end{align*}
converges to zero in probability. We decompose this into two terms
\begin{align*}
	\mathbb{(I)} \coloneqq  \frac{1}{n} \sum_{i=1}^n R_i^2 - \frac{1}{n} \sum_{i=1}^n \tilde{R}_i^2 \quad \text{and} \quad
		\mathbb{(II)}  \coloneqq  \Big(\frac{1}{n} \sum_{r=1}^n R_r\Big)^2 - \Big(\frac{1}{n} \sum_{r=1}^n \tilde{R}_r\Big)^2,
\end{align*}
and show each of them converges to zero in probability. For the first term $\mathbb{(I)}$, we have
\begin{align*}
	\frac{1}{n} \sum_{i=1}^n R_i^2 - \frac{1}{n} \sum_{i=1}^n \tilde{R}_i^2 = \underbrace{\frac{1}{n}\sum_{n_{1}+1}^{\bar{n}_1}(R_{i}^2-\tilde{R}_{i}^2)\cdot\mathds{1}(\bar{n}_1 > n_{1})}_{\coloneqq  \tilde{\Delta}_1}+\underbrace{\frac{1}{n}\sum_{\bar{n}_1+1}^{n_{1}}(R_{i}^2-\tilde{R}_{i}^2)\cdot\mathds{1}(\bar{n}_1 \leq n_{1})}_{\coloneqq  \tilde{\Delta}_2}.
\end{align*}
Using the law of total expectation, we obtain
\begin{align*}
	\mE[|\tilde{\Delta}_1|]  &= \mE{\bigg[\Big|\frac{1}{n}\sum_{i=n_{1}+1}^{\bar{n}_1}(R_{i}^2-\tilde{R}_{i}^2)\cdot\mathds{1}(\bar{n}_1 > n_{1})\Big|\bigg]} \\
	&\leq \mE{\bigg[\frac{\bar{n}_1-n_1}{n}\cdot\frac{1}{\bar{n}_1-n_1}\sum_{i=n_{1}+1}^{\bar{n}_1} \mathds{1}(\bar{n}_1 > n_1) \cdot \mE{\bigl\{\lvert R_{i}^2 - \tilde{R}_{i}^2 \rvert \given \bar{n}_1 \bigr\}} \bigg]} \\
	&\leq \frac{2}{n} \mE[|\bar{n}_1 - n_1|] \mV(Y_1)  \leq \frac{\mV(Y_1)}{\sqrt{n}},
\end{align*}
where the first inequality is derived from Jensen's inequality, and the remaining steps follow from the previous results. A similar argument applies to $\tilde{\Delta}_2$. By Markov's inequality, $\mathbb{(I)}$ converges to zero.
For the second term $\mathbb{(II)}$, we have
\begin{align*}
	\bigg\lvert
	\Big(\frac{1}{n}\sum_{r=1}^n R_r\Big)^2 - \Big(\frac{1}{n} \sum_{r=1}^n \tilde{R}_r\Big)^2
	\bigg\rvert \leq \bigg\lvert \frac{1}{n} \sum_{r=1}^n R_r + \frac{1}{n} \sum_{r=1}^n \tilde{R}_r \bigg\rvert\cdot\bigg\lvert\frac{1}{n} \sum_{r=1}^n R_r - \frac{1}{n} \sum_{r=1}^n \tilde{R}_r\bigg\rvert,
\end{align*}
which can be shown to converge to zero in probability using the previous results. 
\item \textbf{Terms (c) and (d): $\hat{\sigma}_R^2 - \sigma_{\tilde{R}}^2$ and $\hat{\sigma}_{\tilde{R}}^2 - \sigma_{\tilde{R}}^2$.} We can see that the term (d) converges to zero in probability by the conventional law of large numbers. The term (c) also converges to zero as well since $\hat{\sigma}_R^2 - \sigma_{\tilde{R}}^2 = (\mathrm{c})' + (\mathrm{c})''$, where $(\mathrm{c})' = \hat{\sigma}_R^2 - \hat{\sigma}_{\tilde{R}}^2$ and $(\mathrm{c})'' = \hat{\sigma}_{\tilde{R}}^2 - \sigma_{\tilde{R}}^2$. In fact, $(\mathrm{c})' = (\mathrm{b})$ and $(\mathrm{c})'' = (\mathrm{d})$, and both are known to converge to zero in probability based on the previous results. 
\end{enumerate}
This completes the proof which shows that $T_n$ and $\tilde{T}_n$ are asymptotically equivalent for the stable case. \revised{Hence $T_n$ is stable in the sense of \Cref{def:stability}.}
 

\section{Supporting Lemmas} \label{Section: Supporting Lemmas}
In this section, we gather several lemmas from the existing literature, some of which we prove or adapt as they are employed in our framework. The proof of the following lemma can be found, for example, in \citet{mulzer2018five}.
\begin{lemma} \label{Lemma: Concentration inequality}
	Let $Z_1,\ldots,Z_n$ be i.i.d.~Bernoulli random variables with success probability $p \in [0,1]$ and $S_n = \sum_{i=1}^n Z_i$. For any $\delta \in [0,1]$, it holds that 
	\begin{align*}
		& \mP\{S_n \geq (1 + \delta) np\} \leq e^{- \frac{np \delta^2}{3}} \quad \text{and} \quad \mP\{S_n \leq (1 - \delta) np \} \leq e^{- \frac{np \delta^2}{3}}.
	\end{align*}
\end{lemma}
The following is the uniform central limit theorem result in \citet[][Lemma 18]{shah2020hardness}.
\begin{lemma} (\citealp[][Lemma 18]{shah2020hardness}) \label{Lemma: Uniform CLT}
	Let $\mathcal{P}$ be a family of distributions for a random variable $\zeta \in \mathbb{R}$ and suppose that $\zeta_1\zeta_2,\ldots$ are i.i.d.~copies of $\zeta$. For each $n \in \mathbb{N}$, let $S_n \coloneqq n^{-1/2} \sum_{i=1}^n \zeta_i$. Suppose that for all $P \in \mathcal{P}$, we have $\mE_P(\zeta) = 0$, $\mE_P(\zeta^2)= 1$ and $\mE_P(|\zeta|^{2+\eta}) < c$ for some $\eta,c>0$. We have that 
	\begin{align*}
		\lim_{n \rightarrow \infty} \sup_{P \in \mathcal{P}} \sup_{t \in \mathbb{R}} \big| \mP_P (S_n \leq t) - \Phi(t) \big| = 0.
	\end{align*} 
\end{lemma}

The next lemma corresponds to \citet[][Lemma S8]{lundborg2022projected} on conditional uniform central limit theorem. 
\begin{lemma} (\citealp[][Lemma S8]{lundborg2022projected}) \label{Lemma: conditional clt}
		Let $(X_{n, i})_{n \in \mathbb{N}, i \in [n]}$ be a triangular array of real-valued random variables and let $(\mathcal{F}_n)_{n \in \mathbb{N}}$ be a filtration on $\mathcal{F}$. Assume that
		\begin{enumerate}
			\item $X_{n, 1}, \dots, X_{n, n}$ are conditionally independent given $\mathcal{F}_n$, for each $n \in \mathbb{N}$;
			\item $\mathbb{E}_P(X_{n, i} \given \mathcal{F}_n) = 0$ for all $n \in \mathbb{N}, i \in [n]$;
			\item $\bigl| n^{-1} \sum_{i=1}^n \mathbb{E}_P(X_{n, i}^2 \given \mathcal{F}_n) - 1\bigr| = o_\mathcal{P}(1)$;
			\item there exists $\delta > 0$ such that
			\begin{align*}
			\frac{1}{n}\sum_{i=1}^n \mE_{P}\bigl(|X_{n, i}|^{2+\delta} \given \mathcal{F}_n\bigr) = o_{\mathcal{P}}(n^{\delta/2}).
			\end{align*}
		\end{enumerate}
		Then $S_n = n^{-1/2} \sum_{m=1}^n X_{n,m}$ converges uniformly in distribution to $N(0, 1)$, i.e. 
		\begin{align*}
		\lim_{n \to \infty} \sup_{P \in \mathcal{P}} \sup_{x \in \mathbb{R}} |\mathbb{P}_P(S_n \leq x) - \Phi(x)| = 0.
		\end{align*}
\end{lemma}

The next lemma corresponds to \citet[][Lemma 20]{lundborg2022projected} on uniform Slutsky's theorem. 
\begin{lemma} (\citealp[][Lemma 20]{shah2020hardness}) \label{Lemma: uniform Slutsky}
Let $\mathcal{P}$ be a family of distributions that determines the law of a sequences $\left(V_n\right)_{n \in \mathbb{N}}$ and $\left(W_n\right)_{n \in \mathbb{N}}$ of random variables. Suppose

\begin{align*}
    \lim _{n \rightarrow \infty} \sup _{P \in \mathcal{P}} \sup _{t \in \mathbb{R}}\left|\mathbb{P}_P\left(V_n \leq t\right)-\Phi(t)\right|=0.
\end{align*}

\noindent Then we have the following. 
\begin{itemize}
    \item[(a)] If $W_n=o_{\mathcal{P}}(1)$, we have $\lim _{n \rightarrow \infty} \sup _{P \in \mathcal{P}} \sup _{t \in \mathbb{R}}\left|\mathbb{P}_P\left(V_n+W_n \leq t\right)-\Phi(t)\right|=0$.
    \item[(b)] If $W_n=1+o_{\mathcal{P}}(1)$, we have $\lim _{n \rightarrow \infty} \sup _{P \in \mathcal{P}} \sup _{t \in \mathbb{R}}\left|\mathbb{P}_P\left(V_n / W_n \leq t\right)-\Phi(t)\right|=0$.
\end{itemize}
\end{lemma}

The next lemma corresponds to \citet[Lemma~S7]{lundborg2022projected}, which establishes a uniform continuous mapping theorem.

\begin{lemma}(\citealp[][Lemma S7]{lundborg2022projected})
Let $(X_n)_{n \in \mathbb{N}}$ be a sequence of real-valued random variables and let $X$ be another such variable. Assume that $|X_n-X|=o_{\mathcal{P}}(1)$ and let $h: \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function. Suppose that at least one of the following conditions hold:
\begin{itemize}
    \item[(i)] $h$ is uniformly continuous,
    \item[(ii)] $X$ is uniformly tight, that is,
    \begin{align*}
        \lim _{M \rightarrow \infty} \sup _{P \in \mathcal{P}} \mathbb{P}_P(|X|>M)=0.
    \end{align*}
\end{itemize}
Then $|h(X_n)-h(X)|=o_{\mathcal{P}}(1)$.
\end{lemma}

\medskip

The following two lemmas provide moment bounds for canonical $U$-statistics. Lemma~\ref{Lemma : moment bound U stat} gives a general inequality for decoupled kernels \citep{gine2000exponential}, while Lemma~\ref{Lemma : moment bound T1} specializes this to the block statistic $T_{1,1}$, showing that its $(2+\delta)$-moment is controlled by the kernel moment. Standard decoupling results 
\citep{delapena1995decoupling,delapena1999decoupling} then extend the bound to the original statistic.




\begin{lemma}(\citealp[][Equation~(3.3)]{gine2000exponential})
\label{Lemma : moment bound U stat}
Let $h : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ be a bounded canonical kernel, 
and define $h_{i,j} := h(X_i^{(1)}, X_j^{(2)})$ for $1 \leq i,j \leq n$, 
where $\{X_i^{(1)}\}_{i=1}^n$ and $\{X_j^{(2)}\}_{j=1}^n$ are independent sequences of random variables. 
Then, for any $p \geq 2$, there exists a constant $C:=C_p > 0$ such that
\begin{align*}
\mE\Big| \sum_{i=1}^n \sum_{j=1}^n h_{i,j} \Big|^p
\leq C_p \max \Biggl\{&
p^p \biggl(\sum_{i=1}^n \sum_{j=1}^n \mathbb{E}\, h_{i,j}^2\biggr)^{\!p/2}, p^{3p/2}\, \mathbb{E}_1 \max_{1 \leq i \leq n} 
   \biggl(\sum_{j=1}^n \mathbb{E}_2\, h_{i,j}^2\biggr)^{\!p/2}, \\
& p^{3p/2}\, \mathbb{E}_2 \max_{1 \leq j \leq n} \biggl(\sum_{i=1}^n \mathbb{E}_1\, h_{i,j}^2\biggr)^{p/2},  p^{2p}\, \mathbb{E}\max_{1 \leq i,j \leq n} |h_{i,j}|^p
\Biggr\}.
\end{align*}
Here, $\mathbb{E}_1$ and $\mathbb{E}_2$ denote expectations with respect to $X^{(1)}$ and $X^{(2)}$, respectively.
\end{lemma}


% lemma? corollary? 에 대한 설명도 추가하
\begin{lemma}[Moment bound for $T_{1,1}$ in \Cref{Proof of theorem: Asymptotic Normality of Block wise MMD}]
\label{Lemma : moment bound T1}
Let $H:\mathcal{W}\times\mathcal{W}\to\mathbb{R}$ be a symmetric canonical kernel. 
Define the decoupled statistic
\begin{align*}
T_{1,1}^{\mathrm{dec}}
= \frac{1}{B}\sum_{i=1}^B \sum_{j=1}^B H(W_i^{(1)},W_j^{(2)}),
\end{align*}
where $\{W_i^{(1)}\}_{i=1}^B$ and $\{W_j^{(2)}\}_{j=1}^B$ are independent i.i.d.\ samples from $P$. 
Then, for any $\delta > 0$, there exists a constant $C_\delta > 0$, independent of $B$, such that
\begin{align*}
\mE\big[|T_{1,1}^{\mathrm{dec}}|^{2+\delta}\big] 
\;\leq\; C_\delta \,\mE[|H(W_1,W_2)|^{2+\delta}].
\end{align*}
Moreover, by classical decoupling inequalities 
\citep[see][]{delapena1995decoupling,delapena1999decoupling}, 
the same bound also holds for the original $U$-statistic
\begin{align*}
T_{1,1}
= \frac{2}{B(B-1)}\sum_{1 \leq i < j \le B} H(W_i,W_j),
\end{align*}
up to a universal constant independent of $B$.
\end{lemma}

Proof of \Cref{Lemma : moment bound T1}. Fix $p=2+\delta$ for some $\delta>0$. Applying Lemma~\ref{Lemma : moment bound U stat} with $h_{i,j} := H(W_i^{(1)},W_j^{(2)})$, we obtain
\begin{align*}
\mE\bigg|\sum_{i=1}^B \sum_{j=1}^B h_{i,j}\bigg|^p
\;\lesssim\; A_1 + A_2 + A_3 + A_4,
\end{align*}
where we omit multiplicative constants that do not depend on $B$. 
In deriving this inequality, we also used the elementary bound 
$\max\{a_1,\dots,a_m\} \leq a_1+\cdots+a_m$. The four terms are given by
\begin{align*}
\begin{alignedat}{2}
A_1 &= \Big(\sum_{i=1}^B \sum_{j=1}^B \mE[h_{i,j}^2]\Big)^{p/2}, 
&\qquad A_2 &= \mE_1\Big[ \max_{1\le i\le B} 
       \Big(\sum_{j=1}^B \mE_2[h_{i,j}^2]\Big)^{p/2}\Big], \\
A_3 &= \mE_2\Big[ \max_{1\le j\le B} 
       \Big(\sum_{i=1}^B \mE_1[h_{i,j}^2]\Big)^{p/2}\Big], 
&\qquad A_4 &= \mE\Big[\max_{1\le i,j\le B} |h_{i,j}|^p\Big].
\end{alignedat}
\end{align*}


We now bound each term:
\begin{enumerate}
\item \textbf{Term $A_1$.} By the i.i.d.\ structure,
\begin{align*}
    \sum_{i=1}^B \sum_{j=1}^B \mE[h_{ij}^2]
    = B^2 \,\mE[h_{12}^2],
\end{align*}
and therefore
\begin{align*}
    A_1 \;\lesssim\; B^p \cdot (\mE[h_{12}^2])^{p/2}
    \;\leq\; B^p \cdot \mE[|h_{12}|^p],
\end{align*}
where the last inequality follows from Jensen's inequality.
\item \textbf{Term $A_2$. and $A_{3}$} Replacing the maximum with a sum, for term $A_{2}$
\begin{align*}
\mE_1\bigg[ \max_{1\le i\le B} 
       \Big(\sum_{j=1}^B \mE_2 [h_{i,j}^2]\Big)^{p/2}\bigg]
&\leq \sum_{i=1}^B \mE_1 
   \Big(\sum_{j=1}^B \mE_2 [h_{12}^2]\Big)^{p/2}.
\end{align*}
By symmetry in $i$, this equals
\begin{align*}
B \cdot\mE_1\Big(\sum_{j=1}^B \mE_2 [h_{1j}^2]\Big)^{p/2}.
\end{align*}
Since $p>2$ and $x\mapsto x^{p/2}$ is convex, Jensen’s inequality yields
\begin{align*}
\Big(\sum_{j=1}^B \mE_2 [h_{1j}^2]\Big)^{p/2}
&\leq B^{p/2-1} \sum_{j=1}^B \big(\mE_2[h_{1j}^2]\big)^{p/2}.
\end{align*}
Each summand is identical by i.i.d.\ structure, hence
\begin{align*}
\mE_1\Big(\sum_{j=1}^B \mE_2 [h_{1j}^2]\Big)^{p/2}
&\leq B^{p/2} (\mE [h_{12}^2])^{p/2}.
\end{align*}
Therefore, again by Jensen's inequality
\begin{align*}
A_2 \;\leq\; B^{1+p/2}\cdot \mE [|h_{12}|^p].
\end{align*}
The same bound applies to $A_3$ by symmetry.

\item \textbf{Term $A_4$.} Again replacing the maximum with a sum yields
\begin{align*}
    \mE \Big[\max_{1\le i,j \le B} |h_{ij}|^p\Big]
    &\leq B^2 \,\mE |h_{12}|^p,
\end{align*}
thus
\begin{align*}
    A_4 \;\leq\; B^2 \,\mE|h_{12}|^p.
\end{align*}
\end{enumerate}

Combining the above estimates, we obtain
\begin{align*}
\mE\Bigg|\sum_{i=1}^B \sum_{j=1}^B h_{ij}\Bigg|^p 
\;\lesssim\; B^p \cdot\mE[|h_{12}|^p].
\end{align*}
Normalizing by $B^{-p}$, we conclude
\begin{align*}
\mE\big|T_{1,1}^{\mathrm{dec}}\big|^p 
\;\lesssim\; \mE[|h_{12}|^p].
\end{align*}
Finally, classical decoupling inequalities 
\citep[see][]{delapena1995decoupling,delapena1999decoupling}
ensure that the same bound applies to the original $U$-statistic $T_{1,1}$ 
(up to a universal constant which is independent of $B$). This completes the proof.

% The following lemma provides a Lipschitz-type concentration bound for the distribution function of an infinite weighted sum of independent chi-square variables.

\revised{Update the references using \texttt{cite} commands.}
% \begin{lemma}[\textcolor{blue}{Lee et al.\ 2025}, Lemma~1]
% \label{Esséen concentration inequality}
% Let $(V_\ell)_{\ell \ge 1}$ be i.i.d.\ standard Gaussian random variables, and suppose $\sum_{\ell=1}^\infty \lambda_\ell > 0$.  
% Define
% \begin{align*}
% H(x) := \mathbb{P}\!\left( \sum_{\ell=1}^\infty \lambda_\ell V_\ell^2 < x \right).
% \end{align*}
% Then $H$ satisfies the Lipschitz condition
% \begin{align*}
% \sup_{x \in \mathbb{R}} |H(x+\varepsilon) - H(x)|
% \leq 
% c \Bigg( \frac{\varepsilon}{\sqrt{\sum_{\ell=1}^\infty \lambda_\ell^2}} \wedge 1 \Bigg),
% \end{align*}
% for all $\varepsilon > 0$, where $c>0$ is an absolute constant and $a \wedge b := \min(a,b)$.
% \end{lemma}
 
The following lemma provides a Berry--Esseen type bound for degenerate U-statistics.
\begin{lemma}[\textcolor{blue}{Lee et al.\ 2025}, Theorem~1]
\label{lemma:degenerate_ustat_berryessen}
Let $X_1, \ldots, X_n$ be i.i.d.\ random variables with distribution $P$ on a measurable space $\mathcal{X}$. 
Consider a measurable and symmetric function $h:\mathcal{X}^2 \to \mathbb{R}$ satisfying
\begin{align*}
\mathbb{E}[h(x,X)]=0 \quad \text{for all } x\in\mathcal{X}.    
\end{align*}
Define the degenerate $U$-statistic
\begin{align*}
U_n = \frac{1}{n(n-1)}\sum_{1\le i\ne j\le n}h(X_i,X_j).    
\end{align*}
Let $Q$ denote the Hilbert--Schmidt operator associated with $h$, i.e.,
\begin{align*}
(Qg)(x)=\int h(x,y)g(y)\,dP(y),    
\end{align*}
and let $\{(\lambda_\ell,\phi_\ell):\ell\ge1\}$ be its eigenvalue–eigenfunction pairs, ordered such that $\lambda_1\ge\lambda_2\ge\cdots$.
Define
\begin{align*}
U_0=\sum_{\ell=1}^\infty \lambda_\ell(V_\ell^2-1),\qquad V_\ell\stackrel{\text{i.i.d.}}{\sim}N(0,1),  
\end{align*}
and let
\begin{align*}
\Delta_n=\sup_{t\in\mathbb{R}}
\big|
    \mathbb{P}(nU_n\le t)-\mathbb{P}(U_0\le t)
\big|.    
\end{align*}
Then there exists a universal constant $C>0$ such that
\begin{align*}
\Delta_n
\le
C\bigg(
n^{-1/3}
+
\bigg(
\frac{
    \mathbb{E}[h(X_1,X_2)^4]
}{
    n\,\mathbb{E}[h(X_1,X_2)^2]^2
}\bigg)^{1/8}\;
\bigg).    
\end{align*}
\end{lemma}
Building on Lemma~\ref{lemma:degenerate_ustat_berryessen}, we next recall the bootstrap result 
as established in {\textcolor{blue}{Lee et al.\ (2025), Theorem~2}}, 
which shows that the multiplier bootstrap provides a consistent approximation of the same non-Gaussian limit $U_0$.

\begin{lemma}[\textcolor{blue}{Lee et al.\ 2025}, Theorem~2] \label{lemma:multiplier_bootstrap_consistency}
Let $U_n^\ast$ denote the multiplier bootstrap statistic defined as
\begin{align*}
U_n^\ast = \frac{1}{n(n-1)}\sum_{1\le i\ne j\le n}\xi_i\xi_j h(X_i,X_j),
\quad
\xi_1,\ldots,\xi_n\stackrel{\text{i.i.d.}}{\sim}N(0,1),    
\end{align*}
independent of the data $\mathbb{X}_n:=(X_1,\ldots,X_n)$.
Let $U_0$ denote the non-Gaussian quadratic-form limit defined in Lemma~\ref{lemma:degenerate_ustat_berryessen}, 
and define the conditional Kolmogorov distance
\begin{align*}
\Delta_n^\ast
:=
\sup_{t\in\mathbb{R}}
\Big|
\mathbb{P}(nU_n^\ast\le t\mid\mathbb{X}_n)
-
\mathbb{P}(U_0\le t)
\Big|.    
\end{align*}
Assume the moment control condition
\begin{align*}
\frac{
\mathbb{E}[h(X_1,X_1)^2]
}{
\mathbb{E}[h(X_1,X_2)^4]^{1/2}
}
\le C
\quad\text{for some universal constant }C>0. 
\end{align*}
Then, for any $\varepsilon>0$, there exists a universal constant $C'>0$ such that
\begin{align*}
\mathbb{P}(\Delta_n^\ast\ge\varepsilon)
\leq
\frac{C'}{\varepsilon\,n^{1/6}}
\left(
\frac{
\mathbb{E}[h(X_1,X_2)^4]
}{
n\,\mathbb{E}[h(X_1,X_2)^2]^2
}\right)^{1/6}.    
\end{align*}
Consequently, if
\begin{align*}
\frac{
\mathbb{E}[h(X_1,X_2)^4]
}{
n\,\mathbb{E}[h(X_1,X_2)^2]^2
}\to 0
\quad\text{as }n\to\infty,    
\end{align*}
then the multiplier bootstrap is consistent, i.e.,
\begin{align*}
\lim_{n \to \infty} \mathbb{P}(\Delta_n^\ast > \varepsilon) = 0
\quad \text{for all } \varepsilon > 0.
\end{align*}
\end{lemma}

The following lemma is central to establishing the hardness result under the specified marginal distribution. While we adapt the argument to our setting, the main idea is based on \citet{neykov2021minimax}.

\begin{lemma} (\citealp[][Modified version of Lemma A.1]{neykov2021minimax}) \label{Lemma: hardness marginal fixed}
Suppose $(Z, Y, X) \in \{1,2\} \times \mathbb{R}^{d_{Y}+d_{X}}$ has a distribution supported either on $\{1,2\} \times [-M, M]^{d_{Y}+d_{X}}$ for some $M \in (0, \infty)$, or on $\{1,2\} \times (-\infty, \infty)^{d_{Y}+d_{X}}$. Let $\{ (Z_i, Y_i, X_i) \}_{i \in [n]}$ be i.i.d. copies of $(Z, Y, X)$, with $\mathbb{P}(Z = 1) = \lambda_n \in (0,1)$ and $\mathbb{P}(Z = 2) = 1 - \lambda_n$. Then, for any $\delta > 0$, there exists a constant $C := C(\delta) > 0$ such that for any $\varepsilon > 0$ and any Borel set
 \begin{align*}
D \subseteq (\{1,2\} \times \mathbb{R}^{d_{Y}+d_{X}})^n \times [0,1],
 \end{align*}
it is possible to construct an i.i.d. sequence $\{ (\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i) \}_{i \in [n]}$ satisfying $\widetilde{Z}_i \perp \widetilde{Y}_i \mid \widetilde{X}_i$ for all $i \in [n]$ and the following two properties.
\begin{enumerate}
    \item[(i)] First, with probability at least $1-\delta$, the modified sequence is close to the original sample in the sense that 
\begin{align*}
\mathbb{P} \Big( \max_{i \in [n]} \big\| (\widetilde{Y}_i, \widetilde{X}_i) - (Y_i, X_i) \big\|{\infty} < \varepsilon, \ \widetilde{Z}_i = Z_i \text{ for all } i \Big) > 1 - \delta.
\end{align*}
\item[(ii)] Second, if $U \sim \operatorname{Unif}[0,1]$ is independent of $\{ (\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i) \}_{i \in [n]}$, then the joint probability satisfies the inequality
\begin{align*}
\mathbb{P} \Big( \big( \{ (\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i) \}_{i \in [n]}, U \big) \in D \Big) \leq C \cdot \mu(D),
\end{align*}
where $\mu$ denotes the product of counting measure on $\{1,2\}^n$ and Lebesgue measure on $\mathbb{R}^{(d_{X}+d_{Y})n} \times [0,1]$.
\end{enumerate}
\end{lemma}
Proof of \Cref{Lemma: hardness marginal fixed}. To simplify the presentation, we focus on the case where $d_X = d_Y = 1$. The general case with higher dimensions can be handled similarly, requiring only minor adjustments. As in the proof of \cite{shah2020hardness}, it is enough to establish the following key lemma, which forms the core of their argument.
Most of the technical arguments and constructions in our proof are directly adapted from \cite{neykov2021minimax}. Our main modification lies in specifying the marginal distribution of $Z$, thereby restricting the null distribution class. However, this specification does not affect the construction of the lemma, and the core argument remains intact.
\paragraph{Step 1 (Preparation)}
First consider the case where the support is $\{1,2\} \times (-\infty, \infty)^2$, i.e., $M = \infty$. For any $\delta > 0$, one can select $M' := M'(\delta) < \infty$ such that
\begin{align*}
\mathbb{P} \big( \lVert (Y, X) \rVert_{\infty} > M' \big) < \frac{\delta}{2n}.
\end{align*}
Define the modified variables $(\bar{Z}, \bar{Y}, \bar{X})$ by setting $\bar{Z} := Z$ always, and setting $(\bar{Y}, \bar{X}) := (Y, X)$ when $\lVert (Y, X) \rVert_{\infty} \leq M'$, while replacing $(\bar{Y}, \bar{X})$ by an independent sample drawn uniformly from $[-M', M']^2$ otherwise. By the union bound,
\begin{align*}
\mathbb{P}\big( \forall i \in[n] : (\bar{Z}_i, \bar{Y}_i, \bar{X}_i) = (Z_i, Y_i, X_i) \big) > 1 - \frac{\delta}{2}.
\end{align*}
Henceforth, we work with $\{ (\bar{Z}_i, \bar{Y}_i, \bar{X}_i) \}_{i \in [n]}$ (denoted by $\{ (Z_i, Y_i, X_i) \}_{i \in [n]}$ for convenience), and we will construct $\{ (\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i) \}_{i \in [n]}$ satisfying
\begin{align*}
\mathbb{P} \Big( \max_{i \in [n]} \big\| (\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i) - (\bar{Z}_i, \bar{Y}_i, \bar{X}_i) \big\|_{\infty} < \varepsilon \Big) = 1.
\end{align*}
Next, we assume that the conditional densities $p_{Y, X \mid Z=z}(y, x)$ are bounded by some constant $L := L(\delta)$, uniformly over $z \in \{1,2\}$. For any $\bar{L}>0$, define
\begin{align*}
S_{\bar{L}} := \big\{(z, y, x) \mid p_{Y, X \mid Z=z}(y, x) > \bar{L} \big\}.
\end{align*}
As $\bar{L} \to \infty$, $S_{\bar{L}} \downarrow \varnothing$. Hence, for any given $\delta$, we can choose $\bar{L}(\delta)$ large enough so that
\begin{align*}
\mathbb{P}\Big( (Z, Y, X) \in S_{\bar{L}(\delta)}^c \Big) > 1 - \frac{\delta}{2n}.
\end{align*}
Construct $(\bar{Z}, \bar{Y}, \bar{X})$ by setting $\bar{Z} := z$, and $(\bar{Y}, \bar{X}) := (Y, X)$ if $(Z, Y, X) \in S_{\bar{L}(\delta)}^c$, while drawing $(\bar{Y}, \bar{X})$ uniformly from $[-M, M]^2$ otherwise. Then the resulting conditional densities are bounded by
\begin{align*}
L(\delta) := \bar{L}(\delta) + \frac{\delta}{2n(2M)^2}.
\end{align*}
Therefore, we again have
\begin{align*}
\mathbb{P} \big( \forall i \in [n] : (\bar{Z}_i, \bar{Y}_i, \bar{X}_i) = (Z_i, Y_i, X_i) \big) > 1 - \frac{\delta}{2}.
\end{align*}
\paragraph{Step 2 (Construction)}
Let $\{A_1, A_2\}=\{\{1\},\{2\}\}$ denote the (trivial) partition of $\{1,2\}$ corresponding to the value of $Z$. Similarly, let $\{B_1, \ldots, B_m\}$ and $\{C_1, \ldots, C_m\}$ be equi-partitions of $[-M, M]$ into intervals of length $2 M / m$. Divide each $C_k$ further into $m^2$ sub-intervals of equal length, denoted by $C_{i j k}$, so that each small interval corresponds to a pair $\big(A_i, B_j\big)$, with $i \in\{1,2\}, j \in[m]$.
Given a draw $(Z, Y, X)$, we construct $(\widetilde{Z}, \widetilde{Y}, \widetilde{X})$ as follows. Suppose that $X \in A_i, Y \in B_j$, and $Z \in C_k$. Then we set $\widetilde{Z}:=Z$, generate $\widetilde{X}$ uniformly in $C_{i j k,}$ and generate $\widetilde{Y}$ uniformly in $B_j$. By construction, this guarantees $\widetilde{Z} \perp \widetilde{Y} \mid \widetilde{X}$. Moreover, it is clear that by construction
\begin{align*}
\mathbb{P} \Big( \max_{i \in [n]} \big\| (\widetilde{Y}_i, \widetilde{X}_i) - (Y_i, Z_i) \big\|_\infty < \frac{2M}{m}, \ \widetilde{Z}_i = Z_i \ \text{for all } i \Big) = 1.
\end{align*}
Hence if we take $m$ large enough so that $\frac{2M}{m}<\varepsilon$ we guarantee that $(i)$ is satisfied. What is more may write out the density of $(\tilde{Z},\tilde{Y},\tilde{X})$ as 
\begin{align*}
p_{\widetilde{Y}, \widetilde{X} \mid \widetilde{Z}=i}(\widetilde{y}, \widetilde{x}) 
= \sum_{j,k} \frac{m^3}{(2M)^2} \mathds{1}(\widetilde{y} \in B_j, \widetilde{x} \in C_{ijk}) \cdot \mathbb{P}(Y \in B_j, X \in C_k \mid Z=i).
\end{align*}
\paragraph{Step 3 (showing part $(ii)$)}
Recall that we are assuming that the conditional density satisfies $p_{Y, Z \mid X=x}(y, z) \leq L$ for some constant $L>0$. It is simple to see that the probability that $(X, Y) \in A_i \times B_j$ is bounded as
\begin{align*}
\sum_{z \in\{1,2\}} \int_{B_j \times[-M, M]} p_{Y, X \mid X=x}(y, x) \cdot p_z(z) d y d z \leq \frac{L \lambda_n(2 M)^2}{m}+\frac{L\big(1-\lambda_n\big)(2 M)^2}{m}=\frac{L(2 M)^2}{m}.    
\end{align*}
It follows that if we have $n$ observations $\big\{\big(Z_i, Y_i, X_i\big)\big\}_{i \in[n]}$, the probability that at least two points $Y_k$ and $Y_{\ell}$ fall into the same interval $B_j$, for some $j \in[m]$, is bounded by
\begin{align*}
\bigg(\frac{L(2 M)^2}{m}\bigg)^n\big[m^n-m(m-1) \cdots(m-n+1)\big]=\mathcal{O}\bigg(\frac{n^2\big(L(2 M)^2\big)^n}{m}\bigg).     
\end{align*}
Since the number of all possible arrangements with points falling into different intervals $B_j$ is $m(m- 1) \cdots(m-n+1)$, while the total number of possible arrangements for the $n$ points is $m^n$. Denote by $S$ the complement of this event. Note that when $S$ occurs, all $Y_i$ fall into different intervals $B_j$, and vice versa.
Next, suppose that $D$ is an arbitrary fixed Borel set. We have
\begin{align*}
\mathbb{P}\Big(\Big(\big(\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i\big)_{i \in[n]}, U\Big) \in D\Big) & \leq \mathbb{P}\Big(\Big(\big(\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i\big)_{i \in[n]}, U\Big) \in D \cap(S \times[0,1])\Big) \\& \quad+\mathbb{P}\Big(\Big(\big(\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i\big)_{i \in[n]}, U\Big) \notin S \times[0,1]\Big)
\end{align*}
We already have a bound on the second term on the RHS above
\begin{align*}
\mathbb{P}\Big(\Big(\big(\widetilde{Z}_i, \widetilde{Y}_i, \widetilde{X}_i\big)_{i \in[n]}, U\Big) \notin S \times[0,1]\Big)=\mathcal{O}\bigg(\frac{n^2\big(L(2 M)^2\big)^n}{m}\bigg).    
\end{align*}
Suppose now that we randomize the assignment on the set $C_{j k}$. In other words, there is a permutation $\pi:[m] \mapsto[m]$ that assigns each interval $B_j$ to a sub-interval $C_{\pi_j k}$. Denote by $\big(\widetilde{X}^\pi, \widetilde{Y}^\pi, \widetilde{Z}^\pi\big)$ the vectors generated in this manner. Clearly all properties described above hold for $\big\{\big(\widetilde{Z}_i^\pi, \widetilde{Y}_i^\pi, \widetilde{X}_i^\pi\big)\big\}_{i \in[n]}$ for any permutation $\pi$. We have that
\begin{align*}
& \frac{1}{m!} \sum_{\pi \in S_m} \mathbb{P}\Big(\big(\big\{\big(\widetilde{Z}_i^\pi, \widetilde{Y}_i^\pi, \widetilde{X}_i^\pi\big)\big\}_{i \in[n]}, U\big) \in D \cap(S \times[0,1])\Big) \\
& \quad=\frac{1}{m!} \sum_{\pi \in S_m} \int{D \cap(S \times[0,1)]} \prod_{l \in[n]} \sum_{x_l \in\{1,2\}} \sum_{j_l, k_l} \frac{m^3}{(2 M)^2} \mathds{1}\big(\widetilde{z}_l=z_l, \widetilde{y}_l \in B_{j_l}, \widetilde{x}_l \in C_{\pi{j_l} k_l}\big) \\
& \quad\quad\; \times \mathbb{P}\big(Z=z_l, Y \in B_{j_l}, X \in C_{k_l}\big) \\
& \quad \leq \frac{(L m^2)^n}{m!} \sum_{\pi \in S_m} \int_{D \cap(S \times[0,1])} \sum_{\big\{x_l\big\},\big\{j_l\big\},\big\{k_l\big\}} \mathds{1}\big(\widetilde{Z}_l^\pi=x_l, \widetilde{Y}_l^\pi \in B_{j_l}, \widetilde{X}_l^\pi \in C_{\pi_{j_l} k_l}\big).
\end{align*}
In the above summation, the sequences $\big\{j_l\big\}_{l \in[n]},\big\{k_l\big\}_{l \in[n]}$ are sequences of $n$ numbers from $[m]$. Since the integration is over the set $D \cap(S \times[0,1])$, all indices $j_l$ must be distinct; otherwise, the integral is zero. Thus the summation is effectively over all sequences $\big\{j_l\big\}_{l \in[n]},\big\{k_l\big\}_{l \in[n]}$ such that no two indices $j_l, j_l$ are equal.
Fixing $\pi_{j l}$ for these $n$ distinct values and permuting the remaining $m-n$ indices, the number of permutations is $(m-n)!$. Since
\begin{align*}
 \prod_{l \in[n]} C_{k_l}=\prod_{l \in[n]} \sum_j C_{j k_l},   
\end{align*}
contains all unique permutations of $n$ elements (and more), we obtain the following bound:
\begin{align*}
& \frac{(L m^2)^n}{m!} \sum_{j_l, k_l} \int_{D \cap(S \times[0,1])} \sum_{\pi \in S_m} \prod_{l \in[n]} \mathds{1}\big(\widetilde{Y}_l^\pi \in B_{j_l}, \widetilde{X}_l^\pi \in C_{\pi_{j_l} k_l}\big) \\
& \leq \frac{(L m^2)^n(m-n)!}{m!} \sum_{j_l, k_l} \int_{D \cap(S \times[0,1])} \prod_{l \in[n]} \mathds{1}\big(\widetilde{Y}_l^\pi \in B_{j_l}, \widetilde{X}_l^\pi \in C_{k_l}\big) \\
& \leq \frac{(L m^2)^n(m-n)!}{m!} \mu(D \cap(S \times[0,1])) \leq \frac{(L m^2)^n(m-n)!}{m!} \mu(D).
\end{align*}
Therefore, there exists a permutation $\pi^*$ such that
\begin{align*}
\mathbb{P}\Big(\big(\big\{\big(\widetilde{Z}_i^{\pi^*}, \widetilde{Y}_i^{\pi^*}, \widetilde{X}_i^{\pi^*}\big)\big\}{i \in[n]}, U\big) \in D \cap(S \times[0,1])\Big) \leq \frac{(L m^2)^n(m-n)!}{m!} \mu(D) .   
\end{align*}
Finally, taking $m$ sufficiently large, we obtain
\begin{align*}
\mathbb{P}\big(\big(\big\{\big(\widetilde{X}_i^{\pi^*}, \widetilde{Y}_i^{\pi^*}, \widetilde{Z}_i^{\pi^*}\big)\big\}_{i \in[n]}, U\big) \in D\big) \leq \frac{(L m^2)^n(m-n)!}{m!} \mu(D)+\mathcal{O}\big(\frac{n^{2}\big(L(2 M)^2\big)^n}{m}\big) \leq C \mu(D).    
\end{align*}

\section{Additional Numerical Experiments} \label{section: Additional Experiments}
In this section, we present the experimental details and additional numerical experiments not included in the main body of the paper. Specifically, we describe the experimental details in \Cref{Section: Experimental Details} and provide additional simulation results, including an empirical analysis of the impact of density ratio estimation errors on DRT methods in \Cref{Appendix : Density Ratio Estimation Error Analysis}, a real data analysis for CIT methods in \Cref{Appendix: CIT Real Data Results}, a sensitivity analysis of the application of \Cref{Algorithm: Converting C2ST into CIT} to the CIT approach in \Cref{Appendix: With_Without_Algorithm_1}, and a sensitivity analysis of \Cref{Algorithm: Converting C2ST into CIT} to adjustment parameter $\varepsilon$ in \Cref{Appendix: Sensitivity Analysis by epsilon}.


\subsection{Experimental Details} \label{Section: Experimental Details}

We begin with the implementation details of our numerical experiments, including density ratio estimation techniques, linear-time MMD test, classifier-based test, and conditional independence testing approaches. 

\paragraph{Density Ratio Estimation.} We estimate the density ratio $r_X(x)$ defined in \eqref{Eq: density ratio} using a probabilistic classification-based approach described in \citet[Section 3]{sugiyama2010density}. Specifically, we focus on two classifiers: linear logistic regression (\revised{LLR}) and kernel logistic regression (KLR). 
\medskip

\noindent Given samples $\{(X_i^{(1)},Y_i^{(1)})\}_{i=1}^{n_1} \iid P_{XY}^{(1)}$ and $\{(X_j^{(2)},Y_j^{(2)})\}_{j=1}^{n_2} \iid P_{XY}^{(2)}$, consider $\{(X_i, \ell_i)\}_{i=1}^{n}$, where $(X_1, \ldots, X_n) = (X_1^{(1)}, \ldots, X_{n_1}^{(1)},X_1^{(2)}, \ldots, X_{n_2}^{(2)})$ and $(\ell_1,\ldots,\ell_{n_1},\ell_{n_1+1},\ldots,\ell_{n}) = (0,\ldots,0,1,\ldots,1)$ with $\ell_i = \mathds{1}(i \geq n_1 +1)$. Further denote $X_i=\big(X_i(1), \ldots, X_i(p)\big)^\top$ where $p$ is the dimension of $X_i$ and let  $\boldsymbol{\beta} \coloneqq (\beta_0, \beta_1, \ldots, \beta_p)^\top$.


\begin{itemize}
	\item For \revised{LLR} method, we model the posterior probability as
	\begin{align*}
		\eta(X_i; \boldsymbol{\beta}) = \mP(\ell = 1 \given X_i) = \frac{1}{1 + \exp{(-\beta_0 + \sum_{j=1}^{p}\beta_{j}X_i(j))}}.
	\end{align*}

    The estimated coefficients $\hat{\boldsymbol{\beta}}$ are obtained by minimizing the negative log-likelihood. 
	\item For KLR method \citep{zhu2005kernel}, we use $\eta(X_i; \boldsymbol{\beta}) = 1/\left(1 + \exp{(-\theta(X_i; \boldsymbol{\beta}))}\right),$ where $\theta(X_i;\boldsymbol{\beta}) = \beta_0 + \sum_{j=1}^{p} \beta_j k(X_i(j), x)$ and $k(x, y) = \exp(-\|x - y\|^2 / \sigma^2)$. The estimated coefficients $\hat{\boldsymbol{\beta}}$ are obtained by minimizing the following penalized negative log-likelihood:
	\begin{align*}
		- \sum_{i=1}^n \left[\ell_i \theta(X_i;\boldsymbol{\beta}) - \log{\left(1 + \exp\left(\theta(X_i;\boldsymbol{\beta})\right)\right)}\right] + \frac{\lambda}{2}\|\theta\|^2_{\mathcal{H}_k},
	\end{align*}
	where $\mathcal{H}_k$ is the reproducing kernel Hilbert space generated by $k$ and $\lambda$ is a regularization parameter.
\end{itemize}

\noindent The density ratio estimate is then:
\begin{align}
    \hat{r}_X(X_i) = \frac{n_2}{n_1} \cdot \frac{{\eta}(X_i;\hat{\boldsymbol{\beta}})}{1 - {\eta}(X_i;\hat{\boldsymbol{\beta}})}.
\end{align}

\noindent For the joint density ratio, we use $(X_i, Y_i)$ instead of $X_i$ alone. We set $\sigma^2 = 200$, following \citet{hu2024two}, and fix $\lambda = 0.0005$ throughout our simulations. 

\paragraph{Linear-Time MMD Test.} For the linear-time MMD tests, we use a Gaussian kernel with the bandwidth parameter fixed at $1$ across all experiments. In the cross-validated version ($^\dagger$MMD-$\ell$), we use 2-fold cross validation (i.e., $K = 2$) with an equal splitting ratio.

\paragraph{Classifier-based Test.} As mentioned in \Cref{Section: Classifier-based Approach}, under the balanced-sample setting, the Bayes optimal classifier is defined as:

\begin{equation*}
h^*(y,x) \coloneqq  \mathds{1}\left(\frac{f_{YX}^{(1)}(y,x)}{f_{YX}^{(1)}(y,x) + f_{YX}(y,x)} > \frac{1}{2}\right).
\end{equation*}

\noindent This classifier can be equivalently expressed using density ratios:  

\begin{equation*}
    h^\star(y, x) = \mathds{1}\left(\frac{r_X(x)}{r_X(x) + r_{YX}(y,x)} > \frac{1}{2}\right),
\end{equation*}
where $r_X(x) = f_X^{(1)}(x) / f_X^{(2)}(x)$ as in \eqref{Eq: density ratio} and $r_{YX}(y,x) \coloneqq f_{YX}^{(1)}(y,x) / f_{YX}^{(2)}(y,x)$. The empirical classifier is then defined as a plug-in estimator of $h^\star$: 

\begin{equation} \label{Eq: Empirical Classifier}
\hat{h}(y, x) \coloneqq  \mathds{1}\left(\frac{\hat{r}_X(x)}{\hat{r}_X(x) + \hat{r}_{YX}(y,x)} > \frac{1}{2}\right),
\end{equation} 
where $\hat{r}_X$ and $\hat{r}_{YX}$ are the estimated marginal and joint density ratios, respectively,  obtained using the classification-based approach described above.

The classifier $\hat{h}$ is constructed based on the training set, whereas the testing set is further split into two subsets with a ratio of 8:2. A larger subset is used for density ratio estimation, and the other subset is used for calculating the test statistic. In the cross-validated version ($^\dagger$CLF), we use 2-fold cross-validation as in $^\dagger$MMD-$\ell$, maintaining 8:2 splitting ratio within each fold. 

\paragraph{Randomized Conditional Independence Test.} The RCIT method is implemented using the default hyperparameter settings specified in \citet{strobl2019approximate}. Specifically, we use the default approximation method (Lindsay–Pilla–Basak method) for the null distribution. The number of random Fourier features is set to $100$ for the conditioning set, and $5$ for the non-conditioning sets.

\paragraph{Regression Methods for CIT.} In our implementation of CIT approaches, we utilize several standard regression techniques. \Cref{tab : CIT regression method} below provides an overview of the key methods and their corresponding hyperparameter settings.

\begin{table}[h!]
\centering
\caption{Description of regression methods used in CIT approach.}
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{cccc}
\toprule
\textbf{Regression Method} & \textbf{R Implementation} & \textbf{Tuning Parameters} & \textbf{Description} \\
\midrule
Random Forests & \texttt{ranger} & $mtry = \sqrt{p}$ & $\#$ of variables to split at each node \\
\midrule
\multirow{2}{*}{XGBoost} & \multirow{2}{*}{\texttt{xgboost}} & $\max$ depth $= 6$ & maximum tree depth \\
 &  & $\eta = 0.3$ & learning rate \\
\bottomrule
\end{tabular}
}
\label{tab : CIT regression method}
\end{table}


\noindent The code for reproducing all of our simulation results (including those in \Cref{Appendix : Density Ratio Estimation Error Analysis,Appendix: CIT Real Data Results,Appendix: With_Without_Algorithm_1,Appendix: Sensitivity Analysis by epsilon}) and for more detailed settings is available on GitHub: \url{https://github.com/suman-cha/Cond2ST}.

\subsection{Impact of Density Ratio Estimation Errors on DRT Methods}\label{Appendix : Density Ratio Estimation Error Analysis}

To complement the real data analysis presented in \Cref{Section: Real Data Analysis}, we conduct experiments to examine the relationship between density ratio estimation errors and type I errors of DRT methods. Figure \ref{fig:density_ratio_mse} illustrates the log-scaled mean squared error (MSE) of the marginal density ratio $r_X$ and the conditional density ratio $r_{Y|X}$ estimates for both the \revised{LLR} and KLR methods across various sample sizes. Our experimental setup involves 500 simulations for each combination of sample size, dataset, and estimation method. We report the median MSE to provide a robust measure of estimation accuracy. For better visualization, the log-scaled MSE values are clipped: marginal density ratio errors above $10$ are capped at $10$ and conditional density ratio errors are limited to a maximum of $1$. Notably, the true error values for the \revised{LLR} method in high-dimensional settings, significantly exceed these clipped limits.

In the low-dimensional diamonds dataset, both \revised{LLR} and KLR methods show relatively low MSE values for both marginal and conditional density ratio estimation. The performance gap between \revised{LLR} and KLR methods diminishes as the sample size increases. This observation aligns with the findings in \Cref{Section: Real Data Analysis}, where simpler methods like \revised{LLR} suffice to control the type I error in low-dimensional settings. The low estimation errors explain their similar performance in such scenarios.

In contrast, the high-dimensional superconductivity dataset shows significant differences between the methods. KLR consistently outperforms \revised{LLR} in both marginal and conditional density ratio estimation, maintaining low and stable MSE values across all sample sizes. On the other hand, \revised{LLR} shows extremely high MSE values, particularly for small sample sizes. Although \revised{LLR} shows some improvement as the sample size increases, it remains inferior to KLR in terms of estimation accuracy.

The high estimation errors for \revised{LLR} in high-dimensional settings, even beyond the clipping applied, account for poor type I error control observed in \Cref{fig:real DRT}. These results highlight the need for more advanced density ratio estimation techniques to ensure the validity of tests in complex and high-dimensional scenarios.



\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/Error_ll_klr_1x4.pdf}
      \caption{Log-scaled mean squared errors of marginal density ratio $r_X(x)$ (\emph{left}) and conditional density ratio $r_{Y|X}(y|x)$ (\emph{right}) estimates for \revised{LLR} and KLR methods across various sample sizes. Results are shown for diamonds and superconductivity datasets, based on median values from 500 simulations under the null hypothesis.} \label{fig:density_ratio_mse}
\end{figure}

\subsection{Real Data Analysis for CIT Methods}\label{Appendix: CIT Real Data Results}

We present the results for CIT methods applied to the diamonds and superconductivity datasets, complementing the analysis discussed in \Cref{Section: Real Data Analysis}. Figure \ref{fig:real CIT} shows the rejection rates for these methods under both the null and alternative hypotheses across various sample sizes. 


For the low-dimensional diamonds dataset, the CIT methods generally exhibit good type I error control, with rejection rates close to the significance level $\alpha = 0.05$ under the null hypothesis. Under the alternative hypothesis, we observe increasing power for all methods except for WGSC as the sample size grows. Notably, RCIT and GCM show superior performance in terms of power.


In the high-dimensional superconductivity dataset, the performance of CIT methods is similar to that observed in the diamonds dataset, with no significant differences compared to DRT methods as shown in Figure \ref{fig:real DRT}. In terms of type I error control, GCM exhibits increasing rejection rates under the null hypothesis as the sample size grows. RCIT shows more inflated type I error, especially at small sample size. Regarding power, the CIT methods demonstrate relatively consistent performance across both datasets.
\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{Figures/Real_CIT_line.pdf}
    \caption{Rejection rates of CIT methods on the diamonds and superconductivity datasets under null and alternative hypotheses. Results are averaged over 500 repetitions with significance level $\alpha = 0.05$.}
    \label{fig:real CIT}
\end{figure}


\subsection{Sensitivity Analysis of CIT Methods to \Cref{Algorithm: Converting C2ST into CIT}} \label{Appendix: With_Without_Algorithm_1}


We examine the impact of \Cref{Algorithm: Converting C2ST into CIT} on CIT methods across scenarios with unbounded marginal density ratios, as outlined in \Cref{Section: Synthetic Data Examples}. Our analysis encompasses RCIT and the regression-based methods; PCM, GCM, and WGSC. The latter three are implemented using various regression models, such as linear models~(\texttt{lm}), Random Forests~(\texttt{rf}), and XGBoost~(\texttt{xgb}). \Cref{tab:scenario1u_performance_by_alg1,tab:scenario2u_performance_by_alg1,tab:scenario3u_performance_by_alg1} show the results for Scenarios 1(U), 2(U), and 3(U), respectively. In these tables, a checkmark ($\checkmark$) indicates that \Cref{Algorithm: Converting C2ST into CIT} is applied, whereas a cross ($\times$) indicates it is not.


RCIT exhibits significant sensitivity to the application of \Cref{Algorithm: Converting C2ST into CIT}, particularly in Scenario 1(U). In this case, without the algorithm, the type I error rates of RCIT increase with sample size, whereas with the algorithm, these rates decrease as the sample size grows. This behavior highlights the potential stabilizing effect of \Cref{Algorithm: Converting C2ST into CIT} on the performance of RCIT. On the other hand, GCM shows consistent performance across different regression methods, suggesting the robustness to the choice of underlying regression models. In contrast, the performance of PCM varies significantly with the choice of regression method. WGSC shows inconsistent patterns across scenarios, indicating potential sensitivity to specific data properties or model assumptions. This variability underscores the need for careful consideration when applying WGSC for conditional two-sample testing.



\subsection{\texorpdfstring{Sensitivity Analysis of \Cref{Algorithm: Converting C2ST into CIT} to $\varepsilon$}{Sensitivity Analysis of Algorithm: Converting C2ST into CIT to epsilon}}
\label{Appendix: Sensitivity Analysis by epsilon}

We conduct a sensitivity analysis of \Cref{Algorithm: Converting C2ST into CIT} with respect to the adjustment parameter $\varepsilon$. This parameter determines the size of the constructed testing dataset $\mathcal{D}_{\tilde{n}}$, through the equation $\tilde{n} = kn$, where $k$ is a function of $\varepsilon$, as defined in \Cref{Algorithm: Converting C2ST into CIT}. The goal of this section is to examine the impact of different $\varepsilon$ values on the performance of conditional two-sample testing via CIT methods. We consider 3 candidates for $\varepsilon$: $\bigl\{1/n, 1/\log(n), 1/\sqrt{\log(n)}\bigr\}$. The analysis covers scenarios with unbounded marginal density ratios, as described in \Cref{Section: Synthetic Data Examples}, examining both null and alternative hypotheses across different sample sizes. The settings of these experiments correspond to the settings detailed in \Cref{Section: Numerical Experiments} with $n \in \{200,500,1000\}$.

\Cref{tab:scenario1u_varepsilon,tab:scenario2u_varepsilon,tab:scenario3u_varepsilon} present the results for each scenario, comparing the performance of RCIT, PCM, GCM, and WGSC across different $\varepsilon$ values and sample sizes. Although the performance of the CIT methods remains relatively stable across different choices of $\varepsilon$, some variations in rejection rates are observed, particularly for small sample sizes.


\begin{table}[htbp]
\centering
\caption{Performance comparison of CIT methods for Scenario 1(U) under null and alternative hypotheses. Rejection rates are provided for RCIT and regression-based methods (PCM, GCM, WGSC), implemented using \texttt{lm}, \texttt{rf}, and \texttt{xgb}. Results are shown for various sample sizes, with and without \Cref{Algorithm: Converting C2ST into CIT}.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
\toprule[1.5pt]
\multirow{2}{*}{$n$} & \multirow{2}{*}{Hypothesis} & \multirow{2}{*}{\Cref{Algorithm: Converting C2ST into CIT}} & \multirow{2}{*}{RCIT} & \multicolumn{3}{c}{PCM} & \multicolumn{3}{c}{GCM} & \multicolumn{3}{c}{WGSC} \\
\cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
& & & & \texttt{lm} & \texttt{rf} & \texttt{xgb} & \texttt{lm} & \texttt{rf} & \texttt{xgb} & \texttt{lm} & \texttt{rf} & \texttt{xgb} \\
\midrule
\multirow{4}{*}{200} & \multirow{2}{*}{Null} & $\checkmark$ & 0.166 & 0.034 & 0.060 & 0.104 & 0.048 & 0.028 & 0.082 & 0.000 & 0.278 & 0.076 \\
& & $\times$ & 0.202 & 0.044 & 0.064 & 0.098 & 0.032 & 0.036 & 0.074 & 0.000 & 0.062 & 0.052 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.240 & 0.038 & 0.068 & 0.102 & 0.164 & 0.076 & 0.144 & 0.000 & 0.284 & 0.082 \\
& & $\times$ & 0.362 & 0.046 & 0.078 & 0.104 & 0.300 & 0.178 & 0.268 & 0.000 & 0.082 & 0.074 \\
\midrule
\multirow{4}{*}{500} & \multirow{2}{*}{Null} & $\checkmark$ & 0.056 & 0.066 & 0.066 & 0.074 & 0.036 & 0.024 & 0.060 & 0.000 & 0.248 & 0.050 \\
& & $\times$ & 0.210 & 0.040 & 0.060 & 0.078 & 0.046 & 0.050 & 0.064 & 0.000 & 0.070 & 0.076 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.228 & 0.070 & 0.092 & 0.100 & 0.356 & 0.170 & 0.296 & 0.000 & 0.252 & 0.048 \\
& & $\times$ & 0.536 & 0.038 & 0.138 & 0.126 & 0.552 & 0.444 & 0.602 & 0.000 & 0.086 & 0.078 \\
\midrule
\multirow{4}{*}{1000} & \multirow{2}{*}{Null} & $\checkmark$ & 0.064 & 0.050 & 0.050 & 0.060 & 0.048 & 0.024 & 0.060 & 0.000 & 0.270 & 0.056 \\
& & $\times$ & 0.296 & 0.056 & 0.038 & 0.068 & 0.044 & 0.062 & 0.058 & 0.000 & 0.068 & 0.026 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.394 & 0.050 & 0.098 & 0.124 & 0.586 & 0.262 & 0.516 & 0.000 & 0.268 & 0.054 \\
& & $\times$ & 0.762 & 0.054 & 0.178 & 0.182 & 0.764 & 0.650 & 0.858 & 0.000 & 0.054 & 0.044 \\
\midrule
\multirow{4}{*}{2000} & \multirow{2}{*}{Null} & $\checkmark$ & 0.048 & 0.054 & 0.030 & 0.056 & 0.046 & 0.028 & 0.062 & 0.000 & 0.282 & 0.016 \\
& & $\times$ & 0.500 & 0.036 & 0.042 & 0.056 & 0.048 & 0.054 & 0.048 & 0.000 & 0.038 & 0.022 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.610 & 0.050 & 0.112 & 0.110 & 0.796 & 0.502 & 0.820 & 0.000 & 0.282 & 0.024 \\
& & $\times$ & 0.914 & 0.036 & 0.292 & 0.272 & 0.890 & 0.812 & 0.998 & 0.000 & 0.086 & 0.028 \\
\midrule[1.5pt]
\end{tabular}
}
\label{tab:scenario1u_performance_by_alg1}
\end{table}



\begin{table}
\centering
\caption{Performance comparison of CIT methods for Scenario 2(U) under null and alternative hypotheses. Rejection rates are provided for RCIT and regression-based methods (PCM, GCM, WGSC), implemented using \texttt{lm}, \texttt{rf}, and \texttt{xgb}. Results are shown for various sample sizes, with and without \Cref{Algorithm: Converting C2ST into CIT}.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
\toprule[1.5pt]
\multirow{2}{*}{$n$} & \multirow{2}{*}{Hypothesis} & \multirow{2}{*}{\Cref{Algorithm: Converting C2ST into CIT}} & \multirow{2}{*}{RCIT} & \multicolumn{3}{c}{PCM} & \multicolumn{3}{c}{GCM} & \multicolumn{3}{c}{WGSC} \\
\cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
& & & & \texttt{lm} & \texttt{rf} & \texttt{xgb} & \texttt{lm} & \texttt{rf} & \texttt{xgb} & \texttt{lm} & \texttt{rf} & \texttt{xgb} \\
\midrule
\multirow{4}{*}{200} & \multirow{2}{*}{Null} & $\checkmark$ & 0.162 & 0.036 & 0.074 & 0.098 & 0.072 & 0.044 & 0.084 & 0.000 & 0.256 & 0.060 \\
& & $\times$ & 0.116 & 0.046 & 0.048 & 0.090 & 0.048 & 0.058 & 0.110 & 0.000 & 0.056 & 0.040 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.730 & 0.038 & 0.466 & 0.188 & 0.064 & 0.044 & 0.104 & 0.000 & 0.284 & 0.192 \\
& & $\times$ & 0.808 & 0.050 & 0.504 & 0.254 & 0.054 & 0.050 & 0.110 & 0.000 & 0.200 & 0.174 \\
\midrule
\multirow{4}{*}{500} & \multirow{2}{*}{Null} & $\checkmark$ & 0.074 & 0.056 & 0.060 & 0.104 & 0.044 & 0.046 & 0.094 & 0.000 & 0.270 & 0.042 \\
& & $\times$ & 0.094 & 0.042 & 0.042 & 0.076 & 0.060 & 0.056 & 0.072 & 0.000 & 0.058 & 0.056 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.906 & 0.056 & 0.864 & 0.564 & 0.054 & 0.050 & 0.092 & 0.000 & 0.320 & 0.330 \\
& & $\times$ & 0.880 & 0.040 & 0.924 & 0.702 & 0.044 & 0.048 & 0.074 & 0.000 & 0.364 & 0.494 \\
\midrule
\multirow{4}{*}{1000} & \multirow{2}{*}{Null} & $\checkmark$ & 0.086 & 0.046 & 0.032 & 0.070 & 0.048 & 0.020 & 0.056 & 0.000 & 0.272 & 0.030 \\
& & $\times$ & 0.070 & 0.048 & 0.040 & 0.094 & 0.048 & 0.044 & 0.094 & 0.000 & 0.046 & 0.040 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.968 & 0.046 & 0.990 & 0.974 & 0.052 & 0.026 & 0.066 & 0.000 & 0.398 & 0.604 \\
& & $\times$ & 0.954 & 0.048 & 1.000 & 0.998 & 0.040 & 0.048 & 0.080 & 0.000 & 0.724 & 0.832 \\
\midrule
\multirow{4}{*}{2000} & \multirow{2}{*}{Null} & $\checkmark$ & 0.048 & 0.036 & 0.030 & 0.064 & 0.032 & 0.018 & 0.048 & 0.000 & 0.272 & 0.030 \\
& & $\times$ & 0.076 & 0.040 & 0.036 & 0.070 & 0.042 & 0.046 & 0.072 & 0.000 & 0.046 & 0.020 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.980 & 0.034 & 1.000 & 1.000 & 0.030 & 0.020 & 0.044 & 0.000 & 0.486 & 0.900 \\
& & $\times$ & 0.980 & 0.042 & 1.000 & 1.000 & 0.040 & 0.026 & 0.066 & 0.000 & 0.960 & 0.994 \\
\midrule[1.5pt]
\end{tabular}
}
\label{tab:scenario2u_performance_by_alg1}
\end{table}
 


\begin{table}
\centering
\caption{Performance comparison of CIT methods for Scenario 3(U) under null and alternative hypotheses. Rejection rates are provided for RCIT and regression-based methods (PCM, GCM, WGSC), implemented using \texttt{lm}, \texttt{rf}, and \texttt{xgb}. Results are shown for various sample sizes, with and without \Cref{Algorithm: Converting C2ST into CIT}.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
\toprule[1.5pt]
\multirow{2}{*}{$n$} & \multirow{2}{*}{Hypothesis} & \multirow{2}{*}{\Cref{Algorithm: Converting C2ST into CIT}} & \multirow{2}{*}{RCIT} & \multicolumn{3}{c}{PCM} & \multicolumn{3}{c}{GCM} & \multicolumn{3}{c}{WGSC} \\
\cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
& & & & \texttt{lm} & \texttt{rf} & \texttt{xgb} & \texttt{lm} & \texttt{rf} & \texttt{xgb} & \texttt{lm} & \texttt{rf} & \texttt{xgb} \\
\midrule
\multirow{4}{*}{200} & \multirow{2}{*}{Null} & $\checkmark$ & 0.166 & 0.048 & 0.062 & 0.100 & 0.052 & 0.028 & 0.096 & 0.000 & 0.254 & 0.064 \\
& & $\times$ & 0.142 & 0.042 & 0.048 & 0.086 & 0.058 & 0.056 & 0.098 & 0.000 & 0.066 & 0.056 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.688 & 0.046 & 0.726 & 0.578 & 0.214 & 0.204 & 0.266 & 0.000 & 0.406 & 0.636 \\
& & $\times$ & 0.718 & 0.040 & 0.758 & 0.646 & 0.236 & 0.248 & 0.282 & 0.000 & 0.658 & 0.670 \\
\midrule
\multirow{4}{*}{500} & \multirow{2}{*}{Null} & $\checkmark$ & 0.076 & 0.058 & 0.064 & 0.072 & 0.046 & 0.030 & 0.088 & 0.000 & 0.268 & 0.038 \\
& & $\times$ & 0.066 & 0.042 & 0.034 & 0.070 & 0.046 & 0.050 & 0.080 & 0.000 & 0.056 & 0.060 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.734 & 0.062 & 0.804 & 0.762 & 0.268 & 0.250 & 0.294 & 0.000 & 0.426 & 0.708 \\
& & $\times$ & 0.764 & 0.046 & 0.808 & 0.794 & 0.264 & 0.270 & 0.292 & 0.000 & 0.742 & 0.754 \\
\midrule
\multirow{4}{*}{1000} & \multirow{2}{*}{Null} & $\checkmark$ & 0.064 & 0.052 & 0.034 & 0.072 & 0.050 & 0.034 & 0.066 & 0.000 & 0.272 & 0.020 \\
& & $\times$ & 0.086 & 0.048 & 0.046 & 0.072 & 0.036 & 0.036 & 0.080 & 0.000 & 0.052 & 0.036 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.774 & 0.056 & 0.826 & 0.832 & 0.244 & 0.214 & 0.246 & 0.000 & 0.470 & 0.762 \\
& & $\times$ & 0.818 & 0.048 & 0.830 & 0.838 & 0.236 & 0.236 & 0.268 & 0.000 & 0.800 & 0.806 \\
\midrule
\multirow{4}{*}{2000} & \multirow{2}{*}{Null} & $\checkmark$ & 0.070 & 0.034 & 0.024 & 0.062 & 0.078 & 0.050 & 0.084 & 0.000 & 0.274 & 0.026 \\
& & $\times$ & 0.082 & 0.046 & 0.036 & 0.084 & 0.076 & 0.068 & 0.098 & 0.000 & 0.044 & 0.024 \\
\cmidrule(lr){2-13}
& \multirow{2}{*}{Alternative} & $\checkmark$ & 0.790 & 0.038 & 0.806 & 0.816 & 0.224 & 0.220 & 0.250 & 0.000 & 0.472 & 0.790 \\
& & $\times$ & 0.810 & 0.044 & 0.814 & 0.824 & 0.232 & 0.232 & 0.268 & 0.000 & 0.816 & 0.802 \\
\midrule[1.5pt]
\end{tabular}
}
\label{tab:scenario3u_performance_by_alg1}
\end{table}





\begin{table}[h]
\centering
\caption{Sensitivity analysis of \Cref{Algorithm: Converting C2ST into CIT} for Scenario 1(U) under null and alternative hypotheses. The table shows rejection rates of four CIT methods~(RCIT, PCM, GCM, WGSC) for $\varepsilon$ values and sample sizes.}
\small
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{18pt} 
\renewcommand{\arraystretch}{1} % Adjust the row height
\begin{tabular}{ccccccc}
\toprule[1.8pt]
$n$ & Hypothesis & $\varepsilon$ & RCIT & PCM & GCM & WGSC \\
\midrule
\multirow{6}{*}{200} & \multirow{3}{*}{Null} & $1/n$ & 0.164 & 0.064 & 0.022 & 0.072 \\
 & & $1/\log{(n)}$ & 0.166 & 0.064 & 0.036 & 0.076 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.168 & 0.072 & 0.018 & 0.062 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.220 & 0.070 & 0.080 & 0.068 \\
 & & $1/\log{(n)}$ & 0.240 & 0.068 & 0.076 & 0.082 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.224 & 0.074 & 0.072 & 0.066 \\
\midrule
\multirow{6}{*}{500} & \multirow{3}{*}{Null} & $1/n$ & 0.090 & 0.038 & 0.026 & 0.052 \\
 & & $1/\log{(n)}$ & 0.056 & 0.066 & 0.024 & 0.050 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.080 & 0.036 & 0.036 & 0.044 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.250 & 0.050 & 0.126 & 0.040 \\
 & & $1/\log{(n)}$ & 0.228 & 0.092 & 0.170 & 0.048 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.244 & 0.062 & 0.166 & 0.046 \\
\midrule
\multirow{6}{*}{1000} & \multirow{3}{*}{Null} & $1/n$ & 0.046 & 0.052 & 0.030 & 0.038 \\
 & & $1/\log{(n)}$ & 0.064 & 0.050 & 0.024 & 0.056 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.062 & 0.040 & 0.030 & 0.040 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.376 & 0.078 & 0.258 & 0.048 \\
 & & $1/\log{(n)}$ & 0.394 & 0.098 & 0.262 & 0.054 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.394 & 0.078 & 0.288 & 0.038 \\
\midrule[1.5pt]
\end{tabular}}
\label{tab:scenario1u_varepsilon}
\end{table}

\begin{table}[h]
\centering
\caption{Sensitivity analysis of \Cref{Algorithm: Converting C2ST into CIT} for Scenario 2(U) under null and alternative hypotheses. The table shows rejection rates of four CIT methods~(RCIT, PCM, GCM, WGSC) for $\varepsilon$ values and sample sizes.}
\small
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{18pt} 
\renewcommand{\arraystretch}{1} % Adjust the row height
\begin{tabular}{ccccccc}
\toprule[1.5pt]
$n$ & Hypothesis & $\varepsilon$ & RCIT & PCM & GCM & WGSC \\
\midrule
\multirow{6}{*}{200} & \multirow{3}{*}{Null} & $1/n$ & 0.168 & 0.070 & 0.030 & 0.064 \\
 & & $1/\log{(n)}$ & 0.162 & 0.074 & 0.044 & 0.060 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.164 & 0.068 & 0.028 & 0.064 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.694 & 0.418 & 0.026 & 0.200 \\
 & & $1/\log{(n)}$ & 0.730 & 0.466 & 0.044 & 0.192 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.724 & 0.500 & 0.038 & 0.178 \\
\midrule
\multirow{6}{*}{500} & \multirow{3}{*}{Null} & $1/n$ & 0.080 & 0.054 & 0.020 & 0.046 \\
 & & $1/\log{(n)}$ & 0.074 & 0.060 & 0.046 & 0.042 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.078 & 0.040 & 0.024 & 0.038 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.874 & 0.824 & 0.028 & 0.276 \\
 & & $1/\log{(n)}$ & 0.906 & 0.864 & 0.050 & 0.330 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.902 & 0.812 & 0.030 & 0.324 \\
\midrule
\multirow{6}{*}{1000} & \multirow{3}{*}{Null} & $1/n$ & 0.094 & 0.044 & 0.028 & 0.038 \\
 & & $1/\log{(n)}$ & 0.086 & 0.032 & 0.020 & 0.030 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.064 & 0.042 & 0.032 & 0.040 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.958 & 0.968 & 0.022 & 0.576 \\
 & & $1/\log{(n)}$ & 0.968 & 0.990 & 0.026 & 0.604 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.964 & 0.980 & 0.022 & 0.626 \\
\midrule[1.5pt]
\end{tabular}}
\label{tab:scenario2u_varepsilon}
\end{table}



\begin{table}[h]
\centering
\caption{Sensitivity analysis of \Cref{Algorithm: Converting C2ST into CIT} for Scenario 3(U) under null and alternative hypotheses. The table shows rejection rates of four CIT methods~(RCIT, PCM, GCM, WGSC) for $\varepsilon$ values and sample sizes.}
\small
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{18pt} 
\renewcommand{\arraystretch}{1} % Adjust the row height
\begin{tabular}{ccccccc}
\toprule[1.5pt]
$n$ & Hypothesis & $\varepsilon$ & RCIT & PCM & GCM & WGSC \\
\midrule
\multirow{6}{*}{200} & \multirow{3}{*}{Null} & $1/n$ & 0.160 & 0.070 & 0.026 & 0.070 \\
 & & $1/\log{(n)}$ & 0.166 & 0.062 & 0.028 & 0.064 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.112 & 0.060 & 0.026 & 0.076 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.688 & 0.704 & 0.216 & 0.626 \\
 & & $1/\log{(n)}$ & 0.688 & 0.726 & 0.204 & 0.636 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.698 & 0.722 & 0.210 & 0.644 \\
\midrule
\multirow{6}{*}{500} & \multirow{3}{*}{Null} & $1/n$ & 0.074 & 0.038 & 0.006 & 0.048 \\
 & & $1/\log{(n)}$ & 0.076 & 0.064 & 0.030 & 0.038 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.084 & 0.054 & 0.030 & 0.038 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.722 & 0.792 & 0.250 & 0.692 \\
 & & $1/\log{(n)}$ & 0.734 & 0.804 & 0.250 & 0.708 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.730 & 0.782 & 0.252 & 0.694 \\
\midrule
\multirow{6}{*}{1000} & \multirow{3}{*}{Null} & $1/n$ & 0.076 & 0.042 & 0.038 & 0.048 \\
 & & $1/\log{(n)}$ & 0.064 & 0.034 & 0.034 & 0.020 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.060 & 0.038 & 0.032 & 0.036 \\
\cmidrule(lr){2-7}
 & \multirow{3}{*}{Alternative} & $1/n$ & 0.766 & 0.822 & 0.216 & 0.738 \\
 & & $1/\log{(n)}$ & 0.774 & 0.826 & 0.214 & 0.762 \\
 & & $1/\sqrt{\log{(n)}}$ & 0.768 & 0.822 & 0.208 & 0.760 \\
\midrule[1.5pt]
\end{tabular}}
\label{tab:scenario3u_varepsilon}
\end{table}

\subsection{Additional Simulations}\label{sec:additional_simulations}

To provide a more comprehensive evaluation of the proposed methods, we extend our synthetic examples beyond the Gaussian covariate distributions to include heavy-tailed and non-Gaussian scenarios. These additional scenarios are designed to assess performance when the marginal distributions of $X$ deviate from normality. 

\textbf{Scenario 4: Linear Model with Heavy-tailed Covariates.} For the unbounded case (4U), we generate covariates as $X^{(j)} = \mu^{(j)} + \tau^{(j)}$ for $j \in {1, 2}$, where $\tau^{(j)} = (\tau_1^{(j)}, \ldots, \tau_d^{(j)})^\top$ with independent components $\tau_i^{(j)} \sim t_{\nu_j}$. The degrees of freedom are set to $\nu_1 = 5$ and $\nu_2 = 4$, where the smaller degrees of freedom for group 2 produces heavier tails and results in an unbounded density ratio $r(x) = f_1(x)/f_2(x)$. The location vectors are $\mu^{(1)} = (1, 1, -1, -1, 0, \ldots, 0)^\top$ and $\mu^{(2)} = \mathbf{0}_d$. In the bounded case (4B), we maintain the same degrees of freedom specification but truncate the support of both distributions to the compact set $[-3, 3]^d$, ensuring the density ratio remains bounded.

The conditional distribution follows the same structure as Scenario 1: $Y^{(j)} \given X^{(j)} = \delta^{(j)} + {X^{(j)}}^\top\beta + \epsilon^{(j)}$, where $\beta = (1, -1, 1, -1, 0, \ldots, 0)^\top$ and $\epsilon^{(j)}$ follows a $t$-distribution with 2 degrees of freedom. Under the null hypothesis, we set $\delta^{(1)} = \delta^{(2)} = 0$, while for the alternative hypothesis, we introduce a mean shift by setting $\delta^{(1)} = 0$ and $\delta^{(2)} = 0.5$.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{Figures/Scenario4.pdf}
    \captionsetup{skip=0pt}  
    \caption{Rejection rates for Scenario 4 under null and alternative hypotheses, shown for both unbounded (U) and bounded (B) settings. Results are averaged over 500 repetitions with significance level $\alpha = 0.05$.}
    \label{fig:Scenario 1}
\end{figure}

\noindent \textbf{Scenario 5: Nonlinear Model with Beta-distributed Covariates.}
This scenario examines the performance of methods when covariates follow group-specific product Beta distributions on $(0,1)^d$. For group 1, $X^{(1)}$ has independent $\mathrm{Beta}(0.5, 2)$ components, and for group 2, $X^{(2)}$ has independent $\mathrm{Beta}(2, 2)$ components. In the unbounded case (5U), we use the full support, resulting in an unbounded density ratio near zero due to shape differences. The bounded case (5B) truncates support to $[\epsilon, 1-\epsilon]^d$ with $\epsilon=0.1$, ensuring a bounded density ratio. The conditional response is generated as 
$$
Y^{(j)} \mid X^{(j)} = \sin(2\pi X^{(j)}_1) + 0.5 \log(1 + 10 X^{(j)}_2) + 0.3 (X^{(j)}_3 - 0.5)^2 + \epsilon + \delta^{(j)},
$$

where $\epsilon \sim N(0,1)$, with no shift under the null ($\delta^{(1)} = \delta^{(2)} = 0$) and a shift of $\delta^{(2)}=0.5$ for group 2 under the alternative.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{Figures/Scenario5.pdf}
    \captionsetup{skip=0pt}  
    \caption{Rejection rates for Scenario 5 under null and alternative hypotheses, shown for both unbounded (U) and bounded (B) settings. Results are averaged over 500 repetitions with significance level $\alpha = 0.05$.}
    \label{fig:Scenario 1}
\end{figure}

\subsection{Bandwidth Selection for MMD-based Tests}

In our main experiments, we use a fixed bandwidth of $\lambda = 1$ for the Gaussian kernel in all MMD-based tests. Fixed bandwidth selection offers simplicity and facilitates theoretical analysis, as the kernel remains deterministic and independent of the data \citep{gretton2012kernel, schrab2023mmd}. 

To investigate the impact of bandwidth choice on test performance, we conduct additional experiments on Scenarios 1, 2, and 3 using $\mathrm{MMD}\text{-}\ell$ with bandwidths $\lambda \in \{0.1, 0.5, 1, \lambda_{\text{med}}, 5, 10\}$, where $\lambda{\text{med}}$ denotes the median heuristic bandwidth. Figure~\ref{fig:bandwidth_comparison} presents the rejection rates under both null and alternative hypotheses across sample sizes $n \in \{200, 500, 1000, 2000\}$.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{Figures/bandwidth_comparison_LinearMMD.pdf}
    \captionsetup{skip=0pt}  
    \caption{Rejection rates for $\mathrm{MMD}\text{-}\ell$ across different bandwidth choices on Scenarios 1, 2, and 3. Each panel shows results for one scenario under null (top) and alternative (bottom) hypotheses. Results are averaged over 500 repetitions with significance level $\alpha = 0.05$. The median heuristic bandwidth is denoted as "median".}
    \label{fig:bandwidth_comparison}
\end{figure}

The results demonstrate that the median heuristic generally achieves competitive or superior power compared to fixed bandwidths across most scenarios. However, for the sake of simplicity and to maintain consistency with our theoretical framework, we retain the fixed bandwidth $\sigma = 1$ in our main experiments.






\subsection{Computational Cost Analysis}\label{sec:computational_cost}

This section summarizes the empirical computational costs of the methods considered in \Cref{Section: Numerical Experiments} and provides a brief discussion to guide practitioners. Unless stated otherwise, each result indicates the average elapsed time (in seconds) to run a test end-to-end. Each result is derived from 500 repetitions and is executed in \textsf{R} using a single thread.

\paragraph{Empirical Performance Analysis.}
Table~\ref{tab:comp_cost_sim} presents empirical computation time for Scenario 1(U) from \Cref{Section: Synthetic Data Examples} varying in $n \in \{200, 500, 1000, 2000\}$. DRT methods with LLR density ratio estimator show low computational costs, but DCP shows higher computational costs compared to other DRT methods because its cross-fitting and orthogonalization step. CIT methods show relatively higher costs than DRT methods. 

Shifting to high-dimensional settings, Table~\ref{tab:comp_cost_highdim} reports times for the superconductivity dataset. Here, DRT costs rise notably with KLR, reflecting its kernel computations in high dimensions, whereas LLR remains efficient. CIT methods show similar patterns, with \texttt{rf} again proving costliest. Figure~\ref{fig:computational-costs} visualizes these trends on a log-scale, highlighting linear growth for most methods while underscoring estimator impacts.

These empirical patterns suggest that for time-sensitive applications, simpler estimators (e.g., LLR or \texttt{lm}) paired with efficient tests like MMD-$\ell$ or RCIT offer computational advantages, especially in high dimensions.

\paragraph{High-level takeaways.}
\begin{itemize}
    \item \emph{Computationally efficient methods.} For a fixed kernel and linear density ratio estimator, the DRT methods MMD-$\ell$, CLF, and CP exhibit empirical runtimes that scale approximately linearly in the sample size $n$. Cross-fitting (denoted $^\dagger$ increases the computational time by a constant factor but preserves the linear scaling. DCP incurs a greater computational cost due to cross-fitting and an orthogonalization step. 
    \item \emph{Impact of estimator choice.} The choice of estimator is critical to performance. Within the DRT framework, KLR can be substantially slower than LLR, particularly in high dimensions. Similarly, for CITs, random forests (\texttt{rf}) are more computationally intensive than linear models (\texttt{lm}). For problems where the signal is approximately linear or low-dimensional, the simpler estimator (LLR, \texttt{lm}) offer a favorable balance between computational cost and statistical power. 
\end{itemize}

% CIT와 DRT clustering 
% legend 위치 변경
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\textwidth]{Figures/compute_cost_by_samplesize.pdf}
%     \caption{
%         Average computation time (seconds) versus power with $n= 2000$ on the Scenario1(U) dataset for all methods. Plots reflect the mean over 500 repetitions. Full details are provided in Appendix C.6.
%     }
%     \label{fig:computational-costs}
% \end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Figures/compute_cost_n2000.pdf}
    \caption{
        Average computation time (seconds) versus power with $n= 2000$ on the Scenario1(U) dataset for all methods. Plots reflect the mean over 500 repetitions. Full details are provided in Appendix C.6.
    }
    \label{fig:computational-costs}
\end{figure}

\begin{table}[h!]
    \centering
    \caption{Average computation time (seconds) for Scenario 1(U) across sample sizes($n$). Values are means for 500 repetitions. Methods marked $^\dagger$ use 2-fold cross-fitting.}
    \label{tab:comp_cost_sim}
    \begin{tabular}{ccccc}
    \toprule
    \textbf{Method} & \multicolumn{4}{c}{\textbf{Sample Size($n$)}} \\ \cmidrule(lr){2-5} & \multicolumn{1}{c}{\textbf{200}} & \multicolumn{1}{c}{\textbf{500}} & \multicolumn{1}{c}{\textbf{1000}} & \multicolumn{1}{c}{\textbf{2000}} \\ 
    \midrule
    GCM & 0.029 & 0.037 & 0.071 & 0.193 \\
    PCM & 0.062 & 0.072 & 0.104 & 0.235 \\
    RCIT & 0.017 & 0.031 & 0.049 & 0.089 \\
    WGSC & 5.098 & 6.590 & 7.526 & 12.010 \\
    \midrule
    MMD-\(\ell\) & 0.005 & 0.008 & 0.012 & 0.022 \\ 
    CLF & 0.008 & 0.011 & 0.017 & 0.028 \\
    $^{\dagger}\mathrm{MMD}\text{-}\ell$ & 0.010 & 0.015 & 0.024 & 0.044 \\
    $^{\dagger}\mathrm{CLF}$ & 0.017 & 0.023 & 0.034 & 0.058 \\
    CP & 0.007 & 0.012 & 0.021 & 0.047 \\
    DCP & 0.071 & 0.243 & 1.090 & 3.895 \\
    \bottomrule 
    \end{tabular}
    
\end{table}

\begin{table*}[t!]
\centering
\caption{Average computation time (seconds) on the high-dimensional Superconductivity dataset. CIT methods are shown with linear models (\texttt{lm}) and random forests (\texttt{rf}). DRT methods are shown with linear logistic regression (LLR) and kernel logistic regression (KLR). Means are over 500 repetitions and $^\dagger$ indicates 2-fold cross-fitting.}
\label{tab:comp_cost_highdim}
\begin{tabular}{c c cccccc}
\toprule
\textbf{Method} & \textbf{Estimator} & \multicolumn{6}{c}{\textbf{Sample Size ($n$)}} \\
\cmidrule(lr){3-8}
 & & \multicolumn{1}{c}{\textbf{200}} & \multicolumn{1}{c}{\textbf{400}} & \multicolumn{1}{c}{\textbf{800}} & \multicolumn{1}{c}{\textbf{1200}} & \multicolumn{1}{c}{\textbf{1600}} & \multicolumn{1}{c}{\textbf{2000}} \\
\midrule
\multirow{1}{*}{RCIT} & & 0.041 & 0.072 & 0.106 & 0.117 & 0.164 & 0.159 \\
\addlinespace

\multirow{2}{*}{GCM} & \texttt{lm} & 0.013 & 0.021 & 0.037 & 0.058 & 0.089 & 0.092 \\
 & \texttt{rf} & 0.042 & 0.070 & 0.145 & 0.228 & 0.347 & 0.406 \\
\addlinespace
\multirow{2}{*}{PCM} & \texttt{lm} & 0.031 & 0.032 & 0.051 & 0.075 & 0.085 & 0.125 \\
 & \texttt{rf} & 0.084 & 0.118 & 0.197 & 0.316 & 0.360 & 0.520 \\
\addlinespace
\multirow{2}{*}{WGSC} & \texttt{lm} & 0.130 & 0.198 & 0.396 & 0.649 & 0.837 & 1.035 \\
 & \texttt{rf} & 0.420 & 0.662 & 1.319 & 2.064 & 2.748 & 3.515 \\
\midrule
\multirow{2}{*}{MMD-$\ell$} & LLR & 0.030 & 0.037 & 0.053 & 0.059 & 0.074 & 0.088 \\
 & KLR & 0.043 & 0.139 & 0.661 & 1.337 & 2.862 & 5.069 \\
\addlinespace
\multirow{2}{*}{$^\dagger$MMD-$\ell$} & LLR & 0.063 & 0.071 & 0.111 & 0.112 & 0.161 & 0.195 \\
 & KLR & 0.079 & 0.275 & 1.186 & 2.674 & 5.817 & 9.739 \\
\addlinespace
\multirow{2}{*}{CLF} & LLR & 0.067 & 0.068 & 0.089 & 0.092 & 0.127 & 0.178 \\
 & KLR & 0.061 & 0.209 & 0.799 & 1.806 & 4.251 & 7.902 \\
\addlinespace
\multirow{2}{*}{$^\dagger$CLF} & LLR & 0.125 & 0.116 & 0.159 & 0.179 & 0.226 & 0.266 \\
 & KLR & 0.117 & 0.335 & 1.414 & 3.633 & 8.026 & 13.501 \\
\addlinespace
\multirow{2}{*}{CP} & LLR & 0.035 & 0.047 & 0.065 & 0.067 & 0.097 & 0.119 \\
 & KLR & 0.046 & 0.162 & 0.670 & 1.332 & 3.178 & 5.405 \\
\addlinespace
\multirow{2}{*}{DCP} & LLR & 0.245 & 0.387 & 0.868 & 1.692 & 3.200 & 4.903 \\
 & KLR & 0.214 & 0.491 & 1.884 & 3.693 & 7.181 & 11.895 \\
\bottomrule
\end{tabular}
\end{table*}

\end{document}
